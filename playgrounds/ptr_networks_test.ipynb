{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import torch.nn as nn\n",
    "from timeit import timeit\n",
    "import pickle\n",
    "from os import path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = t.device(\"cuda\")\n",
    "FLOAT_TYPE = t.float32\n",
    "INT_TYPE = t.int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pickle(in_file: str, out_file: str, n_points: int, float_dtype: t.dtype, int_dtype: t.dtype, cap: int | None = None):\n",
    "    with open(in_file, 'r') as f:\n",
    "        raw_data = f.readlines()\n",
    "        data = {\n",
    "            \"x\": t.zeros(len(raw_data), n_points, 2, dtype=float_dtype, device=\"cpu\"),\n",
    "            \"y\": t.zeros(len(raw_data), n_points + 1, dtype=int_dtype, device=\"cpu\") # loops back around\n",
    "        }\n",
    "        for i, line in enumerate(raw_data):\n",
    "            if cap is not None and i >= cap:\n",
    "                data[\"x\"] = data[\"x\"][:i]\n",
    "                data[\"y\"] = data[\"y\"][:i]\n",
    "\n",
    "                break\n",
    "\n",
    "            if (i + 1) % 10000 == 0:\n",
    "                print(f\"Loaded data: {i + 1}/{len(raw_data)}; cap: {cap}\")\n",
    "                pickle.dump(data, open(out_file, \"wb\"))\n",
    "\n",
    "            index = 0\n",
    "            x_mode = True\n",
    "            for token in line.split(\" \"):\n",
    "                if token == \"\\n\":\n",
    "                    continue\n",
    "\n",
    "                if token == \"output\":\n",
    "                    x_mode = False\n",
    "                    index = 0\n",
    "                    continue\n",
    "                \n",
    "                if x_mode:\n",
    "                    data[\"x\"][i, index // 2, index % 2] = float(token)\n",
    "                    index += 1\n",
    "                else:\n",
    "                    data[\"y\"][i, index] = float(token)\n",
    "                    index += 1\n",
    "\n",
    "        pickle.dump(data, open(out_file, \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.isfile(\"../data/ptr_nets/tsp10.pkl\"):\n",
    "    generate_pickle(\"../data/ptr_nets/tsp10.txt\", \"../data/ptr_nets/tsp10.pkl\", 10, FLOAT_TYPE, INT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvexHullDataset(Dataset):\n",
    "    def __init__(self, in_file: str, n_points: int, device: t.device, float_dtype: t.dtype, int_dtype: t.dtype):\n",
    "        self.n_points = n_points\n",
    "        self.data = pickle.load(open(in_file, \"rb\"))\n",
    "        \n",
    "        assert self.data[\"x\"].shape[1] == self.n_points, \"Number of points in each sample does not match n_points\"\n",
    "        assert self.data[\"x\"].shape[2] == 2, \"Each point must have 2 coordinates\"\n",
    "        assert self.data[\"x\"].dtype == float_dtype, \"Data type of x does not match float_dtype\"\n",
    "        assert self.data[\"y\"].dtype == int_dtype, \"Data type of y does not match int_dtype\"\n",
    "            \n",
    "        print(\"Copying x to device\")\n",
    "        self.data[\"x\"] = self.data[\"x\"].to(device)\n",
    "        print(\"Copying y to device\")\n",
    "        self.data[\"y\"] = self.data[\"y\"].to(device)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data[\"x\"])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[\"x\"][idx], self.data[\"y\"][idx].long() # NO LONGER - 1 # -1 to make it 0-indexed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying x to device\n",
      "Copying y to device\n"
     ]
    }
   ],
   "source": [
    "convex_hull_dataset = ConvexHullDataset(\"../data/ptr_nets/tsp10.pkl\", 10, DEVICE, FLOAT_TYPE, INT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PtrNet2(nn.Module):\n",
    "    def __init__(self, in_dim: int, m_out: int, dtype: t.dtype, hidden_dim: int = 512) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_dim = in_dim # input embedding size\n",
    "        self.n_out = m_out   # number of output indices to return\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.encoder = nn.LSTM(in_dim, hidden_dim, 1, dtype=dtype)\n",
    "        self.decoder = nn.LSTM(in_dim, hidden_dim, 1, dtype=dtype)\n",
    "\n",
    "        self.starting_token = t.ones(in_dim, dtype=dtype) * -1.0\n",
    "        self.ending_token = t.ones(hidden_dim, dtype=dtype)\n",
    "\n",
    "        for i in range(hidden_dim):\n",
    "            self.ending_token[i] = - ((i + 1) % 2)\n",
    "\n",
    "        self.W1 = nn.Parameter(t.randn(hidden_dim, hidden_dim, dtype=dtype, requires_grad=True))\n",
    "        self.W2 = nn.Parameter(t.randn(hidden_dim, hidden_dim, dtype=dtype, requires_grad=True))\n",
    "        self.v  = nn.Parameter(t.randn(1, hidden_dim, dtype=dtype, requires_grad=True))\n",
    "\n",
    "\n",
    "    def forward(self, x: t.Tensor, y: t.Tensor | None = None):\n",
    "        \"\"\"\n",
    "        x is a batch_size x in_len x in_dim tensor\n",
    "        y is a batch_size x n_out tensor of indices. The indices are one-indexed:\n",
    "          used for during training as teacher forcing. ignored during validation\n",
    "          if y[n, i] = k, this means for the nth batch, the ith output vector is to be equal to the (k)th input vector\n",
    "          the 0th index is reserved for the ending token\n",
    "\n",
    "        returns: a tuple (A, (encoder_all_hiddens, decoder_all_hiddens)) where:\n",
    "            A is a batch_size x (n_out + 1) x (in_len + 1) matrix; for each batch, we get (n_out + 1) probability pointers to the input sequence. The pointers are effectively 1-indexed: the 0th index refers to a pointer to the ending token, which is a signal to stop. The 1st index refers to a pointer to the 0th position in the input sequence, and so on.\n",
    "            encoder_all_hiddens is a batch_size x in_len + 1 x hidden_dim matrix; the first position is the ending token\n",
    "            decoder_all_hiddens is a batch_size x n_out + 1 x hidden_dim matrix; the first position is first output token, viz. the hidden state AFTER the starting token; the (i + 1)th position is the hidden state AFTER the ith output token; the (n_out + 1)th position is the hidden state AFTER the final output token, which should be used to generate a pointer to the start token\n",
    "        \"\"\"\n",
    "\n",
    "        # force batched\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(0)\n",
    "        assert x.dim() == 3\n",
    "        batch_size = len(x)\n",
    "        batch_size_arange = t.arange(batch_size)\n",
    "        in_len = x.shape[1]\n",
    "\n",
    "        if self.training:\n",
    "            assert y is not None\n",
    "            assert x.shape[0] == y.shape[0]\n",
    "            assert y.shape[1] == self.n_out\n",
    "\n",
    "        \n",
    "        assert x.shape[2] == self.in_dim\n",
    "\n",
    "        x = x.permute(1, 0, 2) # TODO: change it so that input is already permuted\n",
    "\n",
    "        # define L\n",
    "        # define encoder_all_hiddens, an N * L + 1 * embedding_dim matrix\n",
    "        encoder_all_hiddens = t.zeros(in_len + 1, batch_size, self.hidden_dim, dtype=self.dtype, device=self.device)\n",
    "        encoder_first_hidden = self.ending_token.repeat(1, batch_size, 1) # NOTE: FIRST position in L dimension is the ending token. Thus, a pointer to the FIRST (0)th position is a signal to stop\n",
    "        encoder_all_hiddens[0:1, :, :] = encoder_first_hidden\n",
    "\n",
    "        # encode\n",
    "        encoder_all_hiddens[1:, :, :], (encoder_last_hidden, encoder_last_cell_state) = self.encoder(x, (encoder_first_hidden, t.zeros(1, batch_size, self.hidden_dim, dtype=self.dtype, device=self.device)))\n",
    "\n",
    "        # initialize decode variables\n",
    "        decoder_all_hiddens = t.zeros(self.n_out + 1, batch_size, self.hidden_dim, dtype=self.dtype, device=self.device)\n",
    "        decoder_current_cell_state = encoder_last_cell_state\n",
    "\n",
    "        # decode\n",
    "        if self.training:\n",
    "            decoder_input = t.empty(self.n_out + 1, batch_size, self.in_dim, dtype=self.dtype, device=self.device)\n",
    "            decoder_input[0:1, :, :] = self.starting_token.repeat(1, batch_size, 1)\n",
    "            decoder_input[1:, :, :] = x[y.t() - 1, batch_size_arange.view(1, batch_size), :]\n",
    "\n",
    "            decoder_all_hiddens[:, :, :], _ = self.decoder(decoder_input, (encoder_last_hidden, decoder_current_cell_state))\n",
    "\n",
    "            lin_enc = self.W1 @ encoder_all_hiddens.permute(1, 2, 0)\n",
    "            lin_dec = self.W2 @ decoder_all_hiddens.permute(1, 2, 0)\n",
    "\n",
    "            TANH = (lin_enc.unsqueeze(3) + lin_dec.unsqueeze(2)).tanh()\n",
    "            U = self.v @ TANH.permute(0, 3, 1, 2) # batch_size x (n_out + 1) x 1 x in_len + 1\n",
    "\n",
    "            A = U.squeeze(dim=2)\n",
    "            # A = U.softmax(dim=3).squeeze(dim=2) # batch_size x (n_out + 1) x in_len + 1\n",
    "\n",
    "            return A, (encoder_all_hiddens.permute(1, 0, 2), decoder_all_hiddens.permute(1, 0, 2))\n",
    "                      # returns a batch_size x in_len + 1 x hidden_dim matrix,\n",
    "                      # a batch_size x n_out + 1 x hidden_dim matrix,\n",
    "                      # and a batch_size x (n_out + 1) x in_len + 1 matrix\n",
    "        else:\n",
    "            decoder_current_input  = self.starting_token.repeat(1, batch_size, 1)\n",
    "            decoder_current_hidden = encoder_last_hidden\n",
    "\n",
    "            \n",
    "            A = t.empty(self.n_out + 1, batch_size, in_len + 1, dtype=self.dtype, device=self.device)\n",
    "\n",
    "            for i in range(self.n_out + 1):\n",
    "                _, (decoder_current_hidden, decoder_current_cell_state) = self.decoder(decoder_current_input, (decoder_current_hidden, decoder_current_cell_state))\n",
    "                TANH_i = (self.W1 @ encoder_all_hiddens.permute(1, 2, 0) + self.W2 @ decoder_current_hidden.permute(1, 2, 0)).tanh()\n",
    "                U_i    = self.v @ TANH_i\n",
    "\n",
    "                A_i = U_i.squeeze(dim=1) # no softmaxing here\n",
    "\n",
    "                # A_i    = U_i.softmax(dim=2).squeeze(dim=1) # gives an batch_size x in_len matrix where m[n] is a probability distribution over in_length input indices\n",
    "                \n",
    "                next_index = A_i.argmax(dim=1) # TODO: handle when next_index[n][0] is 0 for some n, meaning that for the nth batch, the decoder should stop. TODO: add beam search functionality\n",
    "                A[i, :, :] = A_i\n",
    "                decoder_current_input = x[next_index - 1, batch_size_arange].unsqueeze(0)\n",
    "                decoder_all_hiddens[i, :, :] = decoder_current_hidden.squeeze(dim=0)\n",
    "            \n",
    "            return A.permute(1, 0, 2), (encoder_all_hiddens.permute(1, 0, 2), decoder_all_hiddens.permute(1, 0, 2))\n",
    "        \n",
    "    \n",
    "    def to(self, *args, **kwargs):\n",
    "        super().to(*args, **kwargs)\n",
    "        self.device = args[0]\n",
    "        self.starting_token = self.starting_token.to(self.device)\n",
    "        self.ending_token = self.ending_token.to(self.device)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one = PtrNet2(2, 11, FLOAT_TYPE).to(DEVICE)\n",
    "optim_one = t.optim.Adam(model_one.parameters(), lr=3e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_size = len(convex_hull_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(Subset(convex_hull_dataset, range(9 * train_dataset_size // 10)), batch_size=512, shuffle=True)\n",
    "eval_dataloader  = DataLoader(Subset(convex_hull_dataset, range(9 * train_dataset_size // 10, train_dataset_size)), batch_size=512, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: PtrNet2, train_dataloader: DataLoader, eval_dataloader: DataLoader, optim: t.optim.Optimizer, model_path: str, n_epochs: int = 100):\n",
    "    # NOTE: assuming all inputs are of same sequence length model.n_out\n",
    "    n_out = model.n_out\n",
    "    n_batches_train = len(train_dataloader)\n",
    "    n_batches_eval = len(eval_dataloader)\n",
    "\n",
    "    ce = t.nn.CrossEntropyLoss()\n",
    "    for epoch_number in range(n_epochs):\n",
    "        for training in [True, False]:\n",
    "            if not training:\n",
    "                pickle.dump(model, open(f\"{model_path}_{epoch_number}.pkl\", \"wb\"))\n",
    "            for batch_number, (x_train, y_train) in enumerate(train_dataloader if training else eval_dataloader):\n",
    "                if training:\n",
    "                    model.train()\n",
    "                    optim.zero_grad()\n",
    "                else:\n",
    "                    model.eval()\n",
    "\n",
    "                probabilities, _ = model(x_train, y_train)\n",
    "\n",
    "                targets = t.zeros((y_train.shape[0], n_out + 1), dtype=t.long, device=x_train.device)\n",
    "                targets[:,:-1] = y_train\n",
    "                targets[:,-1] = 0\n",
    "\n",
    "                loss = ce(probabilities.permute(0, 2, 1), targets)\n",
    "\n",
    "                if batch_number % 100 == 0:\n",
    "                    print(f\"Epoch {epoch_number + 1}/{n_epochs} {'Training' if training else 'Evaluation'} Batch {batch_number + 1}/{(n_batches_train if training else n_batches_eval)} loss {loss.item()} accuracy: {(probabilities.argmax(dim=2)[:,1:-1] == targets[:,1:-1]).sum() / targets[:,1:-1].numel()}\")\n",
    "                \n",
    "                if training:\n",
    "                    loss.backward()\n",
    "                    optim.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 Training Batch 1/1758 loss 0.6837473511695862 accuracy: 0.7007812857627869\n",
      "Epoch 1/100 Training Batch 101/1758 loss 0.6291791796684265 accuracy: 0.7220703363418579\n",
      "Epoch 1/100 Training Batch 201/1758 loss 0.9140381813049316 accuracy: 0.6216797232627869\n",
      "Epoch 1/100 Training Batch 301/1758 loss 0.6196596026420593 accuracy: 0.7275390625\n",
      "Epoch 1/100 Training Batch 401/1758 loss 0.6746962070465088 accuracy: 0.708203136920929\n",
      "Epoch 1/100 Training Batch 501/1758 loss 0.5961706638336182 accuracy: 0.738476574420929\n",
      "Epoch 1/100 Training Batch 601/1758 loss 0.6433029174804688 accuracy: 0.715624988079071\n",
      "Epoch 1/100 Training Batch 701/1758 loss 0.5862244963645935 accuracy: 0.7437500357627869\n",
      "Epoch 1/100 Training Batch 801/1758 loss 0.5921444892883301 accuracy: 0.742382824420929\n",
      "Epoch 1/100 Training Batch 901/1758 loss 0.5942407250404358 accuracy: 0.7496094107627869\n",
      "Epoch 1/100 Training Batch 1001/1758 loss 0.6093252897262573 accuracy: 0.740429699420929\n",
      "Epoch 1/100 Training Batch 1101/1758 loss 1.1655341386795044 accuracy: 0.572265625\n",
      "Epoch 1/100 Training Batch 1201/1758 loss 0.8005029559135437 accuracy: 0.627734363079071\n",
      "Epoch 1/100 Training Batch 1301/1758 loss 0.7312543988227844 accuracy: 0.666210949420929\n",
      "Epoch 1/100 Training Batch 1401/1758 loss 0.6835088729858398 accuracy: 0.6923828125\n",
      "Epoch 1/100 Training Batch 1501/1758 loss 0.7062625288963318 accuracy: 0.683398425579071\n",
      "Epoch 1/100 Training Batch 1601/1758 loss 0.7295588850975037 accuracy: 0.6705078482627869\n",
      "Epoch 1/100 Training Batch 1701/1758 loss 0.6886128783226013 accuracy: 0.6962890625\n",
      "Epoch 1/100 Evaluation Batch 1/1758 loss 2.6552646160125732 accuracy: 0.3677734434604645\n",
      "Epoch 1/100 Evaluation Batch 101/1758 loss 2.6027672290802 accuracy: 0.3638671934604645\n",
      "Epoch 1/100 Evaluation Batch 201/1758 loss 2.7355804443359375 accuracy: 0.35273438692092896\n",
      "Epoch 1/100 Evaluation Batch 301/1758 loss 2.568648099899292 accuracy: 0.3597656190395355\n",
      "Epoch 1/100 Evaluation Batch 401/1758 loss 2.549513816833496 accuracy: 0.36601564288139343\n",
      "Epoch 1/100 Evaluation Batch 501/1758 loss 2.702176332473755 accuracy: 0.36152344942092896\n",
      "Epoch 1/100 Evaluation Batch 601/1758 loss 2.637131929397583 accuracy: 0.35625001788139343\n",
      "Epoch 1/100 Evaluation Batch 701/1758 loss 2.7548418045043945 accuracy: 0.3486328125\n",
      "Epoch 1/100 Evaluation Batch 801/1758 loss 2.7592122554779053 accuracy: 0.35664063692092896\n",
      "Epoch 1/100 Evaluation Batch 901/1758 loss 2.7451021671295166 accuracy: 0.3294921815395355\n",
      "Epoch 1/100 Evaluation Batch 1001/1758 loss 2.7007102966308594 accuracy: 0.357421875\n",
      "Epoch 1/100 Evaluation Batch 1101/1758 loss 2.553016424179077 accuracy: 0.37578126788139343\n",
      "Epoch 1/100 Evaluation Batch 1201/1758 loss 2.617048501968384 accuracy: 0.36835938692092896\n",
      "Epoch 1/100 Evaluation Batch 1301/1758 loss 2.7245032787323 accuracy: 0.3490234315395355\n",
      "Epoch 1/100 Evaluation Batch 1401/1758 loss 2.6290106773376465 accuracy: 0.35527345538139343\n",
      "Epoch 1/100 Evaluation Batch 1501/1758 loss 2.580338716506958 accuracy: 0.369140625\n",
      "Epoch 1/100 Evaluation Batch 1601/1758 loss 2.5485332012176514 accuracy: 0.38300782442092896\n",
      "Epoch 1/100 Evaluation Batch 1701/1758 loss 2.5633389949798584 accuracy: 0.36445313692092896\n",
      "Epoch 2/100 Training Batch 1/1758 loss 0.7304345965385437 accuracy: 0.6666015982627869\n",
      "Epoch 2/100 Training Batch 101/1758 loss 0.6453256607055664 accuracy: 0.7046875357627869\n",
      "Epoch 2/100 Training Batch 201/1758 loss 0.6546194553375244 accuracy: 0.7158203125\n",
      "Epoch 2/100 Training Batch 301/1758 loss 0.8299495577812195 accuracy: 0.6353515982627869\n",
      "Epoch 2/100 Training Batch 401/1758 loss 0.650327205657959 accuracy: 0.7142578363418579\n",
      "Epoch 2/100 Training Batch 501/1758 loss 0.6870667934417725 accuracy: 0.6878906488418579\n",
      "Epoch 2/100 Training Batch 601/1758 loss 0.6653371453285217 accuracy: 0.698046863079071\n",
      "Epoch 2/100 Training Batch 701/1758 loss 0.7267407774925232 accuracy: 0.6630859375\n",
      "Epoch 2/100 Training Batch 801/1758 loss 0.6549926996231079 accuracy: 0.7066406607627869\n",
      "Epoch 2/100 Training Batch 901/1758 loss 0.6837034225463867 accuracy: 0.6888672113418579\n",
      "Epoch 2/100 Training Batch 1001/1758 loss 0.6385058164596558 accuracy: 0.7171875238418579\n",
      "Epoch 2/100 Training Batch 1101/1758 loss 0.6614274978637695 accuracy: 0.6988281607627869\n",
      "Epoch 2/100 Training Batch 1201/1758 loss 0.6356956362724304 accuracy: 0.7177734375\n",
      "Epoch 2/100 Training Batch 1301/1758 loss 0.6629965305328369 accuracy: 0.701953113079071\n",
      "Epoch 2/100 Training Batch 1401/1758 loss 0.6444842219352722 accuracy: 0.7105469107627869\n",
      "Epoch 2/100 Training Batch 1501/1758 loss 0.6381832957267761 accuracy: 0.7173828482627869\n",
      "Epoch 2/100 Training Batch 1601/1758 loss 0.6466671824455261 accuracy: 0.712695300579071\n",
      "Epoch 2/100 Training Batch 1701/1758 loss 0.6322914958000183 accuracy: 0.7230468988418579\n",
      "Epoch 2/100 Evaluation Batch 1/1758 loss 2.8510608673095703 accuracy: 0.2972656190395355\n",
      "Epoch 2/100 Evaluation Batch 101/1758 loss 2.6184866428375244 accuracy: 0.3359375\n",
      "Epoch 2/100 Evaluation Batch 201/1758 loss 2.7774715423583984 accuracy: 0.30449220538139343\n",
      "Epoch 2/100 Evaluation Batch 301/1758 loss 2.780021905899048 accuracy: 0.3119140565395355\n",
      "Epoch 2/100 Evaluation Batch 401/1758 loss 2.831573724746704 accuracy: 0.30781251192092896\n",
      "Epoch 2/100 Evaluation Batch 501/1758 loss 2.821424722671509 accuracy: 0.3017578125\n",
      "Epoch 2/100 Evaluation Batch 601/1758 loss 2.7563092708587646 accuracy: 0.3121093809604645\n",
      "Epoch 2/100 Evaluation Batch 701/1758 loss 2.776508092880249 accuracy: 0.2972656190395355\n",
      "Epoch 2/100 Evaluation Batch 801/1758 loss 2.7798116207122803 accuracy: 0.30742189288139343\n",
      "Epoch 2/100 Evaluation Batch 901/1758 loss 2.809098243713379 accuracy: 0.3099609315395355\n",
      "Epoch 2/100 Evaluation Batch 1001/1758 loss 2.8610904216766357 accuracy: 0.3023437559604645\n",
      "Epoch 2/100 Evaluation Batch 1101/1758 loss 2.776507616043091 accuracy: 0.31132814288139343\n",
      "Epoch 2/100 Evaluation Batch 1201/1758 loss 2.7101011276245117 accuracy: 0.31328126788139343\n",
      "Epoch 2/100 Evaluation Batch 1301/1758 loss 2.788123846054077 accuracy: 0.30585938692092896\n",
      "Epoch 2/100 Evaluation Batch 1401/1758 loss 2.69973087310791 accuracy: 0.32109376788139343\n",
      "Epoch 2/100 Evaluation Batch 1501/1758 loss 2.736268997192383 accuracy: 0.32109376788139343\n",
      "Epoch 2/100 Evaluation Batch 1601/1758 loss 2.8151862621307373 accuracy: 0.3062500059604645\n",
      "Epoch 2/100 Evaluation Batch 1701/1758 loss 2.829885244369507 accuracy: 0.3080078065395355\n",
      "Epoch 3/100 Training Batch 1/1758 loss 1.0731412172317505 accuracy: 0.547656238079071\n",
      "Epoch 3/100 Training Batch 101/1758 loss 0.6192365288734436 accuracy: 0.725390613079071\n",
      "Epoch 3/100 Training Batch 201/1758 loss 0.6178084015846252 accuracy: 0.7232422232627869\n",
      "Epoch 3/100 Training Batch 301/1758 loss 0.6669532656669617 accuracy: 0.699414074420929\n",
      "Epoch 3/100 Training Batch 401/1758 loss 0.6299468874931335 accuracy: 0.7177734375\n",
      "Epoch 3/100 Training Batch 501/1758 loss 0.6647166609764099 accuracy: 0.702343761920929\n",
      "Epoch 3/100 Training Batch 601/1758 loss 0.7943153381347656 accuracy: 0.6509765982627869\n",
      "Epoch 3/100 Training Batch 701/1758 loss 0.6714756488800049 accuracy: 0.705078125\n",
      "Epoch 3/100 Training Batch 801/1758 loss 0.6474711298942566 accuracy: 0.710156261920929\n",
      "Epoch 3/100 Training Batch 901/1758 loss 0.6479394435882568 accuracy: 0.7171875238418579\n",
      "Epoch 3/100 Training Batch 1001/1758 loss 0.6545734405517578 accuracy: 0.708203136920929\n",
      "Epoch 3/100 Training Batch 1101/1758 loss 0.8742091059684753 accuracy: 0.605664074420929\n",
      "Epoch 3/100 Training Batch 1201/1758 loss 0.658341109752655 accuracy: 0.708789050579071\n",
      "Epoch 3/100 Training Batch 1301/1758 loss 0.7117910385131836 accuracy: 0.688671886920929\n",
      "Epoch 3/100 Training Batch 1401/1758 loss 0.6914231777191162 accuracy: 0.698046863079071\n",
      "Epoch 3/100 Training Batch 1501/1758 loss 0.652220606803894 accuracy: 0.708984375\n",
      "Epoch 3/100 Training Batch 1601/1758 loss 0.6600053906440735 accuracy: 0.71875\n",
      "Epoch 3/100 Training Batch 1701/1758 loss 0.6653891205787659 accuracy: 0.7080078125\n",
      "Epoch 3/100 Evaluation Batch 1/1758 loss 2.5896148681640625 accuracy: 0.36992189288139343\n",
      "Epoch 3/100 Evaluation Batch 101/1758 loss 2.6776888370513916 accuracy: 0.35039064288139343\n",
      "Epoch 3/100 Evaluation Batch 201/1758 loss 2.6395175457000732 accuracy: 0.3666015565395355\n",
      "Epoch 3/100 Evaluation Batch 301/1758 loss 2.5096728801727295 accuracy: 0.3646484315395355\n",
      "Epoch 3/100 Evaluation Batch 401/1758 loss 2.690293550491333 accuracy: 0.34882813692092896\n",
      "Epoch 3/100 Evaluation Batch 501/1758 loss 2.5961458683013916 accuracy: 0.36210939288139343\n",
      "Epoch 3/100 Evaluation Batch 601/1758 loss 2.5775134563446045 accuracy: 0.3509765565395355\n",
      "Epoch 3/100 Evaluation Batch 701/1758 loss 2.4724977016448975 accuracy: 0.36894533038139343\n",
      "Epoch 3/100 Evaluation Batch 801/1758 loss 2.5976524353027344 accuracy: 0.3666015565395355\n",
      "Epoch 3/100 Evaluation Batch 901/1758 loss 2.637842893600464 accuracy: 0.36406251788139343\n",
      "Epoch 3/100 Evaluation Batch 1001/1758 loss 2.6093826293945312 accuracy: 0.35957032442092896\n",
      "Epoch 3/100 Evaluation Batch 1101/1758 loss 2.5411784648895264 accuracy: 0.37226563692092896\n",
      "Epoch 3/100 Evaluation Batch 1201/1758 loss 2.6203830242156982 accuracy: 0.3501953184604645\n",
      "Epoch 3/100 Evaluation Batch 1301/1758 loss 2.6335017681121826 accuracy: 0.3548828065395355\n",
      "Epoch 3/100 Evaluation Batch 1401/1758 loss 2.5926849842071533 accuracy: 0.36542969942092896\n",
      "Epoch 3/100 Evaluation Batch 1501/1758 loss 2.5769541263580322 accuracy: 0.3724609315395355\n",
      "Epoch 3/100 Evaluation Batch 1601/1758 loss 2.522324800491333 accuracy: 0.3763671815395355\n",
      "Epoch 3/100 Evaluation Batch 1701/1758 loss 2.5422937870025635 accuracy: 0.3687500059604645\n",
      "Epoch 4/100 Training Batch 1/1758 loss 0.6324904561042786 accuracy: 0.71875\n",
      "Epoch 4/100 Training Batch 101/1758 loss 0.6219101548194885 accuracy: 0.722851574420929\n",
      "Epoch 4/100 Training Batch 201/1758 loss 0.7770891189575195 accuracy: 0.651171863079071\n",
      "Epoch 4/100 Training Batch 301/1758 loss 0.6950730681419373 accuracy: 0.6859375238418579\n",
      "Epoch 4/100 Training Batch 401/1758 loss 0.7814815640449524 accuracy: 0.645703136920929\n",
      "Epoch 4/100 Training Batch 501/1758 loss 0.6056684851646423 accuracy: 0.7294921875\n",
      "Epoch 4/100 Training Batch 601/1758 loss 0.6333584189414978 accuracy: 0.723828136920929\n",
      "Epoch 4/100 Training Batch 701/1758 loss 0.6303960084915161 accuracy: 0.7212890982627869\n",
      "Epoch 4/100 Training Batch 801/1758 loss 0.6794276237487793 accuracy: 0.699414074420929\n",
      "Epoch 4/100 Training Batch 901/1758 loss 0.7669498920440674 accuracy: 0.654296875\n",
      "Epoch 4/100 Training Batch 1001/1758 loss 0.7412135004997253 accuracy: 0.670117199420929\n",
      "Epoch 4/100 Training Batch 1101/1758 loss 0.6226295828819275 accuracy: 0.7320312857627869\n",
      "Epoch 4/100 Training Batch 1201/1758 loss 0.6365393996238708 accuracy: 0.7210937738418579\n",
      "Epoch 4/100 Training Batch 1301/1758 loss 0.6210946440696716 accuracy: 0.7265625\n",
      "Epoch 4/100 Training Batch 1401/1758 loss 0.6356276869773865 accuracy: 0.721484363079071\n",
      "Epoch 4/100 Training Batch 1501/1758 loss 0.6191676259040833 accuracy: 0.720703125\n",
      "Epoch 4/100 Training Batch 1601/1758 loss 0.7203366756439209 accuracy: 0.685742199420929\n",
      "Epoch 4/100 Training Batch 1701/1758 loss 0.6649781465530396 accuracy: 0.716992199420929\n",
      "Epoch 4/100 Evaluation Batch 1/1758 loss 2.660977602005005 accuracy: 0.33085939288139343\n",
      "Epoch 4/100 Evaluation Batch 101/1758 loss 2.5407843589782715 accuracy: 0.34687501192092896\n",
      "Epoch 4/100 Evaluation Batch 201/1758 loss 2.540151834487915 accuracy: 0.3570312559604645\n",
      "Epoch 4/100 Evaluation Batch 301/1758 loss 2.5556814670562744 accuracy: 0.36015626788139343\n",
      "Epoch 4/100 Evaluation Batch 401/1758 loss 2.5965094566345215 accuracy: 0.3433593809604645\n",
      "Epoch 4/100 Evaluation Batch 501/1758 loss 2.515704870223999 accuracy: 0.3482421934604645\n",
      "Epoch 4/100 Evaluation Batch 601/1758 loss 2.391695022583008 accuracy: 0.3765625059604645\n",
      "Epoch 4/100 Evaluation Batch 701/1758 loss 2.5776097774505615 accuracy: 0.3486328125\n",
      "Epoch 4/100 Evaluation Batch 801/1758 loss 2.457228422164917 accuracy: 0.3548828065395355\n",
      "Epoch 4/100 Evaluation Batch 901/1758 loss 2.4264719486236572 accuracy: 0.35859376192092896\n",
      "Epoch 4/100 Evaluation Batch 1001/1758 loss 2.626551389694214 accuracy: 0.33906251192092896\n",
      "Epoch 4/100 Evaluation Batch 1101/1758 loss 2.4033052921295166 accuracy: 0.3583984375\n",
      "Epoch 4/100 Evaluation Batch 1201/1758 loss 2.4889512062072754 accuracy: 0.3677734434604645\n",
      "Epoch 4/100 Evaluation Batch 1301/1758 loss 2.4592883586883545 accuracy: 0.36445313692092896\n",
      "Epoch 4/100 Evaluation Batch 1401/1758 loss 2.5540788173675537 accuracy: 0.353515625\n",
      "Epoch 4/100 Evaluation Batch 1501/1758 loss 2.5005037784576416 accuracy: 0.3589843809604645\n",
      "Epoch 4/100 Evaluation Batch 1601/1758 loss 2.570058584213257 accuracy: 0.34941408038139343\n",
      "Epoch 4/100 Evaluation Batch 1701/1758 loss 2.589046001434326 accuracy: 0.35527345538139343\n",
      "Epoch 5/100 Training Batch 1/1758 loss 0.7140631675720215 accuracy: 0.6751953363418579\n",
      "Epoch 5/100 Training Batch 101/1758 loss 0.6460458040237427 accuracy: 0.7171875238418579\n",
      "Epoch 5/100 Training Batch 201/1758 loss 0.6746425628662109 accuracy: 0.702929675579071\n",
      "Epoch 5/100 Training Batch 301/1758 loss 0.7394289970397949 accuracy: 0.6767578125\n",
      "Epoch 5/100 Training Batch 401/1758 loss 0.650341808795929 accuracy: 0.7177734375\n",
      "Epoch 5/100 Training Batch 501/1758 loss 0.8105449080467224 accuracy: 0.6298828125\n",
      "Epoch 5/100 Training Batch 601/1758 loss 0.6658809781074524 accuracy: 0.7064453363418579\n",
      "Epoch 5/100 Training Batch 701/1758 loss 0.6413946747779846 accuracy: 0.7181640863418579\n",
      "Epoch 5/100 Training Batch 801/1758 loss 1.162675380706787 accuracy: 0.4927734434604645\n",
      "Epoch 5/100 Training Batch 901/1758 loss 0.7010514736175537 accuracy: 0.6957031488418579\n",
      "Epoch 5/100 Training Batch 1001/1758 loss 0.6415389180183411 accuracy: 0.719921886920929\n",
      "Epoch 5/100 Training Batch 1101/1758 loss 0.6236213445663452 accuracy: 0.7359375357627869\n",
      "Epoch 5/100 Training Batch 1201/1758 loss 0.6525775790214539 accuracy: 0.7154297232627869\n",
      "Epoch 5/100 Training Batch 1301/1758 loss 0.7235450744628906 accuracy: 0.6732422113418579\n",
      "Epoch 5/100 Training Batch 1401/1758 loss 0.621839702129364 accuracy: 0.7259765863418579\n",
      "Epoch 5/100 Training Batch 1501/1758 loss 0.6072041392326355 accuracy: 0.728710949420929\n",
      "Epoch 5/100 Training Batch 1601/1758 loss 0.5997651815414429 accuracy: 0.7337890863418579\n",
      "Epoch 5/100 Training Batch 1701/1758 loss 0.7331535220146179 accuracy: 0.6763672232627869\n",
      "Epoch 5/100 Evaluation Batch 1/1758 loss 2.3763582706451416 accuracy: 0.36542969942092896\n",
      "Epoch 5/100 Evaluation Batch 101/1758 loss 2.340928554534912 accuracy: 0.3783203065395355\n",
      "Epoch 5/100 Evaluation Batch 201/1758 loss 2.392408609390259 accuracy: 0.35468751192092896\n",
      "Epoch 5/100 Evaluation Batch 301/1758 loss 2.4087367057800293 accuracy: 0.3656249940395355\n",
      "Epoch 5/100 Evaluation Batch 401/1758 loss 2.428300142288208 accuracy: 0.34882813692092896\n",
      "Epoch 5/100 Evaluation Batch 501/1758 loss 2.362147331237793 accuracy: 0.3580078184604645\n",
      "Epoch 5/100 Evaluation Batch 601/1758 loss 2.428511142730713 accuracy: 0.3453125059604645\n",
      "Epoch 5/100 Evaluation Batch 701/1758 loss 2.4109246730804443 accuracy: 0.3570312559604645\n",
      "Epoch 5/100 Evaluation Batch 801/1758 loss 2.399153470993042 accuracy: 0.3617187440395355\n",
      "Epoch 5/100 Evaluation Batch 901/1758 loss 2.397150993347168 accuracy: 0.3701171875\n",
      "Epoch 5/100 Evaluation Batch 1001/1758 loss 2.5814898014068604 accuracy: 0.32929688692092896\n",
      "Epoch 5/100 Evaluation Batch 1101/1758 loss 2.3781063556671143 accuracy: 0.3589843809604645\n",
      "Epoch 5/100 Evaluation Batch 1201/1758 loss 2.398961305618286 accuracy: 0.37285158038139343\n",
      "Epoch 5/100 Evaluation Batch 1301/1758 loss 2.4730417728424072 accuracy: 0.3531250059604645\n",
      "Epoch 5/100 Evaluation Batch 1401/1758 loss 2.4935905933380127 accuracy: 0.34843751788139343\n",
      "Epoch 5/100 Evaluation Batch 1501/1758 loss 2.44820237159729 accuracy: 0.35546875\n",
      "Epoch 5/100 Evaluation Batch 1601/1758 loss 2.372868061065674 accuracy: 0.3580078184604645\n",
      "Epoch 5/100 Evaluation Batch 1701/1758 loss 2.4541194438934326 accuracy: 0.3564453125\n",
      "Epoch 6/100 Training Batch 1/1758 loss 0.6946535706520081 accuracy: 0.697460949420929\n",
      "Epoch 6/100 Training Batch 101/1758 loss 0.6551271080970764 accuracy: 0.7181640863418579\n",
      "Epoch 6/100 Training Batch 201/1758 loss 0.6370223164558411 accuracy: 0.7162109613418579\n",
      "Epoch 6/100 Training Batch 301/1758 loss 0.6247478723526001 accuracy: 0.7255859375\n",
      "Epoch 6/100 Training Batch 401/1758 loss 0.6106446385383606 accuracy: 0.7294921875\n",
      "Epoch 6/100 Training Batch 501/1758 loss 0.6460902094841003 accuracy: 0.7183594107627869\n",
      "Epoch 6/100 Training Batch 601/1758 loss 0.7380359768867493 accuracy: 0.679492175579071\n",
      "Epoch 6/100 Training Batch 701/1758 loss 0.6157435178756714 accuracy: 0.7320312857627869\n",
      "Epoch 6/100 Training Batch 801/1758 loss 0.6123623847961426 accuracy: 0.728710949420929\n",
      "Epoch 6/100 Training Batch 901/1758 loss 0.6155839562416077 accuracy: 0.7320312857627869\n",
      "Epoch 6/100 Training Batch 1001/1758 loss 0.6035921573638916 accuracy: 0.7347656488418579\n",
      "Epoch 6/100 Training Batch 1101/1758 loss 0.6068494319915771 accuracy: 0.728320300579071\n",
      "Epoch 6/100 Training Batch 1201/1758 loss 0.6014989018440247 accuracy: 0.7347656488418579\n",
      "Epoch 6/100 Training Batch 1301/1758 loss 0.5921703577041626 accuracy: 0.7435547113418579\n",
      "Epoch 6/100 Training Batch 1401/1758 loss 0.5934098958969116 accuracy: 0.7376953363418579\n",
      "Epoch 6/100 Training Batch 1501/1758 loss 0.5952480435371399 accuracy: 0.7349609732627869\n",
      "Epoch 6/100 Training Batch 1601/1758 loss 0.6282786130905151 accuracy: 0.72265625\n",
      "Epoch 6/100 Training Batch 1701/1758 loss 0.6224451661109924 accuracy: 0.723437488079071\n",
      "Epoch 6/100 Evaluation Batch 1/1758 loss 2.687723159790039 accuracy: 0.35332033038139343\n",
      "Epoch 6/100 Evaluation Batch 101/1758 loss 2.526433229446411 accuracy: 0.37968751788139343\n",
      "Epoch 6/100 Evaluation Batch 201/1758 loss 2.647547483444214 accuracy: 0.3716796934604645\n",
      "Epoch 6/100 Evaluation Batch 301/1758 loss 2.5716536045074463 accuracy: 0.3843750059604645\n",
      "Epoch 6/100 Evaluation Batch 401/1758 loss 2.727825164794922 accuracy: 0.36328125\n",
      "Epoch 6/100 Evaluation Batch 501/1758 loss 2.642643928527832 accuracy: 0.36250001192092896\n",
      "Epoch 6/100 Evaluation Batch 601/1758 loss 2.6723825931549072 accuracy: 0.36347657442092896\n",
      "Epoch 6/100 Evaluation Batch 701/1758 loss 2.6135475635528564 accuracy: 0.38261720538139343\n",
      "Epoch 6/100 Evaluation Batch 801/1758 loss 2.658498764038086 accuracy: 0.3617187440395355\n",
      "Epoch 6/100 Evaluation Batch 901/1758 loss 2.7353153228759766 accuracy: 0.3623046875\n",
      "Epoch 6/100 Evaluation Batch 1001/1758 loss 2.6716296672821045 accuracy: 0.3677734434604645\n",
      "Epoch 6/100 Evaluation Batch 1101/1758 loss 2.7255074977874756 accuracy: 0.36113283038139343\n",
      "Epoch 6/100 Evaluation Batch 1201/1758 loss 2.6825342178344727 accuracy: 0.35371094942092896\n",
      "Epoch 6/100 Evaluation Batch 1301/1758 loss 2.549143075942993 accuracy: 0.38261720538139343\n",
      "Epoch 6/100 Evaluation Batch 1401/1758 loss 2.6166837215423584 accuracy: 0.38593751192092896\n",
      "Epoch 6/100 Evaluation Batch 1501/1758 loss 2.627762794494629 accuracy: 0.3833984434604645\n",
      "Epoch 6/100 Evaluation Batch 1601/1758 loss 2.6570959091186523 accuracy: 0.3759765625\n",
      "Epoch 6/100 Evaluation Batch 1701/1758 loss 2.7046515941619873 accuracy: 0.36699220538139343\n",
      "Epoch 7/100 Training Batch 1/1758 loss 0.6009790301322937 accuracy: 0.733593761920929\n",
      "Epoch 7/100 Training Batch 101/1758 loss 0.6057194471359253 accuracy: 0.732226550579071\n",
      "Epoch 7/100 Training Batch 201/1758 loss 0.5847020745277405 accuracy: 0.7470703125\n",
      "Epoch 7/100 Training Batch 301/1758 loss 0.5993097424507141 accuracy: 0.737109363079071\n",
      "Epoch 7/100 Training Batch 401/1758 loss 0.7632817625999451 accuracy: 0.666015625\n",
      "Epoch 7/100 Training Batch 501/1758 loss 0.7121922969818115 accuracy: 0.683398425579071\n",
      "Epoch 7/100 Training Batch 601/1758 loss 0.6559646725654602 accuracy: 0.709179699420929\n",
      "Epoch 7/100 Training Batch 701/1758 loss 0.6410941481590271 accuracy: 0.7125000357627869\n",
      "Epoch 7/100 Training Batch 801/1758 loss 0.6487916111946106 accuracy: 0.712109386920929\n",
      "Epoch 7/100 Training Batch 901/1758 loss 0.6744842529296875 accuracy: 0.702343761920929\n",
      "Epoch 7/100 Training Batch 1001/1758 loss 0.664842963218689 accuracy: 0.7132812738418579\n",
      "Epoch 7/100 Training Batch 1101/1758 loss 0.6316288113594055 accuracy: 0.7259765863418579\n",
      "Epoch 7/100 Training Batch 1201/1758 loss 0.6076000332832336 accuracy: 0.7392578125\n",
      "Epoch 7/100 Training Batch 1301/1758 loss 0.5974854826927185 accuracy: 0.740429699420929\n",
      "Epoch 7/100 Training Batch 1401/1758 loss 0.6323448419570923 accuracy: 0.7294921875\n",
      "Epoch 7/100 Training Batch 1501/1758 loss 0.6400715112686157 accuracy: 0.723437488079071\n",
      "Epoch 7/100 Training Batch 1601/1758 loss 0.6364534497261047 accuracy: 0.726367175579071\n",
      "Epoch 7/100 Training Batch 1701/1758 loss 0.7346577644348145 accuracy: 0.6767578125\n",
      "Epoch 7/100 Evaluation Batch 1/1758 loss 2.5240204334259033 accuracy: 0.37226563692092896\n",
      "Epoch 7/100 Evaluation Batch 101/1758 loss 2.5596072673797607 accuracy: 0.36210939288139343\n",
      "Epoch 7/100 Evaluation Batch 201/1758 loss 2.4767205715179443 accuracy: 0.38164064288139343\n",
      "Epoch 7/100 Evaluation Batch 301/1758 loss 2.5551860332489014 accuracy: 0.35234376788139343\n",
      "Epoch 7/100 Evaluation Batch 401/1758 loss 2.4843690395355225 accuracy: 0.3707031309604645\n",
      "Epoch 7/100 Evaluation Batch 501/1758 loss 2.4816019535064697 accuracy: 0.3677734434604645\n",
      "Epoch 7/100 Evaluation Batch 601/1758 loss 2.5393829345703125 accuracy: 0.36152344942092896\n",
      "Epoch 7/100 Evaluation Batch 701/1758 loss 2.7054014205932617 accuracy: 0.3539062440395355\n",
      "Epoch 7/100 Evaluation Batch 801/1758 loss 2.555783987045288 accuracy: 0.361328125\n",
      "Epoch 7/100 Evaluation Batch 901/1758 loss 2.5299887657165527 accuracy: 0.37128907442092896\n",
      "Epoch 7/100 Evaluation Batch 1001/1758 loss 2.6193478107452393 accuracy: 0.3550781309604645\n",
      "Epoch 7/100 Evaluation Batch 1101/1758 loss 2.527613401412964 accuracy: 0.36250001192092896\n",
      "Epoch 7/100 Evaluation Batch 1201/1758 loss 2.5149893760681152 accuracy: 0.3734374940395355\n",
      "Epoch 7/100 Evaluation Batch 1301/1758 loss 2.5807735919952393 accuracy: 0.36113283038139343\n",
      "Epoch 7/100 Evaluation Batch 1401/1758 loss 2.5791141986846924 accuracy: 0.36835938692092896\n",
      "Epoch 7/100 Evaluation Batch 1501/1758 loss 2.5082778930664062 accuracy: 0.36738282442092896\n",
      "Epoch 7/100 Evaluation Batch 1601/1758 loss 2.5791637897491455 accuracy: 0.36347657442092896\n",
      "Epoch 7/100 Evaluation Batch 1701/1758 loss 2.547302484512329 accuracy: 0.3558593690395355\n",
      "Epoch 8/100 Training Batch 1/1758 loss 0.6846840381622314 accuracy: 0.700976550579071\n",
      "Epoch 8/100 Training Batch 101/1758 loss 0.6420974135398865 accuracy: 0.73046875\n",
      "Epoch 8/100 Training Batch 201/1758 loss 0.6291133165359497 accuracy: 0.722460925579071\n",
      "Epoch 8/100 Training Batch 301/1758 loss 1.0991765260696411 accuracy: 0.5425781607627869\n",
      "Epoch 8/100 Training Batch 401/1758 loss 0.6071708798408508 accuracy: 0.741015613079071\n",
      "Epoch 8/100 Training Batch 501/1758 loss 0.6870620846748352 accuracy: 0.6982421875\n",
      "Epoch 8/100 Training Batch 601/1758 loss 0.6194326281547546 accuracy: 0.726757824420929\n",
      "Epoch 8/100 Training Batch 701/1758 loss 0.6326932311058044 accuracy: 0.7191406488418579\n",
      "Epoch 8/100 Training Batch 801/1758 loss 0.5968123078346252 accuracy: 0.7396484613418579\n",
      "Epoch 8/100 Training Batch 901/1758 loss 0.6156925559043884 accuracy: 0.728320300579071\n",
      "Epoch 8/100 Training Batch 1001/1758 loss 0.6179719567298889 accuracy: 0.7349609732627869\n",
      "Epoch 8/100 Training Batch 1101/1758 loss 0.7441885471343994 accuracy: 0.6705078482627869\n",
      "Epoch 8/100 Training Batch 1201/1758 loss 0.6093952059745789 accuracy: 0.7392578125\n",
      "Epoch 8/100 Training Batch 1301/1758 loss 0.6060931086540222 accuracy: 0.736328125\n",
      "Epoch 8/100 Training Batch 1401/1758 loss 0.7746164798736572 accuracy: 0.669140636920929\n",
      "Epoch 8/100 Training Batch 1501/1758 loss 0.6659537553787231 accuracy: 0.712695300579071\n",
      "Epoch 8/100 Training Batch 1601/1758 loss 0.7229886054992676 accuracy: 0.681835949420929\n",
      "Epoch 8/100 Training Batch 1701/1758 loss 0.641454815864563 accuracy: 0.716015636920929\n",
      "Epoch 8/100 Evaluation Batch 1/1758 loss 2.6531970500946045 accuracy: 0.3705078065395355\n",
      "Epoch 8/100 Evaluation Batch 101/1758 loss 2.6662509441375732 accuracy: 0.37480470538139343\n",
      "Epoch 8/100 Evaluation Batch 201/1758 loss 2.606163501739502 accuracy: 0.38398438692092896\n",
      "Epoch 8/100 Evaluation Batch 301/1758 loss 2.8192031383514404 accuracy: 0.3583984375\n",
      "Epoch 8/100 Evaluation Batch 401/1758 loss 2.684709310531616 accuracy: 0.37226563692092896\n",
      "Epoch 8/100 Evaluation Batch 501/1758 loss 2.66318678855896 accuracy: 0.3882812559604645\n",
      "Epoch 8/100 Evaluation Batch 601/1758 loss 2.6272799968719482 accuracy: 0.37617188692092896\n",
      "Epoch 8/100 Evaluation Batch 701/1758 loss 2.5895192623138428 accuracy: 0.3984375\n",
      "Epoch 8/100 Evaluation Batch 801/1758 loss 2.743574857711792 accuracy: 0.35957032442092896\n",
      "Epoch 8/100 Evaluation Batch 901/1758 loss 2.620983362197876 accuracy: 0.37675783038139343\n",
      "Epoch 8/100 Evaluation Batch 1001/1758 loss 2.71958327293396 accuracy: 0.37773439288139343\n",
      "Epoch 8/100 Evaluation Batch 1101/1758 loss 2.6734838485717773 accuracy: 0.3802734315395355\n",
      "Epoch 8/100 Evaluation Batch 1201/1758 loss 2.606795072555542 accuracy: 0.3792968690395355\n",
      "Epoch 8/100 Evaluation Batch 1301/1758 loss 2.7906277179718018 accuracy: 0.3599609434604645\n",
      "Epoch 8/100 Evaluation Batch 1401/1758 loss 2.6801185607910156 accuracy: 0.37714844942092896\n",
      "Epoch 8/100 Evaluation Batch 1501/1758 loss 2.6229870319366455 accuracy: 0.3851562440395355\n",
      "Epoch 8/100 Evaluation Batch 1601/1758 loss 2.7204456329345703 accuracy: 0.3667968809604645\n",
      "Epoch 8/100 Evaluation Batch 1701/1758 loss 2.692456007003784 accuracy: 0.37226563692092896\n",
      "Epoch 9/100 Training Batch 1/1758 loss 0.6050212979316711 accuracy: 0.744921863079071\n",
      "Epoch 9/100 Training Batch 101/1758 loss 0.6068939566612244 accuracy: 0.7310547232627869\n",
      "Epoch 9/100 Training Batch 201/1758 loss 0.5940045714378357 accuracy: 0.742968738079071\n",
      "Epoch 9/100 Training Batch 301/1758 loss 0.6416621804237366 accuracy: 0.721875011920929\n",
      "Epoch 9/100 Training Batch 401/1758 loss 0.5970748066902161 accuracy: 0.7464843988418579\n",
      "Epoch 9/100 Training Batch 501/1758 loss 0.6213560700416565 accuracy: 0.728320300579071\n",
      "Epoch 9/100 Training Batch 601/1758 loss 0.6076183915138245 accuracy: 0.739062488079071\n",
      "Epoch 9/100 Training Batch 701/1758 loss 0.6023009419441223 accuracy: 0.740234375\n",
      "Epoch 9/100 Training Batch 801/1758 loss 0.5872431397438049 accuracy: 0.7451171875\n",
      "Epoch 9/100 Training Batch 901/1758 loss 0.6009559631347656 accuracy: 0.736523449420929\n",
      "Epoch 9/100 Training Batch 1001/1758 loss 0.5960803031921387 accuracy: 0.739453136920929\n",
      "Epoch 9/100 Training Batch 1101/1758 loss 0.600516140460968 accuracy: 0.7378906607627869\n",
      "Epoch 9/100 Training Batch 1201/1758 loss 0.6090670824050903 accuracy: 0.741015613079071\n",
      "Epoch 9/100 Training Batch 1301/1758 loss 0.6050092577934265 accuracy: 0.736132800579071\n",
      "Epoch 9/100 Training Batch 1401/1758 loss 1.0919826030731201 accuracy: 0.529101550579071\n",
      "Epoch 9/100 Training Batch 1501/1758 loss 0.5933030247688293 accuracy: 0.741992175579071\n",
      "Epoch 9/100 Training Batch 1601/1758 loss 0.5882286429405212 accuracy: 0.7503906488418579\n",
      "Epoch 9/100 Training Batch 1701/1758 loss 0.633186399936676 accuracy: 0.7236328125\n",
      "Epoch 9/100 Evaluation Batch 1/1758 loss 2.447064161300659 accuracy: 0.4164062440395355\n",
      "Epoch 9/100 Evaluation Batch 101/1758 loss 2.716813802719116 accuracy: 0.37675783038139343\n",
      "Epoch 9/100 Evaluation Batch 201/1758 loss 2.6905319690704346 accuracy: 0.3701171875\n",
      "Epoch 9/100 Evaluation Batch 301/1758 loss 2.7129480838775635 accuracy: 0.3701171875\n",
      "Epoch 9/100 Evaluation Batch 401/1758 loss 2.771034002304077 accuracy: 0.3656249940395355\n",
      "Epoch 9/100 Evaluation Batch 501/1758 loss 2.7027552127838135 accuracy: 0.3705078065395355\n",
      "Epoch 9/100 Evaluation Batch 601/1758 loss 2.628979206085205 accuracy: 0.3984375\n",
      "Epoch 9/100 Evaluation Batch 701/1758 loss 2.6931779384613037 accuracy: 0.3720703125\n",
      "Epoch 9/100 Evaluation Batch 801/1758 loss 2.6167333126068115 accuracy: 0.37714844942092896\n",
      "Epoch 9/100 Evaluation Batch 901/1758 loss 2.642096757888794 accuracy: 0.3851562440395355\n",
      "Epoch 9/100 Evaluation Batch 1001/1758 loss 2.774076223373413 accuracy: 0.3671875\n",
      "Epoch 9/100 Evaluation Batch 1101/1758 loss 2.672703504562378 accuracy: 0.38359376788139343\n",
      "Epoch 9/100 Evaluation Batch 1201/1758 loss 2.6784465312957764 accuracy: 0.3818359375\n",
      "Epoch 9/100 Evaluation Batch 1301/1758 loss 2.764248847961426 accuracy: 0.3548828065395355\n",
      "Epoch 9/100 Evaluation Batch 1401/1758 loss 2.7646617889404297 accuracy: 0.3740234375\n",
      "Epoch 9/100 Evaluation Batch 1501/1758 loss 2.6536660194396973 accuracy: 0.38007813692092896\n",
      "Epoch 9/100 Evaluation Batch 1601/1758 loss 2.8117120265960693 accuracy: 0.35917970538139343\n",
      "Epoch 9/100 Evaluation Batch 1701/1758 loss 2.703963041305542 accuracy: 0.37031251192092896\n",
      "Epoch 10/100 Training Batch 1/1758 loss 0.5905185341835022 accuracy: 0.750195324420929\n",
      "Epoch 10/100 Training Batch 101/1758 loss 0.632925271987915 accuracy: 0.731640636920929\n",
      "Epoch 10/100 Training Batch 201/1758 loss 0.5873909592628479 accuracy: 0.7447265982627869\n",
      "Epoch 10/100 Training Batch 301/1758 loss 0.5760872960090637 accuracy: 0.753125011920929\n",
      "Epoch 10/100 Training Batch 401/1758 loss 0.6071140170097351 accuracy: 0.733203113079071\n",
      "Epoch 10/100 Training Batch 501/1758 loss 0.5869202017784119 accuracy: 0.745898425579071\n",
      "Epoch 10/100 Training Batch 601/1758 loss 0.5947299599647522 accuracy: 0.749804675579071\n",
      "Epoch 10/100 Training Batch 701/1758 loss 0.5636947154998779 accuracy: 0.749218761920929\n",
      "Epoch 10/100 Training Batch 801/1758 loss 0.5513620376586914 accuracy: 0.7554687857627869\n",
      "Epoch 10/100 Training Batch 901/1758 loss 0.5576648712158203 accuracy: 0.7562500238418579\n",
      "Epoch 10/100 Training Batch 1001/1758 loss 0.6904894709587097 accuracy: 0.688671886920929\n",
      "Epoch 10/100 Training Batch 1101/1758 loss 0.7789081931114197 accuracy: 0.653124988079071\n",
      "Epoch 10/100 Training Batch 1201/1758 loss 0.6618505120277405 accuracy: 0.705859363079071\n",
      "Epoch 10/100 Training Batch 1301/1758 loss 0.5693480372428894 accuracy: 0.7494140863418579\n",
      "Epoch 10/100 Training Batch 1401/1758 loss 0.5867632627487183 accuracy: 0.742382824420929\n",
      "Epoch 10/100 Training Batch 1501/1758 loss 0.5668487548828125 accuracy: 0.750195324420929\n",
      "Epoch 10/100 Training Batch 1601/1758 loss 0.5717002153396606 accuracy: 0.7427734732627869\n",
      "Epoch 10/100 Training Batch 1701/1758 loss 0.5700722336769104 accuracy: 0.75390625\n",
      "Epoch 10/100 Evaluation Batch 1/1758 loss 3.0497725009918213 accuracy: 0.3802734315395355\n",
      "Epoch 10/100 Evaluation Batch 101/1758 loss 3.070791006088257 accuracy: 0.3792968690395355\n",
      "Epoch 10/100 Evaluation Batch 201/1758 loss 3.1503489017486572 accuracy: 0.37109375\n",
      "Epoch 10/100 Evaluation Batch 301/1758 loss 3.0230648517608643 accuracy: 0.3814453184604645\n",
      "Epoch 10/100 Evaluation Batch 401/1758 loss 3.058457136154175 accuracy: 0.3814453184604645\n",
      "Epoch 10/100 Evaluation Batch 501/1758 loss 3.0241260528564453 accuracy: 0.3759765625\n",
      "Epoch 10/100 Evaluation Batch 601/1758 loss 3.09051251411438 accuracy: 0.3824218809604645\n",
      "Epoch 10/100 Evaluation Batch 701/1758 loss 3.2037057876586914 accuracy: 0.37480470538139343\n",
      "Epoch 10/100 Evaluation Batch 801/1758 loss 3.1209475994110107 accuracy: 0.3775390684604645\n",
      "Epoch 10/100 Evaluation Batch 901/1758 loss 3.0572023391723633 accuracy: 0.38359376788139343\n",
      "Epoch 10/100 Evaluation Batch 1001/1758 loss 2.886822462081909 accuracy: 0.3970703184604645\n",
      "Epoch 10/100 Evaluation Batch 1101/1758 loss 3.255038261413574 accuracy: 0.353515625\n",
      "Epoch 10/100 Evaluation Batch 1201/1758 loss 3.080552816390991 accuracy: 0.376953125\n",
      "Epoch 10/100 Evaluation Batch 1301/1758 loss 3.188556671142578 accuracy: 0.36210939288139343\n",
      "Epoch 10/100 Evaluation Batch 1401/1758 loss 3.177964210510254 accuracy: 0.36113283038139343\n",
      "Epoch 10/100 Evaluation Batch 1501/1758 loss 3.0770905017852783 accuracy: 0.3677734434604645\n",
      "Epoch 10/100 Evaluation Batch 1601/1758 loss 2.9751827716827393 accuracy: 0.39960938692092896\n",
      "Epoch 10/100 Evaluation Batch 1701/1758 loss 3.1730692386627197 accuracy: 0.3636718690395355\n",
      "Epoch 11/100 Training Batch 1/1758 loss 0.6298695802688599 accuracy: 0.721484363079071\n",
      "Epoch 11/100 Training Batch 101/1758 loss 0.5743749737739563 accuracy: 0.743945300579071\n",
      "Epoch 11/100 Training Batch 201/1758 loss 0.6011047959327698 accuracy: 0.7474609613418579\n",
      "Epoch 11/100 Training Batch 301/1758 loss 0.5809031128883362 accuracy: 0.7451171875\n",
      "Epoch 11/100 Training Batch 401/1758 loss 0.5839342474937439 accuracy: 0.7505859732627869\n",
      "Epoch 11/100 Training Batch 501/1758 loss 0.6802965998649597 accuracy: 0.701367199420929\n",
      "Epoch 11/100 Training Batch 601/1758 loss 0.6366001963615417 accuracy: 0.7298828363418579\n",
      "Epoch 11/100 Training Batch 701/1758 loss 0.6121151447296143 accuracy: 0.727343738079071\n",
      "Epoch 11/100 Training Batch 801/1758 loss 0.6293416023254395 accuracy: 0.7271484732627869\n",
      "Epoch 11/100 Training Batch 901/1758 loss 0.5566641688346863 accuracy: 0.7583984732627869\n",
      "Epoch 11/100 Training Batch 1001/1758 loss 0.9098933339118958 accuracy: 0.609570324420929\n",
      "Epoch 11/100 Training Batch 1101/1758 loss 0.7858417630195618 accuracy: 0.664843738079071\n",
      "Epoch 11/100 Training Batch 1201/1758 loss 0.7482657432556152 accuracy: 0.6787109375\n",
      "Epoch 11/100 Training Batch 1301/1758 loss 0.7026974558830261 accuracy: 0.697265625\n",
      "Epoch 11/100 Training Batch 1401/1758 loss 0.6835969090461731 accuracy: 0.70703125\n",
      "Epoch 11/100 Training Batch 1501/1758 loss 0.7011594772338867 accuracy: 0.698046863079071\n",
      "Epoch 11/100 Training Batch 1601/1758 loss 0.6576352119445801 accuracy: 0.7154297232627869\n",
      "Epoch 11/100 Training Batch 1701/1758 loss 0.6822264194488525 accuracy: 0.709179699420929\n",
      "Epoch 11/100 Evaluation Batch 1/1758 loss 2.4633119106292725 accuracy: 0.3656249940395355\n",
      "Epoch 11/100 Evaluation Batch 101/1758 loss 2.379915714263916 accuracy: 0.3900390565395355\n",
      "Epoch 11/100 Evaluation Batch 201/1758 loss 2.5011866092681885 accuracy: 0.3662109375\n",
      "Epoch 11/100 Evaluation Batch 301/1758 loss 2.4077298641204834 accuracy: 0.3726562559604645\n",
      "Epoch 11/100 Evaluation Batch 401/1758 loss 2.4679510593414307 accuracy: 0.3658203184604645\n",
      "Epoch 11/100 Evaluation Batch 501/1758 loss 2.381247043609619 accuracy: 0.38652345538139343\n",
      "Epoch 11/100 Evaluation Batch 601/1758 loss 2.575648546218872 accuracy: 0.3541015684604645\n",
      "Epoch 11/100 Evaluation Batch 701/1758 loss 2.540242910385132 accuracy: 0.3609375059604645\n",
      "Epoch 11/100 Evaluation Batch 801/1758 loss 2.395014524459839 accuracy: 0.38750001788139343\n",
      "Epoch 11/100 Evaluation Batch 901/1758 loss 2.488906145095825 accuracy: 0.36640626192092896\n",
      "Epoch 11/100 Evaluation Batch 1001/1758 loss 2.4658515453338623 accuracy: 0.3656249940395355\n",
      "Epoch 11/100 Evaluation Batch 1101/1758 loss 2.4301741123199463 accuracy: 0.3740234375\n",
      "Epoch 11/100 Evaluation Batch 1201/1758 loss 2.434859037399292 accuracy: 0.37382814288139343\n",
      "Epoch 11/100 Evaluation Batch 1301/1758 loss 2.5697853565216064 accuracy: 0.35468751192092896\n",
      "Epoch 11/100 Evaluation Batch 1401/1758 loss 2.611158609390259 accuracy: 0.35664063692092896\n",
      "Epoch 11/100 Evaluation Batch 1501/1758 loss 2.358876943588257 accuracy: 0.3837890625\n",
      "Epoch 11/100 Evaluation Batch 1601/1758 loss 2.3268420696258545 accuracy: 0.37714844942092896\n",
      "Epoch 11/100 Evaluation Batch 1701/1758 loss 2.4701602458953857 accuracy: 0.3648437559604645\n",
      "Epoch 12/100 Training Batch 1/1758 loss 0.646151602268219 accuracy: 0.724414050579071\n",
      "Epoch 12/100 Training Batch 101/1758 loss 0.6361603140830994 accuracy: 0.7242187857627869\n",
      "Epoch 12/100 Training Batch 201/1758 loss 0.6440063714981079 accuracy: 0.7328125238418579\n",
      "Epoch 12/100 Training Batch 301/1758 loss 0.6043607592582703 accuracy: 0.739453136920929\n",
      "Epoch 12/100 Training Batch 401/1758 loss 0.63675457239151 accuracy: 0.727734386920929\n",
      "Epoch 12/100 Training Batch 501/1758 loss 0.6190037131309509 accuracy: 0.7435547113418579\n",
      "Epoch 12/100 Training Batch 601/1758 loss 0.6273709535598755 accuracy: 0.7294921875\n",
      "Epoch 12/100 Training Batch 701/1758 loss 0.7154029011726379 accuracy: 0.684374988079071\n",
      "Epoch 12/100 Training Batch 801/1758 loss 0.6013869643211365 accuracy: 0.7386718988418579\n",
      "Epoch 12/100 Training Batch 901/1758 loss 0.6096190810203552 accuracy: 0.7367187738418579\n",
      "Epoch 12/100 Training Batch 1001/1758 loss 0.6063951253890991 accuracy: 0.73828125\n",
      "Epoch 12/100 Training Batch 1101/1758 loss 0.9568943977355957 accuracy: 0.5986328125\n",
      "Epoch 12/100 Training Batch 1201/1758 loss 0.5930712819099426 accuracy: 0.7578125\n",
      "Epoch 12/100 Training Batch 1301/1758 loss 0.5855692028999329 accuracy: 0.752734363079071\n",
      "Epoch 12/100 Training Batch 1401/1758 loss 0.602869987487793 accuracy: 0.744335949420929\n",
      "Epoch 12/100 Training Batch 1501/1758 loss 0.6530525088310242 accuracy: 0.7123047113418579\n",
      "Epoch 12/100 Training Batch 1601/1758 loss 0.596476137638092 accuracy: 0.7447265982627869\n",
      "Epoch 12/100 Training Batch 1701/1758 loss 0.5638994574546814 accuracy: 0.7607421875\n",
      "Epoch 12/100 Evaluation Batch 1/1758 loss 2.7218570709228516 accuracy: 0.3529296815395355\n",
      "Epoch 12/100 Evaluation Batch 101/1758 loss 2.6066770553588867 accuracy: 0.36738282442092896\n",
      "Epoch 12/100 Evaluation Batch 201/1758 loss 2.6841232776641846 accuracy: 0.35273438692092896\n",
      "Epoch 12/100 Evaluation Batch 301/1758 loss 2.669567108154297 accuracy: 0.35625001788139343\n",
      "Epoch 12/100 Evaluation Batch 401/1758 loss 2.706041097640991 accuracy: 0.3617187440395355\n",
      "Epoch 12/100 Evaluation Batch 501/1758 loss 2.7648050785064697 accuracy: 0.359375\n",
      "Epoch 12/100 Evaluation Batch 601/1758 loss 2.7471349239349365 accuracy: 0.3636718690395355\n",
      "Epoch 12/100 Evaluation Batch 701/1758 loss 2.587150812149048 accuracy: 0.39238283038139343\n",
      "Epoch 12/100 Evaluation Batch 801/1758 loss 2.6372082233428955 accuracy: 0.36328125\n",
      "Epoch 12/100 Evaluation Batch 901/1758 loss 2.509399175643921 accuracy: 0.3880859315395355\n",
      "Epoch 12/100 Evaluation Batch 1001/1758 loss 2.6157402992248535 accuracy: 0.3681640625\n",
      "Epoch 12/100 Evaluation Batch 1101/1758 loss 2.6098172664642334 accuracy: 0.3744140565395355\n",
      "Epoch 12/100 Evaluation Batch 1201/1758 loss 2.6072165966033936 accuracy: 0.37187501788139343\n",
      "Epoch 12/100 Evaluation Batch 1301/1758 loss 2.668881416320801 accuracy: 0.3638671934604645\n",
      "Epoch 12/100 Evaluation Batch 1401/1758 loss 2.6075246334075928 accuracy: 0.37910157442092896\n",
      "Epoch 12/100 Evaluation Batch 1501/1758 loss 2.6594784259796143 accuracy: 0.3642578125\n",
      "Epoch 12/100 Evaluation Batch 1601/1758 loss 2.7536027431488037 accuracy: 0.36250001192092896\n",
      "Epoch 12/100 Evaluation Batch 1701/1758 loss 2.7340505123138428 accuracy: 0.3560546934604645\n",
      "Epoch 13/100 Training Batch 1/1758 loss 0.6171230673789978 accuracy: 0.7265625\n",
      "Epoch 13/100 Training Batch 101/1758 loss 0.5947322845458984 accuracy: 0.741992175579071\n",
      "Epoch 13/100 Training Batch 201/1758 loss 0.5772437453269958 accuracy: 0.752734363079071\n",
      "Epoch 13/100 Training Batch 301/1758 loss 0.5744630694389343 accuracy: 0.7496094107627869\n",
      "Epoch 13/100 Training Batch 401/1758 loss 0.5815693736076355 accuracy: 0.748828113079071\n",
      "Epoch 13/100 Training Batch 501/1758 loss 0.6179216504096985 accuracy: 0.733203113079071\n",
      "Epoch 13/100 Training Batch 601/1758 loss 0.5622212290763855 accuracy: 0.7603515982627869\n",
      "Epoch 13/100 Training Batch 701/1758 loss 0.5700920224189758 accuracy: 0.7525390982627869\n",
      "Epoch 13/100 Training Batch 801/1758 loss 0.5796195268630981 accuracy: 0.7523437738418579\n",
      "Epoch 13/100 Training Batch 901/1758 loss 0.5815879702568054 accuracy: 0.7529296875\n",
      "Epoch 13/100 Training Batch 1001/1758 loss 0.7711853981018066 accuracy: 0.6458984613418579\n",
      "Epoch 13/100 Training Batch 1101/1758 loss 0.6959658265113831 accuracy: 0.6796875\n",
      "Epoch 13/100 Training Batch 1201/1758 loss 0.6463610529899597 accuracy: 0.711132824420929\n",
      "Epoch 13/100 Training Batch 1301/1758 loss 0.6278504729270935 accuracy: 0.7171875238418579\n",
      "Epoch 13/100 Training Batch 1401/1758 loss 0.6551974415779114 accuracy: 0.695507824420929\n",
      "Epoch 13/100 Training Batch 1501/1758 loss 0.9659823775291443 accuracy: 0.566601574420929\n",
      "Epoch 13/100 Training Batch 1601/1758 loss 0.6337752938270569 accuracy: 0.714062511920929\n",
      "Epoch 13/100 Training Batch 1701/1758 loss 0.6443544030189514 accuracy: 0.708203136920929\n",
      "Epoch 13/100 Evaluation Batch 1/1758 loss 2.801717519760132 accuracy: 0.36738282442092896\n",
      "Epoch 13/100 Evaluation Batch 101/1758 loss 2.7490341663360596 accuracy: 0.3675781190395355\n",
      "Epoch 13/100 Evaluation Batch 201/1758 loss 2.598198890686035 accuracy: 0.39238283038139343\n",
      "Epoch 13/100 Evaluation Batch 301/1758 loss 2.6797702312469482 accuracy: 0.3822265565395355\n",
      "Epoch 13/100 Evaluation Batch 401/1758 loss 2.763171434402466 accuracy: 0.373046875\n",
      "Epoch 13/100 Evaluation Batch 501/1758 loss 2.82300066947937 accuracy: 0.36894533038139343\n",
      "Epoch 13/100 Evaluation Batch 601/1758 loss 2.7304646968841553 accuracy: 0.38691407442092896\n",
      "Epoch 13/100 Evaluation Batch 701/1758 loss 2.7897872924804688 accuracy: 0.35957032442092896\n",
      "Epoch 13/100 Evaluation Batch 801/1758 loss 2.8025248050689697 accuracy: 0.3734374940395355\n",
      "Epoch 13/100 Evaluation Batch 901/1758 loss 2.914173126220703 accuracy: 0.34746095538139343\n",
      "Epoch 13/100 Evaluation Batch 1001/1758 loss 2.9224913120269775 accuracy: 0.3509765565395355\n",
      "Epoch 13/100 Evaluation Batch 1101/1758 loss 2.710782051086426 accuracy: 0.3837890625\n",
      "Epoch 13/100 Evaluation Batch 1201/1758 loss 2.840791702270508 accuracy: 0.36347657442092896\n",
      "Epoch 13/100 Evaluation Batch 1301/1758 loss 2.7954282760620117 accuracy: 0.3656249940395355\n",
      "Epoch 13/100 Evaluation Batch 1401/1758 loss 2.827080488204956 accuracy: 0.34453126788139343\n",
      "Epoch 13/100 Evaluation Batch 1501/1758 loss 2.6032862663269043 accuracy: 0.3794921934604645\n",
      "Epoch 13/100 Evaluation Batch 1601/1758 loss 2.7758474349975586 accuracy: 0.36835938692092896\n",
      "Epoch 13/100 Evaluation Batch 1701/1758 loss 2.5876669883728027 accuracy: 0.3900390565395355\n",
      "Epoch 14/100 Training Batch 1/1758 loss 0.6468281745910645 accuracy: 0.708789050579071\n",
      "Epoch 14/100 Training Batch 101/1758 loss 0.6162316203117371 accuracy: 0.7310547232627869\n",
      "Epoch 14/100 Training Batch 201/1758 loss 0.6352927684783936 accuracy: 0.7125000357627869\n",
      "Epoch 14/100 Training Batch 301/1758 loss 0.62464839220047 accuracy: 0.725781261920929\n",
      "Epoch 14/100 Training Batch 401/1758 loss 0.5996072292327881 accuracy: 0.735156238079071\n",
      "Epoch 14/100 Training Batch 501/1758 loss 0.9179484844207764 accuracy: 0.5830078125\n",
      "Epoch 14/100 Training Batch 601/1758 loss 0.7376899719238281 accuracy: 0.660937488079071\n",
      "Epoch 14/100 Training Batch 701/1758 loss 0.6898760199546814 accuracy: 0.6841797232627869\n",
      "Epoch 14/100 Training Batch 801/1758 loss 0.6520258188247681 accuracy: 0.706250011920929\n",
      "Epoch 14/100 Training Batch 901/1758 loss 0.6397987604141235 accuracy: 0.725781261920929\n",
      "Epoch 14/100 Training Batch 1001/1758 loss 0.6476297974586487 accuracy: 0.706835925579071\n",
      "Epoch 14/100 Training Batch 1101/1758 loss 0.6896312832832336 accuracy: 0.6988281607627869\n",
      "Epoch 14/100 Training Batch 1201/1758 loss 0.6738993525505066 accuracy: 0.700976550579071\n",
      "Epoch 14/100 Training Batch 1301/1758 loss 0.6822847723960876 accuracy: 0.698437511920929\n",
      "Epoch 14/100 Training Batch 1401/1758 loss 0.6605990529060364 accuracy: 0.699414074420929\n",
      "Epoch 14/100 Training Batch 1501/1758 loss 0.6379268765449524 accuracy: 0.7259765863418579\n",
      "Epoch 14/100 Training Batch 1601/1758 loss 0.6403684020042419 accuracy: 0.721484363079071\n",
      "Epoch 14/100 Training Batch 1701/1758 loss 0.6252509951591492 accuracy: 0.716601550579071\n",
      "Epoch 14/100 Evaluation Batch 1/1758 loss 2.565361976623535 accuracy: 0.3974609375\n",
      "Epoch 14/100 Evaluation Batch 101/1758 loss 2.664198637008667 accuracy: 0.3626953065395355\n",
      "Epoch 14/100 Evaluation Batch 201/1758 loss 2.8462512493133545 accuracy: 0.34003907442092896\n",
      "Epoch 14/100 Evaluation Batch 301/1758 loss 2.612168550491333 accuracy: 0.3910156190395355\n",
      "Epoch 14/100 Evaluation Batch 401/1758 loss 2.5619964599609375 accuracy: 0.396484375\n",
      "Epoch 14/100 Evaluation Batch 501/1758 loss 2.660611391067505 accuracy: 0.37890625\n",
      "Epoch 14/100 Evaluation Batch 601/1758 loss 2.590287923812866 accuracy: 0.39628908038139343\n",
      "Epoch 14/100 Evaluation Batch 701/1758 loss 2.6346232891082764 accuracy: 0.3804687559604645\n",
      "Epoch 14/100 Evaluation Batch 801/1758 loss 2.704904317855835 accuracy: 0.373046875\n",
      "Epoch 14/100 Evaluation Batch 901/1758 loss 2.7844760417938232 accuracy: 0.361328125\n",
      "Epoch 14/100 Evaluation Batch 1001/1758 loss 2.8017075061798096 accuracy: 0.3597656190395355\n",
      "Epoch 14/100 Evaluation Batch 1101/1758 loss 2.729600191116333 accuracy: 0.37226563692092896\n",
      "Epoch 14/100 Evaluation Batch 1201/1758 loss 2.5766336917877197 accuracy: 0.39179688692092896\n",
      "Epoch 14/100 Evaluation Batch 1301/1758 loss 2.6771390438079834 accuracy: 0.3759765625\n",
      "Epoch 14/100 Evaluation Batch 1401/1758 loss 2.6658551692962646 accuracy: 0.3642578125\n",
      "Epoch 14/100 Evaluation Batch 1501/1758 loss 2.651444911956787 accuracy: 0.388671875\n",
      "Epoch 14/100 Evaluation Batch 1601/1758 loss 2.575917959213257 accuracy: 0.38457033038139343\n",
      "Epoch 14/100 Evaluation Batch 1701/1758 loss 2.5660414695739746 accuracy: 0.3880859315395355\n",
      "Epoch 15/100 Training Batch 1/1758 loss 0.6327224373817444 accuracy: 0.7230468988418579\n",
      "Epoch 15/100 Training Batch 101/1758 loss 0.6283079385757446 accuracy: 0.720507800579071\n",
      "Epoch 15/100 Training Batch 201/1758 loss 0.6495023965835571 accuracy: 0.716992199420929\n",
      "Epoch 15/100 Training Batch 301/1758 loss 0.6083793044090271 accuracy: 0.73046875\n",
      "Epoch 15/100 Training Batch 401/1758 loss 0.6174509525299072 accuracy: 0.729687511920929\n",
      "Epoch 15/100 Training Batch 501/1758 loss 0.6117778420448303 accuracy: 0.724609375\n",
      "Epoch 15/100 Training Batch 601/1758 loss 0.6114135384559631 accuracy: 0.726757824420929\n",
      "Epoch 15/100 Training Batch 701/1758 loss 0.6230388879776001 accuracy: 0.720898449420929\n",
      "Epoch 15/100 Training Batch 801/1758 loss 0.6110334992408752 accuracy: 0.7269531488418579\n",
      "Epoch 15/100 Training Batch 901/1758 loss 0.6253724098205566 accuracy: 0.722851574420929\n",
      "Epoch 15/100 Training Batch 1001/1758 loss 0.6575068831443787 accuracy: 0.7044922113418579\n",
      "Epoch 15/100 Training Batch 1101/1758 loss 0.6554499864578247 accuracy: 0.6949219107627869\n",
      "Epoch 15/100 Training Batch 1201/1758 loss 0.6266645789146423 accuracy: 0.7220703363418579\n",
      "Epoch 15/100 Training Batch 1301/1758 loss 0.6167934536933899 accuracy: 0.719531238079071\n",
      "Epoch 15/100 Training Batch 1401/1758 loss 0.6044492125511169 accuracy: 0.728515625\n",
      "Epoch 15/100 Training Batch 1501/1758 loss 0.5954502820968628 accuracy: 0.728320300579071\n",
      "Epoch 15/100 Training Batch 1601/1758 loss 0.7634856700897217 accuracy: 0.6587890982627869\n",
      "Epoch 15/100 Training Batch 1701/1758 loss 0.6089638471603394 accuracy: 0.721875011920929\n",
      "Epoch 15/100 Evaluation Batch 1/1758 loss 2.9479713439941406 accuracy: 0.35820314288139343\n",
      "Epoch 15/100 Evaluation Batch 101/1758 loss 2.7946040630340576 accuracy: 0.39531251788139343\n",
      "Epoch 15/100 Evaluation Batch 201/1758 loss 2.7940571308135986 accuracy: 0.3851562440395355\n",
      "Epoch 15/100 Evaluation Batch 301/1758 loss 2.840528726577759 accuracy: 0.3714843690395355\n",
      "Epoch 15/100 Evaluation Batch 401/1758 loss 2.7437069416046143 accuracy: 0.3814453184604645\n",
      "Epoch 15/100 Evaluation Batch 501/1758 loss 2.8584487438201904 accuracy: 0.36699220538139343\n",
      "Epoch 15/100 Evaluation Batch 601/1758 loss 2.751572370529175 accuracy: 0.390625\n",
      "Epoch 15/100 Evaluation Batch 701/1758 loss 2.7015130519866943 accuracy: 0.3896484375\n",
      "Epoch 15/100 Evaluation Batch 801/1758 loss 2.89700984954834 accuracy: 0.365234375\n",
      "Epoch 15/100 Evaluation Batch 901/1758 loss 2.6731348037719727 accuracy: 0.392578125\n",
      "Epoch 15/100 Evaluation Batch 1001/1758 loss 2.9248857498168945 accuracy: 0.3802734315395355\n",
      "Epoch 15/100 Evaluation Batch 1101/1758 loss 2.7937138080596924 accuracy: 0.3687500059604645\n",
      "Epoch 15/100 Evaluation Batch 1201/1758 loss 2.761874198913574 accuracy: 0.37382814288139343\n",
      "Epoch 15/100 Evaluation Batch 1301/1758 loss 2.613393783569336 accuracy: 0.40625\n",
      "Epoch 15/100 Evaluation Batch 1401/1758 loss 2.7693774700164795 accuracy: 0.37890625\n",
      "Epoch 15/100 Evaluation Batch 1501/1758 loss 2.70953369140625 accuracy: 0.39667969942092896\n",
      "Epoch 15/100 Evaluation Batch 1601/1758 loss 2.783766746520996 accuracy: 0.3759765625\n",
      "Epoch 15/100 Evaluation Batch 1701/1758 loss 2.8896238803863525 accuracy: 0.3607421815395355\n",
      "Epoch 16/100 Training Batch 1/1758 loss 0.6070477962493896 accuracy: 0.724804699420929\n",
      "Epoch 16/100 Training Batch 101/1758 loss 0.612177848815918 accuracy: 0.729687511920929\n",
      "Epoch 16/100 Training Batch 201/1758 loss 0.6256940960884094 accuracy: 0.714648425579071\n",
      "Epoch 16/100 Training Batch 301/1758 loss 0.6579127907752991 accuracy: 0.703906238079071\n",
      "Epoch 16/100 Training Batch 401/1758 loss 0.6014384031295776 accuracy: 0.7320312857627869\n",
      "Epoch 16/100 Training Batch 501/1758 loss 0.6238203644752502 accuracy: 0.716796875\n",
      "Epoch 16/100 Training Batch 601/1758 loss 0.8887026906013489 accuracy: 0.598828136920929\n",
      "Epoch 16/100 Training Batch 701/1758 loss 0.6068661212921143 accuracy: 0.7300781607627869\n",
      "Epoch 16/100 Training Batch 801/1758 loss 0.6564354300498962 accuracy: 0.69140625\n",
      "Epoch 16/100 Training Batch 901/1758 loss 0.5860370993614197 accuracy: 0.7412109375\n",
      "Epoch 16/100 Training Batch 1001/1758 loss 0.602350115776062 accuracy: 0.7250000238418579\n",
      "Epoch 16/100 Training Batch 1101/1758 loss 0.7132760882377625 accuracy: 0.6714844107627869\n",
      "Epoch 16/100 Training Batch 1201/1758 loss 0.6249762177467346 accuracy: 0.7201172113418579\n",
      "Epoch 16/100 Training Batch 1301/1758 loss 0.6047602295875549 accuracy: 0.7367187738418579\n",
      "Epoch 16/100 Training Batch 1401/1758 loss 0.617031991481781 accuracy: 0.723437488079071\n",
      "Epoch 16/100 Training Batch 1501/1758 loss 0.6745246052742004 accuracy: 0.7064453363418579\n",
      "Epoch 16/100 Training Batch 1601/1758 loss 0.6272681355476379 accuracy: 0.717968761920929\n",
      "Epoch 16/100 Training Batch 1701/1758 loss 0.6166732907295227 accuracy: 0.721875011920929\n",
      "Epoch 16/100 Evaluation Batch 1/1758 loss 2.6404037475585938 accuracy: 0.3388671875\n",
      "Epoch 16/100 Evaluation Batch 101/1758 loss 2.732365369796753 accuracy: 0.3333984315395355\n",
      "Epoch 16/100 Evaluation Batch 201/1758 loss 2.7756059169769287 accuracy: 0.3271484375\n",
      "Epoch 16/100 Evaluation Batch 301/1758 loss 2.7235758304595947 accuracy: 0.3310546875\n",
      "Epoch 16/100 Evaluation Batch 401/1758 loss 2.7330119609832764 accuracy: 0.3451171815395355\n",
      "Epoch 16/100 Evaluation Batch 501/1758 loss 2.664911985397339 accuracy: 0.3441406190395355\n",
      "Epoch 16/100 Evaluation Batch 601/1758 loss 2.668206214904785 accuracy: 0.3404296934604645\n",
      "Epoch 16/100 Evaluation Batch 701/1758 loss 2.6880624294281006 accuracy: 0.33710938692092896\n",
      "Epoch 16/100 Evaluation Batch 801/1758 loss 2.7178800106048584 accuracy: 0.33808594942092896\n",
      "Epoch 16/100 Evaluation Batch 901/1758 loss 2.712221384048462 accuracy: 0.3375000059604645\n",
      "Epoch 16/100 Evaluation Batch 1001/1758 loss 2.6387484073638916 accuracy: 0.34980469942092896\n",
      "Epoch 16/100 Evaluation Batch 1101/1758 loss 2.629070520401001 accuracy: 0.3480468690395355\n",
      "Epoch 16/100 Evaluation Batch 1201/1758 loss 2.6622378826141357 accuracy: 0.34843751788139343\n",
      "Epoch 16/100 Evaluation Batch 1301/1758 loss 2.7376039028167725 accuracy: 0.33320313692092896\n",
      "Epoch 16/100 Evaluation Batch 1401/1758 loss 2.805936098098755 accuracy: 0.3207031190395355\n",
      "Epoch 16/100 Evaluation Batch 1501/1758 loss 2.801129102706909 accuracy: 0.31562501192092896\n",
      "Epoch 16/100 Evaluation Batch 1601/1758 loss 2.8279645442962646 accuracy: 0.3277343809604645\n",
      "Epoch 16/100 Evaluation Batch 1701/1758 loss 2.815394401550293 accuracy: 0.3310546875\n",
      "Epoch 17/100 Training Batch 1/1758 loss 0.7945062518119812 accuracy: 0.631640613079071\n",
      "Epoch 17/100 Training Batch 101/1758 loss 0.6386160254478455 accuracy: 0.713671863079071\n",
      "Epoch 17/100 Training Batch 201/1758 loss 0.7053598761558533 accuracy: 0.6929687857627869\n",
      "Epoch 17/100 Training Batch 301/1758 loss 0.747443675994873 accuracy: 0.6683593988418579\n",
      "Epoch 17/100 Training Batch 401/1758 loss 0.642568051815033 accuracy: 0.718945324420929\n",
      "Epoch 17/100 Training Batch 501/1758 loss 0.6419781446456909 accuracy: 0.712890625\n",
      "Epoch 17/100 Training Batch 601/1758 loss 0.6226124167442322 accuracy: 0.730664074420929\n",
      "Epoch 17/100 Training Batch 701/1758 loss 0.6493968367576599 accuracy: 0.712890625\n",
      "Epoch 17/100 Training Batch 801/1758 loss 0.977149248123169 accuracy: 0.55859375\n",
      "Epoch 17/100 Training Batch 901/1758 loss 0.6152669787406921 accuracy: 0.726367175579071\n",
      "Epoch 17/100 Training Batch 1001/1758 loss 0.6128793358802795 accuracy: 0.7330078482627869\n",
      "Epoch 17/100 Training Batch 1101/1758 loss 0.5892457365989685 accuracy: 0.7349609732627869\n",
      "Epoch 17/100 Training Batch 1201/1758 loss 0.6036323308944702 accuracy: 0.729296863079071\n",
      "Epoch 17/100 Training Batch 1301/1758 loss 0.5931524038314819 accuracy: 0.738476574420929\n",
      "Epoch 17/100 Training Batch 1401/1758 loss 0.6022055745124817 accuracy: 0.7347656488418579\n",
      "Epoch 17/100 Training Batch 1501/1758 loss 0.5931171774864197 accuracy: 0.7392578125\n",
      "Epoch 17/100 Training Batch 1601/1758 loss 0.5724948048591614 accuracy: 0.747265636920929\n",
      "Epoch 17/100 Training Batch 1701/1758 loss 0.5950440764427185 accuracy: 0.745898425579071\n",
      "Epoch 17/100 Evaluation Batch 1/1758 loss 2.8828318119049072 accuracy: 0.3763671815395355\n",
      "Epoch 17/100 Evaluation Batch 101/1758 loss 2.83811092376709 accuracy: 0.3804687559604645\n",
      "Epoch 17/100 Evaluation Batch 201/1758 loss 2.6427152156829834 accuracy: 0.4134765565395355\n",
      "Epoch 17/100 Evaluation Batch 301/1758 loss 2.8469858169555664 accuracy: 0.38398438692092896\n",
      "Epoch 17/100 Evaluation Batch 401/1758 loss 2.8337604999542236 accuracy: 0.36796876788139343\n",
      "Epoch 17/100 Evaluation Batch 501/1758 loss 2.798996925354004 accuracy: 0.3724609315395355\n",
      "Epoch 17/100 Evaluation Batch 601/1758 loss 2.7550859451293945 accuracy: 0.38261720538139343\n",
      "Epoch 17/100 Evaluation Batch 701/1758 loss 2.8624227046966553 accuracy: 0.3740234375\n",
      "Epoch 17/100 Evaluation Batch 801/1758 loss 2.7764675617218018 accuracy: 0.3863281309604645\n",
      "Epoch 17/100 Evaluation Batch 901/1758 loss 2.9143388271331787 accuracy: 0.3701171875\n",
      "Epoch 17/100 Evaluation Batch 1001/1758 loss 2.7065823078155518 accuracy: 0.40156251192092896\n",
      "Epoch 17/100 Evaluation Batch 1101/1758 loss 2.819697380065918 accuracy: 0.3646484315395355\n",
      "Epoch 17/100 Evaluation Batch 1201/1758 loss 2.7875897884368896 accuracy: 0.38105469942092896\n",
      "Epoch 17/100 Evaluation Batch 1301/1758 loss 2.887984275817871 accuracy: 0.36113283038139343\n",
      "Epoch 17/100 Evaluation Batch 1401/1758 loss 2.7814552783966064 accuracy: 0.3775390684604645\n",
      "Epoch 17/100 Evaluation Batch 1501/1758 loss 2.8620870113372803 accuracy: 0.3746093809604645\n",
      "Epoch 17/100 Evaluation Batch 1601/1758 loss 2.713555335998535 accuracy: 0.408203125\n",
      "Epoch 17/100 Evaluation Batch 1701/1758 loss 2.728405237197876 accuracy: 0.3921875059604645\n",
      "Epoch 18/100 Training Batch 1/1758 loss 0.577016294002533 accuracy: 0.7421875\n",
      "Epoch 18/100 Training Batch 101/1758 loss 0.5886622071266174 accuracy: 0.7337890863418579\n",
      "Epoch 18/100 Training Batch 201/1758 loss 0.6015556454658508 accuracy: 0.725781261920929\n",
      "Epoch 18/100 Training Batch 301/1758 loss 0.5837419033050537 accuracy: 0.734375\n",
      "Epoch 18/100 Training Batch 401/1758 loss 0.848034679889679 accuracy: 0.607617199420929\n",
      "Epoch 18/100 Training Batch 501/1758 loss 0.6095221638679504 accuracy: 0.7230468988418579\n",
      "Epoch 18/100 Training Batch 601/1758 loss 0.5933993458747864 accuracy: 0.740429699420929\n",
      "Epoch 18/100 Training Batch 701/1758 loss 0.631369411945343 accuracy: 0.7142578363418579\n",
      "Epoch 18/100 Training Batch 801/1758 loss 0.6889979243278503 accuracy: 0.6996093988418579\n",
      "Epoch 18/100 Training Batch 901/1758 loss 0.6859856247901917 accuracy: 0.6957031488418579\n",
      "Epoch 18/100 Training Batch 1001/1758 loss 0.6493598222732544 accuracy: 0.7220703363418579\n",
      "Epoch 18/100 Training Batch 1101/1758 loss 0.595510721206665 accuracy: 0.736328125\n",
      "Epoch 18/100 Training Batch 1201/1758 loss 0.6131493449211121 accuracy: 0.7373046875\n",
      "Epoch 18/100 Training Batch 1301/1758 loss 0.6470281481742859 accuracy: 0.7113281488418579\n",
      "Epoch 18/100 Training Batch 1401/1758 loss 0.6083741188049316 accuracy: 0.7259765863418579\n",
      "Epoch 18/100 Training Batch 1501/1758 loss 0.6389316916465759 accuracy: 0.715624988079071\n",
      "Epoch 18/100 Training Batch 1601/1758 loss 1.0306810140609741 accuracy: 0.517578125\n",
      "Epoch 18/100 Training Batch 1701/1758 loss 0.8424933552742004 accuracy: 0.620312511920929\n",
      "Epoch 18/100 Evaluation Batch 1/1758 loss 2.2931792736053467 accuracy: 0.3384765684604645\n",
      "Epoch 18/100 Evaluation Batch 101/1758 loss 2.2905070781707764 accuracy: 0.3402343690395355\n",
      "Epoch 18/100 Evaluation Batch 201/1758 loss 2.2771923542022705 accuracy: 0.33710938692092896\n",
      "Epoch 18/100 Evaluation Batch 301/1758 loss 2.2572765350341797 accuracy: 0.35429689288139343\n",
      "Epoch 18/100 Evaluation Batch 401/1758 loss 2.313603162765503 accuracy: 0.3333984315395355\n",
      "Epoch 18/100 Evaluation Batch 501/1758 loss 2.312130928039551 accuracy: 0.34843751788139343\n",
      "Epoch 18/100 Evaluation Batch 601/1758 loss 2.2893283367156982 accuracy: 0.3482421934604645\n",
      "Epoch 18/100 Evaluation Batch 701/1758 loss 2.28826904296875 accuracy: 0.3388671875\n",
      "Epoch 18/100 Evaluation Batch 801/1758 loss 2.3376028537750244 accuracy: 0.3349609375\n",
      "Epoch 18/100 Evaluation Batch 901/1758 loss 2.2703306674957275 accuracy: 0.3388671875\n",
      "Epoch 18/100 Evaluation Batch 1001/1758 loss 2.2250003814697266 accuracy: 0.35429689288139343\n",
      "Epoch 18/100 Evaluation Batch 1101/1758 loss 2.247690200805664 accuracy: 0.33906251192092896\n",
      "Epoch 18/100 Evaluation Batch 1201/1758 loss 2.1002233028411865 accuracy: 0.36503908038139343\n",
      "Epoch 18/100 Evaluation Batch 1301/1758 loss 2.2668492794036865 accuracy: 0.35859376192092896\n",
      "Epoch 18/100 Evaluation Batch 1401/1758 loss 2.386855125427246 accuracy: 0.3193359375\n",
      "Epoch 18/100 Evaluation Batch 1501/1758 loss 2.331214189529419 accuracy: 0.3349609375\n",
      "Epoch 18/100 Evaluation Batch 1601/1758 loss 2.2905776500701904 accuracy: 0.34492188692092896\n",
      "Epoch 18/100 Evaluation Batch 1701/1758 loss 2.3374979496002197 accuracy: 0.32636719942092896\n",
      "Epoch 19/100 Training Batch 1/1758 loss 0.7839228510856628 accuracy: 0.656054675579071\n",
      "Epoch 19/100 Training Batch 101/1758 loss 0.7545991539955139 accuracy: 0.6773437857627869\n",
      "Epoch 19/100 Training Batch 201/1758 loss 0.7019298076629639 accuracy: 0.69921875\n",
      "Epoch 19/100 Training Batch 301/1758 loss 0.6613397002220154 accuracy: 0.7083984613418579\n",
      "Epoch 19/100 Training Batch 401/1758 loss 0.6952987313270569 accuracy: 0.6904296875\n",
      "Epoch 19/100 Training Batch 501/1758 loss 0.7056097984313965 accuracy: 0.6865234375\n",
      "Epoch 19/100 Training Batch 601/1758 loss 0.6784549355506897 accuracy: 0.7007812857627869\n",
      "Epoch 19/100 Training Batch 701/1758 loss 0.6310519576072693 accuracy: 0.722460925579071\n",
      "Epoch 19/100 Training Batch 801/1758 loss 0.664739191532135 accuracy: 0.717968761920929\n",
      "Epoch 19/100 Training Batch 901/1758 loss 0.6369853615760803 accuracy: 0.7236328125\n",
      "Epoch 19/100 Training Batch 1001/1758 loss 0.650330126285553 accuracy: 0.716992199420929\n",
      "Epoch 19/100 Training Batch 1101/1758 loss 0.7042710185050964 accuracy: 0.685546875\n",
      "Epoch 19/100 Training Batch 1201/1758 loss 0.6741135716438293 accuracy: 0.704882800579071\n",
      "Epoch 19/100 Training Batch 1301/1758 loss 0.6109747290611267 accuracy: 0.730664074420929\n",
      "Epoch 19/100 Training Batch 1401/1758 loss 0.6385926008224487 accuracy: 0.7191406488418579\n",
      "Epoch 19/100 Training Batch 1501/1758 loss 0.6807563900947571 accuracy: 0.6888672113418579\n",
      "Epoch 19/100 Training Batch 1601/1758 loss 0.5975754261016846 accuracy: 0.739062488079071\n",
      "Epoch 19/100 Training Batch 1701/1758 loss 0.6583886742591858 accuracy: 0.714648425579071\n",
      "Epoch 19/100 Evaluation Batch 1/1758 loss 2.7755072116851807 accuracy: 0.36445313692092896\n",
      "Epoch 19/100 Evaluation Batch 101/1758 loss 2.8183670043945312 accuracy: 0.3763671815395355\n",
      "Epoch 19/100 Evaluation Batch 201/1758 loss 2.790029764175415 accuracy: 0.375\n",
      "Epoch 19/100 Evaluation Batch 301/1758 loss 2.657698392868042 accuracy: 0.39628908038139343\n",
      "Epoch 19/100 Evaluation Batch 401/1758 loss 2.671959161758423 accuracy: 0.388671875\n",
      "Epoch 19/100 Evaluation Batch 501/1758 loss 2.768711805343628 accuracy: 0.37480470538139343\n",
      "Epoch 19/100 Evaluation Batch 601/1758 loss 2.7734768390655518 accuracy: 0.38398438692092896\n",
      "Epoch 19/100 Evaluation Batch 701/1758 loss 2.7201311588287354 accuracy: 0.3638671934604645\n",
      "Epoch 19/100 Evaluation Batch 801/1758 loss 2.665626049041748 accuracy: 0.3949218690395355\n",
      "Epoch 19/100 Evaluation Batch 901/1758 loss 2.738856554031372 accuracy: 0.38886719942092896\n",
      "Epoch 19/100 Evaluation Batch 1001/1758 loss 2.7759807109832764 accuracy: 0.3824218809604645\n",
      "Epoch 19/100 Evaluation Batch 1101/1758 loss 2.7068068981170654 accuracy: 0.3824218809604645\n",
      "Epoch 19/100 Evaluation Batch 1201/1758 loss 2.829228639602661 accuracy: 0.3798828125\n",
      "Epoch 19/100 Evaluation Batch 1301/1758 loss 2.7321298122406006 accuracy: 0.38691407442092896\n",
      "Epoch 19/100 Evaluation Batch 1401/1758 loss 2.5971195697784424 accuracy: 0.4056640565395355\n",
      "Epoch 19/100 Evaluation Batch 1501/1758 loss 2.8653147220611572 accuracy: 0.37226563692092896\n",
      "Epoch 19/100 Evaluation Batch 1601/1758 loss 2.758790969848633 accuracy: 0.37285158038139343\n",
      "Epoch 19/100 Evaluation Batch 1701/1758 loss 2.665848970413208 accuracy: 0.3974609375\n",
      "Epoch 20/100 Training Batch 1/1758 loss 0.609434187412262 accuracy: 0.729296863079071\n",
      "Epoch 20/100 Training Batch 101/1758 loss 0.6188331246376038 accuracy: 0.7294921875\n",
      "Epoch 20/100 Training Batch 201/1758 loss 0.6128783226013184 accuracy: 0.7271484732627869\n",
      "Epoch 20/100 Training Batch 301/1758 loss 0.6014237999916077 accuracy: 0.7369140982627869\n",
      "Epoch 20/100 Training Batch 401/1758 loss 0.6099693775177002 accuracy: 0.726757824420929\n",
      "Epoch 20/100 Training Batch 501/1758 loss 0.5714662671089172 accuracy: 0.749804675579071\n",
      "Epoch 20/100 Training Batch 601/1758 loss 0.5910413861274719 accuracy: 0.736132800579071\n",
      "Epoch 20/100 Training Batch 701/1758 loss 0.5996373295783997 accuracy: 0.7294921875\n",
      "Epoch 20/100 Training Batch 801/1758 loss 0.6576333045959473 accuracy: 0.70703125\n",
      "Epoch 20/100 Training Batch 901/1758 loss 0.6249064803123474 accuracy: 0.716796875\n",
      "Epoch 20/100 Training Batch 1001/1758 loss 0.586401641368866 accuracy: 0.73828125\n",
      "Epoch 20/100 Training Batch 1101/1758 loss 0.705069363117218 accuracy: 0.681835949420929\n",
      "Epoch 20/100 Training Batch 1201/1758 loss 0.5978106260299683 accuracy: 0.7333984375\n",
      "Epoch 20/100 Training Batch 1301/1758 loss 0.570824384689331 accuracy: 0.75390625\n",
      "Epoch 20/100 Training Batch 1401/1758 loss 0.5808157324790955 accuracy: 0.7412109375\n",
      "Epoch 20/100 Training Batch 1501/1758 loss 0.5860055685043335 accuracy: 0.7308593988418579\n",
      "Epoch 20/100 Training Batch 1601/1758 loss 0.6270309090614319 accuracy: 0.7220703363418579\n",
      "Epoch 20/100 Training Batch 1701/1758 loss 0.5788141489028931 accuracy: 0.7455078363418579\n",
      "Epoch 20/100 Evaluation Batch 1/1758 loss 2.7225430011749268 accuracy: 0.3746093809604645\n",
      "Epoch 20/100 Evaluation Batch 101/1758 loss 2.7010996341705322 accuracy: 0.3736328184604645\n",
      "Epoch 20/100 Evaluation Batch 201/1758 loss 2.7398998737335205 accuracy: 0.3716796934604645\n",
      "Epoch 20/100 Evaluation Batch 301/1758 loss 2.698026418685913 accuracy: 0.3794921934604645\n",
      "Epoch 20/100 Evaluation Batch 401/1758 loss 2.6416378021240234 accuracy: 0.3958984315395355\n",
      "Epoch 20/100 Evaluation Batch 501/1758 loss 2.756617307662964 accuracy: 0.365234375\n",
      "Epoch 20/100 Evaluation Batch 601/1758 loss 2.5943000316619873 accuracy: 0.3949218690395355\n",
      "Epoch 20/100 Evaluation Batch 701/1758 loss 2.6788699626922607 accuracy: 0.37812501192092896\n",
      "Epoch 20/100 Evaluation Batch 801/1758 loss 2.6705474853515625 accuracy: 0.38203126192092896\n",
      "Epoch 20/100 Evaluation Batch 901/1758 loss 2.707178831100464 accuracy: 0.3779296875\n",
      "Epoch 20/100 Evaluation Batch 1001/1758 loss 2.7058613300323486 accuracy: 0.3667968809604645\n",
      "Epoch 20/100 Evaluation Batch 1101/1758 loss 2.6531825065612793 accuracy: 0.39472657442092896\n",
      "Epoch 20/100 Evaluation Batch 1201/1758 loss 2.6991050243377686 accuracy: 0.3740234375\n",
      "Epoch 20/100 Evaluation Batch 1301/1758 loss 2.668585777282715 accuracy: 0.3910156190395355\n",
      "Epoch 20/100 Evaluation Batch 1401/1758 loss 2.560694932937622 accuracy: 0.400390625\n",
      "Epoch 20/100 Evaluation Batch 1501/1758 loss 2.6348650455474854 accuracy: 0.39824220538139343\n",
      "Epoch 20/100 Evaluation Batch 1601/1758 loss 2.7225277423858643 accuracy: 0.380859375\n",
      "Epoch 20/100 Evaluation Batch 1701/1758 loss 2.7160751819610596 accuracy: 0.3658203184604645\n",
      "Epoch 21/100 Training Batch 1/1758 loss 0.5922254323959351 accuracy: 0.741992175579071\n",
      "Epoch 21/100 Training Batch 101/1758 loss 0.6086623072624207 accuracy: 0.7291015982627869\n",
      "Epoch 21/100 Training Batch 201/1758 loss 0.5920203328132629 accuracy: 0.7347656488418579\n",
      "Epoch 21/100 Training Batch 301/1758 loss 0.880765974521637 accuracy: 0.610546886920929\n",
      "Epoch 21/100 Training Batch 401/1758 loss 0.6826541423797607 accuracy: 0.697460949420929\n",
      "Epoch 21/100 Training Batch 501/1758 loss 0.6849443912506104 accuracy: 0.698437511920929\n",
      "Epoch 21/100 Training Batch 601/1758 loss 0.6338369250297546 accuracy: 0.7212890982627869\n",
      "Epoch 21/100 Training Batch 701/1758 loss 0.7011959552764893 accuracy: 0.6878906488418579\n",
      "Epoch 21/100 Training Batch 801/1758 loss 0.6303150057792664 accuracy: 0.7281250357627869\n",
      "Epoch 21/100 Training Batch 901/1758 loss 0.6127224564552307 accuracy: 0.7310547232627869\n",
      "Epoch 21/100 Training Batch 1001/1758 loss 0.6306346654891968 accuracy: 0.7109375\n",
      "Epoch 21/100 Training Batch 1101/1758 loss 0.6306083798408508 accuracy: 0.7181640863418579\n",
      "Epoch 21/100 Training Batch 1201/1758 loss 0.6301697492599487 accuracy: 0.7164062857627869\n",
      "Epoch 21/100 Training Batch 1301/1758 loss 0.6051168441772461 accuracy: 0.7269531488418579\n",
      "Epoch 21/100 Training Batch 1401/1758 loss 0.6306629180908203 accuracy: 0.723828136920929\n",
      "Epoch 21/100 Training Batch 1501/1758 loss 0.5975887775421143 accuracy: 0.7376953363418579\n",
      "Epoch 21/100 Training Batch 1601/1758 loss 0.6288463473320007 accuracy: 0.724804699420929\n",
      "Epoch 21/100 Training Batch 1701/1758 loss 0.6323958039283752 accuracy: 0.729296863079071\n",
      "Epoch 21/100 Evaluation Batch 1/1758 loss 2.682448148727417 accuracy: 0.35761719942092896\n",
      "Epoch 21/100 Evaluation Batch 101/1758 loss 2.687405586242676 accuracy: 0.3685546815395355\n",
      "Epoch 21/100 Evaluation Batch 201/1758 loss 2.637079954147339 accuracy: 0.37226563692092896\n",
      "Epoch 21/100 Evaluation Batch 301/1758 loss 2.6678779125213623 accuracy: 0.3755859434604645\n",
      "Epoch 21/100 Evaluation Batch 401/1758 loss 2.5801093578338623 accuracy: 0.3871093690395355\n",
      "Epoch 21/100 Evaluation Batch 501/1758 loss 2.510197401046753 accuracy: 0.3941406309604645\n",
      "Epoch 21/100 Evaluation Batch 601/1758 loss 2.6183292865753174 accuracy: 0.3802734315395355\n",
      "Epoch 21/100 Evaluation Batch 701/1758 loss 2.7305593490600586 accuracy: 0.3597656190395355\n",
      "Epoch 21/100 Evaluation Batch 801/1758 loss 2.658461093902588 accuracy: 0.38652345538139343\n",
      "Epoch 21/100 Evaluation Batch 901/1758 loss 2.6581146717071533 accuracy: 0.38691407442092896\n",
      "Epoch 21/100 Evaluation Batch 1001/1758 loss 2.6932106018066406 accuracy: 0.36738282442092896\n",
      "Epoch 21/100 Evaluation Batch 1101/1758 loss 2.618182897567749 accuracy: 0.3765625059604645\n",
      "Epoch 21/100 Evaluation Batch 1201/1758 loss 2.688201904296875 accuracy: 0.3794921934604645\n",
      "Epoch 21/100 Evaluation Batch 1301/1758 loss 2.7039129734039307 accuracy: 0.3724609315395355\n",
      "Epoch 21/100 Evaluation Batch 1401/1758 loss 2.6097469329833984 accuracy: 0.3685546815395355\n",
      "Epoch 21/100 Evaluation Batch 1501/1758 loss 2.738466262817383 accuracy: 0.361328125\n",
      "Epoch 21/100 Evaluation Batch 1601/1758 loss 2.5500524044036865 accuracy: 0.38671875\n",
      "Epoch 21/100 Evaluation Batch 1701/1758 loss 2.4734115600585938 accuracy: 0.3990234434604645\n",
      "Epoch 22/100 Training Batch 1/1758 loss 0.6125990748405457 accuracy: 0.734375\n",
      "Epoch 22/100 Training Batch 101/1758 loss 0.6190798282623291 accuracy: 0.719531238079071\n",
      "Epoch 22/100 Training Batch 201/1758 loss 0.6075636148452759 accuracy: 0.736328125\n",
      "Epoch 22/100 Training Batch 301/1758 loss 0.6199983954429626 accuracy: 0.731249988079071\n",
      "Epoch 22/100 Training Batch 401/1758 loss 0.5836078524589539 accuracy: 0.7435547113418579\n",
      "Epoch 22/100 Training Batch 501/1758 loss 0.6981510519981384 accuracy: 0.6826171875\n",
      "Epoch 22/100 Training Batch 601/1758 loss 0.6049121022224426 accuracy: 0.7359375357627869\n",
      "Epoch 22/100 Training Batch 701/1758 loss 0.5941908359527588 accuracy: 0.736132800579071\n",
      "Epoch 22/100 Training Batch 801/1758 loss 0.6081668734550476 accuracy: 0.7300781607627869\n",
      "Epoch 22/100 Training Batch 901/1758 loss 0.5993829965591431 accuracy: 0.732421875\n",
      "Epoch 22/100 Training Batch 1001/1758 loss 0.593353271484375 accuracy: 0.7376953363418579\n",
      "Epoch 22/100 Training Batch 1101/1758 loss 0.6198188662528992 accuracy: 0.7242187857627869\n",
      "Epoch 22/100 Training Batch 1201/1758 loss 0.841305673122406 accuracy: 0.625781238079071\n",
      "Epoch 22/100 Training Batch 1301/1758 loss 0.5806787610054016 accuracy: 0.742382824420929\n",
      "Epoch 22/100 Training Batch 1401/1758 loss 0.5830710530281067 accuracy: 0.7398437857627869\n",
      "Epoch 22/100 Training Batch 1501/1758 loss 0.8093302249908447 accuracy: 0.6507812738418579\n",
      "Epoch 22/100 Training Batch 1601/1758 loss 0.7754504680633545 accuracy: 0.646289050579071\n",
      "Epoch 22/100 Training Batch 1701/1758 loss 0.5915735363960266 accuracy: 0.73828125\n",
      "Epoch 22/100 Evaluation Batch 1/1758 loss 3.0642659664154053 accuracy: 0.38007813692092896\n",
      "Epoch 22/100 Evaluation Batch 101/1758 loss 2.962951898574829 accuracy: 0.3841796815395355\n",
      "Epoch 22/100 Evaluation Batch 201/1758 loss 2.8666536808013916 accuracy: 0.3912109434604645\n",
      "Epoch 22/100 Evaluation Batch 301/1758 loss 2.935053586959839 accuracy: 0.38007813692092896\n",
      "Epoch 22/100 Evaluation Batch 401/1758 loss 2.9732182025909424 accuracy: 0.3832031190395355\n",
      "Epoch 22/100 Evaluation Batch 501/1758 loss 2.978269577026367 accuracy: 0.3818359375\n",
      "Epoch 22/100 Evaluation Batch 601/1758 loss 3.0135297775268555 accuracy: 0.37773439288139343\n",
      "Epoch 22/100 Evaluation Batch 701/1758 loss 3.147958755493164 accuracy: 0.36406251788139343\n",
      "Epoch 22/100 Evaluation Batch 801/1758 loss 3.071504592895508 accuracy: 0.36796876788139343\n",
      "Epoch 22/100 Evaluation Batch 901/1758 loss 2.8703787326812744 accuracy: 0.3861328065395355\n",
      "Epoch 22/100 Evaluation Batch 1001/1758 loss 3.0501270294189453 accuracy: 0.37617188692092896\n",
      "Epoch 22/100 Evaluation Batch 1101/1758 loss 2.850534200668335 accuracy: 0.3951171934604645\n",
      "Epoch 22/100 Evaluation Batch 1201/1758 loss 3.071322441101074 accuracy: 0.3687500059604645\n",
      "Epoch 22/100 Evaluation Batch 1301/1758 loss 3.091568946838379 accuracy: 0.3544921875\n",
      "Epoch 22/100 Evaluation Batch 1401/1758 loss 3.1356630325317383 accuracy: 0.36894533038139343\n",
      "Epoch 22/100 Evaluation Batch 1501/1758 loss 3.0183544158935547 accuracy: 0.37773439288139343\n",
      "Epoch 22/100 Evaluation Batch 1601/1758 loss 2.9370968341827393 accuracy: 0.37773439288139343\n",
      "Epoch 22/100 Evaluation Batch 1701/1758 loss 2.861511468887329 accuracy: 0.396484375\n",
      "Epoch 23/100 Training Batch 1/1758 loss 0.5799391269683838 accuracy: 0.74609375\n",
      "Epoch 23/100 Training Batch 101/1758 loss 0.5916780829429626 accuracy: 0.739062488079071\n",
      "Epoch 23/100 Training Batch 201/1758 loss 0.5714854598045349 accuracy: 0.7503906488418579\n",
      "Epoch 23/100 Training Batch 301/1758 loss 0.5663711428642273 accuracy: 0.753125011920929\n",
      "Epoch 23/100 Training Batch 401/1758 loss 0.5618881583213806 accuracy: 0.7496094107627869\n",
      "Epoch 23/100 Training Batch 501/1758 loss 0.5709986090660095 accuracy: 0.756054699420929\n",
      "Epoch 23/100 Training Batch 601/1758 loss 1.090110182762146 accuracy: 0.4876953065395355\n",
      "Epoch 23/100 Training Batch 701/1758 loss 0.8088635802268982 accuracy: 0.625195324420929\n",
      "Epoch 23/100 Training Batch 801/1758 loss 0.8198235630989075 accuracy: 0.6259765625\n",
      "Epoch 23/100 Training Batch 901/1758 loss 0.7230019569396973 accuracy: 0.673828125\n",
      "Epoch 23/100 Training Batch 1001/1758 loss 0.7240448594093323 accuracy: 0.668749988079071\n",
      "Epoch 23/100 Training Batch 1101/1758 loss 0.6517100930213928 accuracy: 0.696093738079071\n",
      "Epoch 23/100 Training Batch 1201/1758 loss 0.6044006943702698 accuracy: 0.721875011920929\n",
      "Epoch 23/100 Training Batch 1301/1758 loss 0.5904162526130676 accuracy: 0.7318359613418579\n",
      "Epoch 23/100 Training Batch 1401/1758 loss 0.6450843214988708 accuracy: 0.712890625\n",
      "Epoch 23/100 Training Batch 1501/1758 loss 0.6091803908348083 accuracy: 0.720898449420929\n",
      "Epoch 23/100 Training Batch 1601/1758 loss 0.6199912428855896 accuracy: 0.722460925579071\n",
      "Epoch 23/100 Training Batch 1701/1758 loss 0.6210113167762756 accuracy: 0.717968761920929\n",
      "Epoch 23/100 Evaluation Batch 1/1758 loss 2.9852447509765625 accuracy: 0.3707031309604645\n",
      "Epoch 23/100 Evaluation Batch 101/1758 loss 3.005652666091919 accuracy: 0.3501953184604645\n",
      "Epoch 23/100 Evaluation Batch 201/1758 loss 2.9836254119873047 accuracy: 0.37109375\n",
      "Epoch 23/100 Evaluation Batch 301/1758 loss 3.0986669063568115 accuracy: 0.3570312559604645\n",
      "Epoch 23/100 Evaluation Batch 401/1758 loss 3.1089322566986084 accuracy: 0.353515625\n",
      "Epoch 23/100 Evaluation Batch 501/1758 loss 2.951946973800659 accuracy: 0.37714844942092896\n",
      "Epoch 23/100 Evaluation Batch 601/1758 loss 2.9674999713897705 accuracy: 0.37519532442092896\n",
      "Epoch 23/100 Evaluation Batch 701/1758 loss 2.8964128494262695 accuracy: 0.3667968809604645\n",
      "Epoch 23/100 Evaluation Batch 801/1758 loss 2.965355634689331 accuracy: 0.36406251788139343\n",
      "Epoch 23/100 Evaluation Batch 901/1758 loss 2.879727602005005 accuracy: 0.37871095538139343\n",
      "Epoch 23/100 Evaluation Batch 1001/1758 loss 2.89538836479187 accuracy: 0.3832031190395355\n",
      "Epoch 23/100 Evaluation Batch 1101/1758 loss 2.8601391315460205 accuracy: 0.3802734315395355\n",
      "Epoch 23/100 Evaluation Batch 1201/1758 loss 2.991478681564331 accuracy: 0.38652345538139343\n",
      "Epoch 23/100 Evaluation Batch 1301/1758 loss 2.995642900466919 accuracy: 0.3792968690395355\n",
      "Epoch 23/100 Evaluation Batch 1401/1758 loss 2.9191417694091797 accuracy: 0.38593751192092896\n",
      "Epoch 23/100 Evaluation Batch 1501/1758 loss 2.9904263019561768 accuracy: 0.37324219942092896\n",
      "Epoch 23/100 Evaluation Batch 1601/1758 loss 3.0008537769317627 accuracy: 0.3646484315395355\n",
      "Epoch 23/100 Evaluation Batch 1701/1758 loss 2.9175970554351807 accuracy: 0.39863282442092896\n",
      "Epoch 24/100 Training Batch 1/1758 loss 0.6034013628959656 accuracy: 0.7271484732627869\n",
      "Epoch 24/100 Training Batch 101/1758 loss 0.5951022505760193 accuracy: 0.7275390625\n",
      "Epoch 24/100 Training Batch 201/1758 loss 0.5680616497993469 accuracy: 0.7408203482627869\n",
      "Epoch 24/100 Training Batch 301/1758 loss 0.6424657702445984 accuracy: 0.7044922113418579\n",
      "Epoch 24/100 Training Batch 401/1758 loss 0.6331531405448914 accuracy: 0.7132812738418579\n",
      "Epoch 24/100 Training Batch 501/1758 loss 0.576484739780426 accuracy: 0.737109363079071\n",
      "Epoch 24/100 Training Batch 601/1758 loss 0.6026616096496582 accuracy: 0.717968761920929\n",
      "Epoch 24/100 Training Batch 701/1758 loss 0.5725368857383728 accuracy: 0.733593761920929\n",
      "Epoch 24/100 Training Batch 801/1758 loss 0.6118218302726746 accuracy: 0.724414050579071\n",
      "Epoch 24/100 Training Batch 901/1758 loss 0.673454761505127 accuracy: 0.6937500238418579\n",
      "Epoch 24/100 Training Batch 1001/1758 loss 0.8174748420715332 accuracy: 0.6236328482627869\n",
      "Epoch 24/100 Training Batch 1101/1758 loss 0.6522056460380554 accuracy: 0.6982421875\n",
      "Epoch 24/100 Training Batch 1201/1758 loss 0.6998626589775085 accuracy: 0.6771484613418579\n",
      "Epoch 24/100 Training Batch 1301/1758 loss 0.6402861475944519 accuracy: 0.70703125\n",
      "Epoch 24/100 Training Batch 1401/1758 loss 0.5837855339050293 accuracy: 0.7294921875\n",
      "Epoch 24/100 Training Batch 1501/1758 loss 0.6213592886924744 accuracy: 0.7210937738418579\n",
      "Epoch 24/100 Training Batch 1601/1758 loss 0.5706533789634705 accuracy: 0.748046875\n",
      "Epoch 24/100 Training Batch 1701/1758 loss 0.5894115567207336 accuracy: 0.722851574420929\n",
      "Epoch 24/100 Evaluation Batch 1/1758 loss 3.0013391971588135 accuracy: 0.3402343690395355\n",
      "Epoch 24/100 Evaluation Batch 101/1758 loss 2.7958061695098877 accuracy: 0.38261720538139343\n",
      "Epoch 24/100 Evaluation Batch 201/1758 loss 2.966501474380493 accuracy: 0.35957032442092896\n",
      "Epoch 24/100 Evaluation Batch 301/1758 loss 2.964463233947754 accuracy: 0.35371094942092896\n",
      "Epoch 24/100 Evaluation Batch 401/1758 loss 2.961686372756958 accuracy: 0.3529296815395355\n",
      "Epoch 24/100 Evaluation Batch 501/1758 loss 3.041221857070923 accuracy: 0.35234376788139343\n",
      "Epoch 24/100 Evaluation Batch 601/1758 loss 3.094623327255249 accuracy: 0.3394531309604645\n",
      "Epoch 24/100 Evaluation Batch 701/1758 loss 3.0781030654907227 accuracy: 0.3505859375\n",
      "Epoch 24/100 Evaluation Batch 801/1758 loss 2.919404983520508 accuracy: 0.359375\n",
      "Epoch 24/100 Evaluation Batch 901/1758 loss 2.989924669265747 accuracy: 0.3441406190395355\n",
      "Epoch 24/100 Evaluation Batch 1001/1758 loss 3.066248655319214 accuracy: 0.3451171815395355\n",
      "Epoch 24/100 Evaluation Batch 1101/1758 loss 2.9123566150665283 accuracy: 0.3603515625\n",
      "Epoch 24/100 Evaluation Batch 1201/1758 loss 2.9772660732269287 accuracy: 0.34746095538139343\n",
      "Epoch 24/100 Evaluation Batch 1301/1758 loss 2.9306013584136963 accuracy: 0.35722658038139343\n",
      "Epoch 24/100 Evaluation Batch 1401/1758 loss 3.035400390625 accuracy: 0.353515625\n",
      "Epoch 24/100 Evaluation Batch 1501/1758 loss 3.128002166748047 accuracy: 0.337890625\n",
      "Epoch 24/100 Evaluation Batch 1601/1758 loss 3.0362069606781006 accuracy: 0.35078126192092896\n",
      "Epoch 24/100 Evaluation Batch 1701/1758 loss 2.9851415157318115 accuracy: 0.35625001788139343\n",
      "Epoch 25/100 Training Batch 1/1758 loss 0.6583721041679382 accuracy: 0.6988281607627869\n",
      "Epoch 25/100 Training Batch 101/1758 loss 0.5780783891677856 accuracy: 0.7427734732627869\n",
      "Epoch 25/100 Training Batch 201/1758 loss 0.5600177645683289 accuracy: 0.739062488079071\n",
      "Epoch 25/100 Training Batch 301/1758 loss 0.5571348071098328 accuracy: 0.7435547113418579\n",
      "Epoch 25/100 Training Batch 401/1758 loss 0.5828779339790344 accuracy: 0.737109363079071\n",
      "Epoch 25/100 Training Batch 501/1758 loss 0.5752838253974915 accuracy: 0.7406250238418579\n",
      "Epoch 25/100 Training Batch 601/1758 loss 0.5720083117485046 accuracy: 0.737500011920929\n",
      "Epoch 25/100 Training Batch 701/1758 loss 0.6779895424842834 accuracy: 0.695117175579071\n",
      "Epoch 25/100 Training Batch 801/1758 loss 0.5938715934753418 accuracy: 0.7398437857627869\n",
      "Epoch 25/100 Training Batch 901/1758 loss 0.6010908484458923 accuracy: 0.737109363079071\n",
      "Epoch 25/100 Training Batch 1001/1758 loss 0.5880259871482849 accuracy: 0.7398437857627869\n",
      "Epoch 25/100 Training Batch 1101/1758 loss 0.5745292901992798 accuracy: 0.7412109375\n",
      "Epoch 25/100 Training Batch 1201/1758 loss 0.5670095086097717 accuracy: 0.7359375357627869\n",
      "Epoch 25/100 Training Batch 1301/1758 loss 0.5703264474868774 accuracy: 0.738085925579071\n",
      "Epoch 25/100 Training Batch 1401/1758 loss 0.5790613293647766 accuracy: 0.7339844107627869\n",
      "Epoch 25/100 Training Batch 1501/1758 loss 0.9369423389434814 accuracy: 0.563281238079071\n",
      "Epoch 25/100 Training Batch 1601/1758 loss 0.7357058525085449 accuracy: 0.665820300579071\n",
      "Epoch 25/100 Training Batch 1701/1758 loss 0.6930325627326965 accuracy: 0.6908203363418579\n",
      "Epoch 25/100 Evaluation Batch 1/1758 loss 2.5435378551483154 accuracy: 0.36699220538139343\n",
      "Epoch 25/100 Evaluation Batch 101/1758 loss 2.3991000652313232 accuracy: 0.39082032442092896\n",
      "Epoch 25/100 Evaluation Batch 201/1758 loss 2.5785961151123047 accuracy: 0.3638671934604645\n",
      "Epoch 25/100 Evaluation Batch 301/1758 loss 2.612072229385376 accuracy: 0.34765625\n",
      "Epoch 25/100 Evaluation Batch 401/1758 loss 2.538119316101074 accuracy: 0.3617187440395355\n",
      "Epoch 25/100 Evaluation Batch 501/1758 loss 2.538926839828491 accuracy: 0.3695312440395355\n",
      "Epoch 25/100 Evaluation Batch 601/1758 loss 2.6507668495178223 accuracy: 0.36054688692092896\n",
      "Epoch 25/100 Evaluation Batch 701/1758 loss 2.504836320877075 accuracy: 0.37382814288139343\n",
      "Epoch 25/100 Evaluation Batch 801/1758 loss 2.3959949016571045 accuracy: 0.38300782442092896\n",
      "Epoch 25/100 Evaluation Batch 901/1758 loss 2.481841564178467 accuracy: 0.3636718690395355\n",
      "Epoch 25/100 Evaluation Batch 1001/1758 loss 2.5387842655181885 accuracy: 0.3695312440395355\n",
      "Epoch 25/100 Evaluation Batch 1101/1758 loss 2.645158052444458 accuracy: 0.34882813692092896\n",
      "Epoch 25/100 Evaluation Batch 1201/1758 loss 2.4766268730163574 accuracy: 0.37109375\n",
      "Epoch 25/100 Evaluation Batch 1301/1758 loss 2.555661916732788 accuracy: 0.3765625059604645\n",
      "Epoch 25/100 Evaluation Batch 1401/1758 loss 2.543825149536133 accuracy: 0.36015626788139343\n",
      "Epoch 25/100 Evaluation Batch 1501/1758 loss 2.5439064502716064 accuracy: 0.35820314288139343\n",
      "Epoch 25/100 Evaluation Batch 1601/1758 loss 2.455336570739746 accuracy: 0.3681640625\n",
      "Epoch 25/100 Evaluation Batch 1701/1758 loss 2.431833028793335 accuracy: 0.380859375\n",
      "Epoch 26/100 Training Batch 1/1758 loss 0.6652098894119263 accuracy: 0.7046875357627869\n",
      "Epoch 26/100 Training Batch 101/1758 loss 0.6402961015701294 accuracy: 0.713085949420929\n",
      "Epoch 26/100 Training Batch 201/1758 loss 0.6291254162788391 accuracy: 0.7291015982627869\n",
      "Epoch 26/100 Training Batch 301/1758 loss 0.5874606966972351 accuracy: 0.7388672232627869\n",
      "Epoch 26/100 Training Batch 401/1758 loss 0.6280809044837952 accuracy: 0.7201172113418579\n",
      "Epoch 26/100 Training Batch 501/1758 loss 0.6127446293830872 accuracy: 0.7220703363418579\n",
      "Epoch 26/100 Training Batch 601/1758 loss 0.5852980017662048 accuracy: 0.744335949420929\n",
      "Epoch 26/100 Training Batch 701/1758 loss 0.6085861325263977 accuracy: 0.7251953482627869\n",
      "Epoch 26/100 Training Batch 801/1758 loss 0.5867583155632019 accuracy: 0.731640636920929\n",
      "Epoch 26/100 Training Batch 901/1758 loss 0.6482017636299133 accuracy: 0.7035156488418579\n",
      "Epoch 26/100 Training Batch 1001/1758 loss 0.9582035541534424 accuracy: 0.567578136920929\n",
      "Epoch 26/100 Training Batch 1101/1758 loss 0.6277540326118469 accuracy: 0.7216796875\n",
      "Epoch 26/100 Training Batch 1201/1758 loss 0.6000024676322937 accuracy: 0.735546886920929\n",
      "Epoch 26/100 Training Batch 1301/1758 loss 0.8539397716522217 accuracy: 0.6103515625\n",
      "Epoch 26/100 Training Batch 1401/1758 loss 0.6024876236915588 accuracy: 0.7269531488418579\n",
      "Epoch 26/100 Training Batch 1501/1758 loss 0.5758189558982849 accuracy: 0.7357422113418579\n",
      "Epoch 26/100 Training Batch 1601/1758 loss 0.5675547122955322 accuracy: 0.743945300579071\n",
      "Epoch 26/100 Training Batch 1701/1758 loss 0.753718912601471 accuracy: 0.6617187857627869\n",
      "Epoch 26/100 Evaluation Batch 1/1758 loss 2.397650957107544 accuracy: 0.3685546815395355\n",
      "Epoch 26/100 Evaluation Batch 101/1758 loss 2.4937446117401123 accuracy: 0.3626953065395355\n",
      "Epoch 26/100 Evaluation Batch 201/1758 loss 2.460624933242798 accuracy: 0.3486328125\n",
      "Epoch 26/100 Evaluation Batch 301/1758 loss 2.4921486377716064 accuracy: 0.3609375059604645\n",
      "Epoch 26/100 Evaluation Batch 401/1758 loss 2.401806116104126 accuracy: 0.3564453125\n",
      "Epoch 26/100 Evaluation Batch 501/1758 loss 2.4577701091766357 accuracy: 0.36992189288139343\n",
      "Epoch 26/100 Evaluation Batch 601/1758 loss 2.4408721923828125 accuracy: 0.36308595538139343\n",
      "Epoch 26/100 Evaluation Batch 701/1758 loss 2.5045955181121826 accuracy: 0.345703125\n",
      "Epoch 26/100 Evaluation Batch 801/1758 loss 2.4735896587371826 accuracy: 0.36308595538139343\n",
      "Epoch 26/100 Evaluation Batch 901/1758 loss 2.4774062633514404 accuracy: 0.3431640565395355\n",
      "Epoch 26/100 Evaluation Batch 1001/1758 loss 2.3433120250701904 accuracy: 0.3687500059604645\n",
      "Epoch 26/100 Evaluation Batch 1101/1758 loss 2.5126426219940186 accuracy: 0.34550783038139343\n",
      "Epoch 26/100 Evaluation Batch 1201/1758 loss 2.48270583152771 accuracy: 0.3412109315395355\n",
      "Epoch 26/100 Evaluation Batch 1301/1758 loss 2.351900339126587 accuracy: 0.37519532442092896\n",
      "Epoch 26/100 Evaluation Batch 1401/1758 loss 2.406770944595337 accuracy: 0.3578124940395355\n",
      "Epoch 26/100 Evaluation Batch 1501/1758 loss 2.4375109672546387 accuracy: 0.3695312440395355\n",
      "Epoch 26/100 Evaluation Batch 1601/1758 loss 2.442387580871582 accuracy: 0.3525390625\n",
      "Epoch 26/100 Evaluation Batch 1701/1758 loss 2.396507978439331 accuracy: 0.3646484315395355\n",
      "Epoch 27/100 Training Batch 1/1758 loss 0.679973304271698 accuracy: 0.6937500238418579\n",
      "Epoch 27/100 Training Batch 101/1758 loss 0.7489438056945801 accuracy: 0.647656261920929\n",
      "Epoch 27/100 Training Batch 201/1758 loss 0.6178614497184753 accuracy: 0.7259765863418579\n",
      "Epoch 27/100 Training Batch 301/1758 loss 0.621300220489502 accuracy: 0.718945324420929\n",
      "Epoch 27/100 Training Batch 401/1758 loss 0.6427236795425415 accuracy: 0.7076172232627869\n",
      "Epoch 27/100 Training Batch 501/1758 loss 0.6289880871772766 accuracy: 0.7037109732627869\n",
      "Epoch 27/100 Training Batch 601/1758 loss 0.6230288147926331 accuracy: 0.723828136920929\n",
      "Epoch 27/100 Training Batch 701/1758 loss 0.6148671507835388 accuracy: 0.7271484732627869\n",
      "Epoch 27/100 Training Batch 801/1758 loss 0.5966512560844421 accuracy: 0.7353515625\n",
      "Epoch 27/100 Training Batch 901/1758 loss 0.5977372527122498 accuracy: 0.728515625\n",
      "Epoch 27/100 Training Batch 1001/1758 loss 0.5764889121055603 accuracy: 0.7416015863418579\n",
      "Epoch 27/100 Training Batch 1101/1758 loss 0.6394557952880859 accuracy: 0.7144531607627869\n",
      "Epoch 27/100 Training Batch 1201/1758 loss 0.5823706388473511 accuracy: 0.747851550579071\n",
      "Epoch 27/100 Training Batch 1301/1758 loss 0.7753346562385559 accuracy: 0.6517578363418579\n",
      "Epoch 27/100 Training Batch 1401/1758 loss 0.6015782356262207 accuracy: 0.727734386920929\n",
      "Epoch 27/100 Training Batch 1501/1758 loss 0.6102662682533264 accuracy: 0.723437488079071\n",
      "Epoch 27/100 Training Batch 1601/1758 loss 0.5905845165252686 accuracy: 0.7406250238418579\n",
      "Epoch 27/100 Training Batch 1701/1758 loss 0.6046888828277588 accuracy: 0.729687511920929\n",
      "Epoch 27/100 Evaluation Batch 1/1758 loss 3.2031354904174805 accuracy: 0.35761719942092896\n",
      "Epoch 27/100 Evaluation Batch 101/1758 loss 2.9157114028930664 accuracy: 0.3994140625\n",
      "Epoch 27/100 Evaluation Batch 201/1758 loss 3.102977991104126 accuracy: 0.36113283038139343\n",
      "Epoch 27/100 Evaluation Batch 301/1758 loss 3.0044658184051514 accuracy: 0.3876953125\n",
      "Epoch 27/100 Evaluation Batch 401/1758 loss 3.1935367584228516 accuracy: 0.36503908038139343\n",
      "Epoch 27/100 Evaluation Batch 501/1758 loss 3.0584936141967773 accuracy: 0.38066408038139343\n",
      "Epoch 27/100 Evaluation Batch 601/1758 loss 2.9507389068603516 accuracy: 0.3919921815395355\n",
      "Epoch 27/100 Evaluation Batch 701/1758 loss 3.0736401081085205 accuracy: 0.3726562559604645\n",
      "Epoch 27/100 Evaluation Batch 801/1758 loss 2.894756555557251 accuracy: 0.41386720538139343\n",
      "Epoch 27/100 Evaluation Batch 901/1758 loss 3.0514156818389893 accuracy: 0.365234375\n",
      "Epoch 27/100 Evaluation Batch 1001/1758 loss 3.132828950881958 accuracy: 0.36738282442092896\n",
      "Epoch 27/100 Evaluation Batch 1101/1758 loss 3.0502192974090576 accuracy: 0.390625\n",
      "Epoch 27/100 Evaluation Batch 1201/1758 loss 3.1672725677490234 accuracy: 0.35625001788139343\n",
      "Epoch 27/100 Evaluation Batch 1301/1758 loss 2.9770781993865967 accuracy: 0.39667969942092896\n",
      "Epoch 27/100 Evaluation Batch 1401/1758 loss 3.0726640224456787 accuracy: 0.36152344942092896\n",
      "Epoch 27/100 Evaluation Batch 1501/1758 loss 3.1221086978912354 accuracy: 0.38359376788139343\n",
      "Epoch 27/100 Evaluation Batch 1601/1758 loss 3.0346014499664307 accuracy: 0.3779296875\n",
      "Epoch 27/100 Evaluation Batch 1701/1758 loss 3.1764910221099854 accuracy: 0.3609375059604645\n",
      "Epoch 28/100 Training Batch 1/1758 loss 0.593802273273468 accuracy: 0.7291015982627869\n",
      "Epoch 28/100 Training Batch 101/1758 loss 0.5410641431808472 accuracy: 0.7535156607627869\n",
      "Epoch 28/100 Training Batch 201/1758 loss 0.5898551344871521 accuracy: 0.7251953482627869\n",
      "Epoch 28/100 Training Batch 301/1758 loss 0.5505035519599915 accuracy: 0.75390625\n",
      "Epoch 28/100 Training Batch 401/1758 loss 0.5864756107330322 accuracy: 0.73828125\n",
      "Epoch 28/100 Training Batch 501/1758 loss 0.6037886738777161 accuracy: 0.7339844107627869\n",
      "Epoch 28/100 Training Batch 601/1758 loss 0.5844647288322449 accuracy: 0.7392578125\n",
      "Epoch 28/100 Training Batch 701/1758 loss 0.5675808787345886 accuracy: 0.7431640625\n",
      "Epoch 28/100 Training Batch 801/1758 loss 0.5548043251037598 accuracy: 0.7457031607627869\n",
      "Epoch 28/100 Training Batch 901/1758 loss 1.0613535642623901 accuracy: 0.510546863079071\n",
      "Epoch 28/100 Training Batch 1001/1758 loss 0.5802994966506958 accuracy: 0.743945300579071\n",
      "Epoch 28/100 Training Batch 1101/1758 loss 0.5914656519889832 accuracy: 0.726367175579071\n",
      "Epoch 28/100 Training Batch 1201/1758 loss 0.5807421207427979 accuracy: 0.745898425579071\n",
      "Epoch 28/100 Training Batch 1301/1758 loss 0.5622026920318604 accuracy: 0.744335949420929\n",
      "Epoch 28/100 Training Batch 1401/1758 loss 0.5764979124069214 accuracy: 0.7353515625\n",
      "Epoch 28/100 Training Batch 1501/1758 loss 0.5445411205291748 accuracy: 0.7505859732627869\n",
      "Epoch 28/100 Training Batch 1601/1758 loss 0.5595995187759399 accuracy: 0.7437500357627869\n",
      "Epoch 28/100 Training Batch 1701/1758 loss 1.1998361349105835 accuracy: 0.48554688692092896\n",
      "Epoch 28/100 Evaluation Batch 1/1758 loss 2.7767465114593506 accuracy: 0.33710938692092896\n",
      "Epoch 28/100 Evaluation Batch 101/1758 loss 2.713671922683716 accuracy: 0.3501953184604645\n",
      "Epoch 28/100 Evaluation Batch 201/1758 loss 2.7066380977630615 accuracy: 0.3531250059604645\n",
      "Epoch 28/100 Evaluation Batch 301/1758 loss 2.596144199371338 accuracy: 0.3642578125\n",
      "Epoch 28/100 Evaluation Batch 401/1758 loss 2.729480504989624 accuracy: 0.34394532442092896\n",
      "Epoch 28/100 Evaluation Batch 501/1758 loss 2.5480659008026123 accuracy: 0.3746093809604645\n",
      "Epoch 28/100 Evaluation Batch 601/1758 loss 2.6163125038146973 accuracy: 0.36796876788139343\n",
      "Epoch 28/100 Evaluation Batch 701/1758 loss 2.626178503036499 accuracy: 0.35820314288139343\n",
      "Epoch 28/100 Evaluation Batch 801/1758 loss 2.751807451248169 accuracy: 0.3384765684604645\n",
      "Epoch 28/100 Evaluation Batch 901/1758 loss 2.5653116703033447 accuracy: 0.3744140565395355\n",
      "Epoch 28/100 Evaluation Batch 1001/1758 loss 2.774250030517578 accuracy: 0.34296876192092896\n",
      "Epoch 28/100 Evaluation Batch 1101/1758 loss 2.7531957626342773 accuracy: 0.35429689288139343\n",
      "Epoch 28/100 Evaluation Batch 1201/1758 loss 2.613240957260132 accuracy: 0.3677734434604645\n",
      "Epoch 28/100 Evaluation Batch 1301/1758 loss 2.7812299728393555 accuracy: 0.3388671875\n",
      "Epoch 28/100 Evaluation Batch 1401/1758 loss 2.649332284927368 accuracy: 0.357421875\n",
      "Epoch 28/100 Evaluation Batch 1501/1758 loss 2.713653326034546 accuracy: 0.36542969942092896\n",
      "Epoch 28/100 Evaluation Batch 1601/1758 loss 2.652329206466675 accuracy: 0.3628906309604645\n",
      "Epoch 28/100 Evaluation Batch 1701/1758 loss 2.666339635848999 accuracy: 0.3492187559604645\n",
      "Epoch 29/100 Training Batch 1/1758 loss 0.7009172439575195 accuracy: 0.677734375\n",
      "Epoch 29/100 Training Batch 101/1758 loss 0.5988718867301941 accuracy: 0.733203113079071\n",
      "Epoch 29/100 Training Batch 201/1758 loss 0.586926281452179 accuracy: 0.7386718988418579\n",
      "Epoch 29/100 Training Batch 301/1758 loss 0.5721094608306885 accuracy: 0.7505859732627869\n",
      "Epoch 29/100 Training Batch 401/1758 loss 0.6011971831321716 accuracy: 0.7265625\n",
      "Epoch 29/100 Training Batch 501/1758 loss 0.5744214653968811 accuracy: 0.743359386920929\n",
      "Epoch 29/100 Training Batch 601/1758 loss 0.5859347581863403 accuracy: 0.736523449420929\n",
      "Epoch 29/100 Training Batch 701/1758 loss 0.5699446201324463 accuracy: 0.7431640625\n",
      "Epoch 29/100 Training Batch 801/1758 loss 0.5836473107337952 accuracy: 0.733203113079071\n",
      "Epoch 29/100 Training Batch 901/1758 loss 0.5554089546203613 accuracy: 0.744140625\n",
      "Epoch 29/100 Training Batch 1001/1758 loss 0.5539042949676514 accuracy: 0.7525390982627869\n",
      "Epoch 29/100 Training Batch 1101/1758 loss 0.5461617112159729 accuracy: 0.758984386920929\n",
      "Epoch 29/100 Training Batch 1201/1758 loss 0.5743066668510437 accuracy: 0.742382824420929\n",
      "Epoch 29/100 Training Batch 1301/1758 loss 0.5624992251396179 accuracy: 0.7529296875\n",
      "Epoch 29/100 Training Batch 1401/1758 loss 0.5484184622764587 accuracy: 0.7542968988418579\n",
      "Epoch 29/100 Training Batch 1501/1758 loss 0.5551589131355286 accuracy: 0.7490234375\n",
      "Epoch 29/100 Training Batch 1601/1758 loss 0.5408176779747009 accuracy: 0.7593750357627869\n",
      "Epoch 29/100 Training Batch 1701/1758 loss 0.5564391016960144 accuracy: 0.747851550579071\n",
      "Epoch 29/100 Evaluation Batch 1/1758 loss 3.1234567165374756 accuracy: 0.3828125\n",
      "Epoch 29/100 Evaluation Batch 101/1758 loss 3.209137201309204 accuracy: 0.365234375\n",
      "Epoch 29/100 Evaluation Batch 201/1758 loss 3.168617010116577 accuracy: 0.36015626788139343\n",
      "Epoch 29/100 Evaluation Batch 301/1758 loss 3.113776206970215 accuracy: 0.3794921934604645\n",
      "Epoch 29/100 Evaluation Batch 401/1758 loss 3.142136335372925 accuracy: 0.3755859434604645\n",
      "Epoch 29/100 Evaluation Batch 501/1758 loss 3.0247690677642822 accuracy: 0.39179688692092896\n",
      "Epoch 29/100 Evaluation Batch 601/1758 loss 3.1996028423309326 accuracy: 0.3734374940395355\n",
      "Epoch 29/100 Evaluation Batch 701/1758 loss 3.1691620349884033 accuracy: 0.37812501192092896\n",
      "Epoch 29/100 Evaluation Batch 801/1758 loss 3.202800750732422 accuracy: 0.36992189288139343\n",
      "Epoch 29/100 Evaluation Batch 901/1758 loss 3.063426971435547 accuracy: 0.3974609375\n",
      "Epoch 29/100 Evaluation Batch 1001/1758 loss 3.1581060886383057 accuracy: 0.3824218809604645\n",
      "Epoch 29/100 Evaluation Batch 1101/1758 loss 3.207624673843384 accuracy: 0.36738282442092896\n",
      "Epoch 29/100 Evaluation Batch 1201/1758 loss 3.182652473449707 accuracy: 0.36054688692092896\n",
      "Epoch 29/100 Evaluation Batch 1301/1758 loss 3.1244490146636963 accuracy: 0.3681640625\n",
      "Epoch 29/100 Evaluation Batch 1401/1758 loss 3.075246810913086 accuracy: 0.37285158038139343\n",
      "Epoch 29/100 Evaluation Batch 1501/1758 loss 3.0548222064971924 accuracy: 0.37617188692092896\n",
      "Epoch 29/100 Evaluation Batch 1601/1758 loss 3.0575153827667236 accuracy: 0.3818359375\n",
      "Epoch 29/100 Evaluation Batch 1701/1758 loss 3.0397167205810547 accuracy: 0.3896484375\n",
      "Epoch 30/100 Training Batch 1/1758 loss 0.548504114151001 accuracy: 0.758007824420929\n",
      "Epoch 30/100 Training Batch 101/1758 loss 0.5595955848693848 accuracy: 0.745312511920929\n",
      "Epoch 30/100 Training Batch 201/1758 loss 0.6207601428031921 accuracy: 0.71875\n",
      "Epoch 30/100 Training Batch 301/1758 loss 0.5547850728034973 accuracy: 0.7476562857627869\n",
      "Epoch 30/100 Training Batch 401/1758 loss 0.7382479310035706 accuracy: 0.6708984375\n",
      "Epoch 30/100 Training Batch 501/1758 loss 0.5438869595527649 accuracy: 0.765820324420929\n",
      "Epoch 30/100 Training Batch 601/1758 loss 0.5607030987739563 accuracy: 0.7496094107627869\n",
      "Epoch 30/100 Training Batch 701/1758 loss 0.5480475425720215 accuracy: 0.756054699420929\n",
      "Epoch 30/100 Training Batch 801/1758 loss 0.5332936644554138 accuracy: 0.765625\n",
      "Epoch 30/100 Training Batch 901/1758 loss 0.5540871024131775 accuracy: 0.7542968988418579\n",
      "Epoch 30/100 Training Batch 1001/1758 loss 0.5457097887992859 accuracy: 0.754687488079071\n",
      "Epoch 30/100 Training Batch 1101/1758 loss 0.554889976978302 accuracy: 0.746289074420929\n",
      "Epoch 30/100 Training Batch 1201/1758 loss 0.56224125623703 accuracy: 0.7548828125\n",
      "Epoch 30/100 Training Batch 1301/1758 loss 0.5363728404045105 accuracy: 0.7642578482627869\n",
      "Epoch 30/100 Training Batch 1401/1758 loss 0.5545753836631775 accuracy: 0.758984386920929\n",
      "Epoch 30/100 Training Batch 1501/1758 loss 0.5496386885643005 accuracy: 0.7496094107627869\n",
      "Epoch 30/100 Training Batch 1601/1758 loss 0.5494256019592285 accuracy: 0.7544922232627869\n",
      "Epoch 30/100 Training Batch 1701/1758 loss 0.5540845990180969 accuracy: 0.7509765625\n",
      "Epoch 30/100 Evaluation Batch 1/1758 loss 3.2354342937469482 accuracy: 0.3824218809604645\n",
      "Epoch 30/100 Evaluation Batch 101/1758 loss 3.1036341190338135 accuracy: 0.38691407442092896\n",
      "Epoch 30/100 Evaluation Batch 201/1758 loss 3.074023485183716 accuracy: 0.40742188692092896\n",
      "Epoch 30/100 Evaluation Batch 301/1758 loss 3.1084864139556885 accuracy: 0.3994140625\n",
      "Epoch 30/100 Evaluation Batch 401/1758 loss 3.3801441192626953 accuracy: 0.3619140684604645\n",
      "Epoch 30/100 Evaluation Batch 501/1758 loss 2.966937780380249 accuracy: 0.4017578065395355\n",
      "Epoch 30/100 Evaluation Batch 601/1758 loss 3.143378257751465 accuracy: 0.3818359375\n",
      "Epoch 30/100 Evaluation Batch 701/1758 loss 3.2786853313446045 accuracy: 0.3818359375\n",
      "Epoch 30/100 Evaluation Batch 801/1758 loss 3.30138897895813 accuracy: 0.38066408038139343\n",
      "Epoch 30/100 Evaluation Batch 901/1758 loss 3.1164238452911377 accuracy: 0.3880859315395355\n",
      "Epoch 30/100 Evaluation Batch 1001/1758 loss 3.0791337490081787 accuracy: 0.400390625\n",
      "Epoch 30/100 Evaluation Batch 1101/1758 loss 3.1963882446289062 accuracy: 0.39179688692092896\n",
      "Epoch 30/100 Evaluation Batch 1201/1758 loss 3.0460855960845947 accuracy: 0.40800783038139343\n",
      "Epoch 30/100 Evaluation Batch 1301/1758 loss 3.219125747680664 accuracy: 0.37382814288139343\n",
      "Epoch 30/100 Evaluation Batch 1401/1758 loss 3.0899574756622314 accuracy: 0.3892578184604645\n",
      "Epoch 30/100 Evaluation Batch 1501/1758 loss 3.091297149658203 accuracy: 0.38652345538139343\n",
      "Epoch 30/100 Evaluation Batch 1601/1758 loss 3.0627267360687256 accuracy: 0.4048828184604645\n",
      "Epoch 30/100 Evaluation Batch 1701/1758 loss 2.9877452850341797 accuracy: 0.4091796875\n",
      "Epoch 31/100 Training Batch 1/1758 loss 0.5473971962928772 accuracy: 0.7587890625\n",
      "Epoch 31/100 Training Batch 101/1758 loss 0.5256560444831848 accuracy: 0.7630859613418579\n",
      "Epoch 31/100 Training Batch 201/1758 loss 0.6251283884048462 accuracy: 0.7060546875\n",
      "Epoch 31/100 Training Batch 301/1758 loss 0.5664147138595581 accuracy: 0.7455078363418579\n",
      "Epoch 31/100 Training Batch 401/1758 loss 0.5835702419281006 accuracy: 0.747851550579071\n",
      "Epoch 31/100 Training Batch 501/1758 loss 0.5124063491821289 accuracy: 0.7720703482627869\n",
      "Epoch 31/100 Training Batch 601/1758 loss 0.5513116717338562 accuracy: 0.7544922232627869\n",
      "Epoch 31/100 Training Batch 701/1758 loss 0.6041919589042664 accuracy: 0.7201172113418579\n",
      "Epoch 31/100 Training Batch 801/1758 loss 0.550879955291748 accuracy: 0.7542968988418579\n",
      "Epoch 31/100 Training Batch 901/1758 loss 0.5244545936584473 accuracy: 0.7681640982627869\n",
      "Epoch 31/100 Training Batch 1001/1758 loss 1.001901388168335 accuracy: 0.55859375\n",
      "Epoch 31/100 Training Batch 1101/1758 loss 0.5682722926139832 accuracy: 0.746289074420929\n",
      "Epoch 31/100 Training Batch 1201/1758 loss 0.6178081631660461 accuracy: 0.723437488079071\n",
      "Epoch 31/100 Training Batch 1301/1758 loss 0.5860356688499451 accuracy: 0.74609375\n",
      "Epoch 31/100 Training Batch 1401/1758 loss 0.5676875114440918 accuracy: 0.7466797232627869\n",
      "Epoch 31/100 Training Batch 1501/1758 loss 0.5547277927398682 accuracy: 0.7496094107627869\n",
      "Epoch 31/100 Training Batch 1601/1758 loss 0.5354990363121033 accuracy: 0.765820324420929\n",
      "Epoch 31/100 Training Batch 1701/1758 loss 0.5454924702644348 accuracy: 0.756640613079071\n",
      "Epoch 31/100 Evaluation Batch 1/1758 loss 2.872022867202759 accuracy: 0.3994140625\n",
      "Epoch 31/100 Evaluation Batch 101/1758 loss 2.979174852371216 accuracy: 0.37480470538139343\n",
      "Epoch 31/100 Evaluation Batch 201/1758 loss 2.8724467754364014 accuracy: 0.3912109434604645\n",
      "Epoch 31/100 Evaluation Batch 301/1758 loss 2.9556424617767334 accuracy: 0.3921875059604645\n",
      "Epoch 31/100 Evaluation Batch 401/1758 loss 3.212531805038452 accuracy: 0.37128907442092896\n",
      "Epoch 31/100 Evaluation Batch 501/1758 loss 3.0760104656219482 accuracy: 0.3822265565395355\n",
      "Epoch 31/100 Evaluation Batch 601/1758 loss 3.048295736312866 accuracy: 0.3697265684604645\n",
      "Epoch 31/100 Evaluation Batch 701/1758 loss 2.923978567123413 accuracy: 0.39375001192092896\n",
      "Epoch 31/100 Evaluation Batch 801/1758 loss 2.935002088546753 accuracy: 0.38300782442092896\n",
      "Epoch 31/100 Evaluation Batch 901/1758 loss 3.051137924194336 accuracy: 0.38007813692092896\n",
      "Epoch 31/100 Evaluation Batch 1001/1758 loss 2.931851625442505 accuracy: 0.39433595538139343\n",
      "Epoch 31/100 Evaluation Batch 1101/1758 loss 2.9710733890533447 accuracy: 0.38789063692092896\n",
      "Epoch 31/100 Evaluation Batch 1201/1758 loss 2.8431742191314697 accuracy: 0.4033203125\n",
      "Epoch 31/100 Evaluation Batch 1301/1758 loss 3.0053184032440186 accuracy: 0.3759765625\n",
      "Epoch 31/100 Evaluation Batch 1401/1758 loss 2.9710705280303955 accuracy: 0.38554689288139343\n",
      "Epoch 31/100 Evaluation Batch 1501/1758 loss 2.919219732284546 accuracy: 0.39238283038139343\n",
      "Epoch 31/100 Evaluation Batch 1601/1758 loss 2.9394540786743164 accuracy: 0.39277344942092896\n",
      "Epoch 31/100 Evaluation Batch 1701/1758 loss 2.8905298709869385 accuracy: 0.3949218690395355\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train(model_one, train_dataloader, train_dataloader, optim_one, \u001b[39m\"\u001b[39;49m\u001b[39m../models/ptr_net\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[51], line 19\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, eval_dataloader, optim, model_path, n_epochs)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     17\u001b[0m     model\u001b[39m.\u001b[39meval()\n\u001b[0;32m---> 19\u001b[0m probabilities, _ \u001b[39m=\u001b[39m model(x_train, y_train)\n\u001b[1;32m     21\u001b[0m targets \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mzeros((y_train\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], n_out \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m), dtype\u001b[39m=\u001b[39mt\u001b[39m.\u001b[39mlong, device\u001b[39m=\u001b[39mx_train\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     22\u001b[0m targets[:,:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m=\u001b[39m y_train\n",
      "File \u001b[0;32m~/anaconda3/envs/german/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[32], line 99\u001b[0m, in \u001b[0;36mPtrNet2.forward\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     96\u001b[0m A \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39mempty(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_out \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, batch_size, in_len \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, dtype\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     98\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_out \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> 99\u001b[0m     _, (decoder_current_hidden, decoder_current_cell_state) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(decoder_current_input, (decoder_current_hidden, decoder_current_cell_state))\n\u001b[1;32m    100\u001b[0m     TANH_i \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW1 \u001b[39m@\u001b[39m encoder_all_hiddens\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mW2 \u001b[39m@\u001b[39m decoder_current_hidden\u001b[39m.\u001b[39mpermute(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m0\u001b[39m))\u001b[39m.\u001b[39mtanh()\n\u001b[1;32m    101\u001b[0m     U_i    \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mv \u001b[39m@\u001b[39m TANH_i\n",
      "File \u001b[0;32m~/anaconda3/envs/german/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/german/lib/python3.11/site-packages/torch/nn/modules/rnn.py:761\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    759\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, hx\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m    760\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mis_scripting():\n\u001b[0;32m--> 761\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_weights_have_changed():\n\u001b[1;32m    762\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init_flat_weights()\n\u001b[1;32m    764\u001b[0m     orig_input \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/german/lib/python3.11/site-packages/torch/nn/modules/rnn.py:246\u001b[0m, in \u001b[0;36mRNNBase._weights_have_changed\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    244\u001b[0m weights_changed \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    245\u001b[0m \u001b[39mfor\u001b[39;00m ref, name \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weight_refs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_weights_names):\n\u001b[0;32m--> 246\u001b[0m     weight \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, name) \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    247\u001b[0m     \u001b[39mif\u001b[39;00m weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ref \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m ref() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m weight:\n\u001b[1;32m    248\u001b[0m         weights_changed \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/german/lib/python3.11/site-packages/torch/nn/modules/module.py:1601\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1598\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_backward_pre_hooks\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1599\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39m=\u001b[39m OrderedDict()\n\u001b[0;32m-> 1601\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getattr__\u001b[39m(\u001b[39mself\u001b[39m, name: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Tensor, \u001b[39m'\u001b[39m\u001b[39mModule\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m   1602\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m:\n\u001b[1;32m   1603\u001b[0m         _parameters \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m[\u001b[39m'\u001b[39m\u001b[39m_parameters\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model_one, train_dataloader, train_dataloader, optim_one, \"../models/ptr_net\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test, y_test = convex_hull_dataset[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 10, 2]), torch.Size([3, 11]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_one.train()\n",
    "p, _ = model_one(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.argmax(dim=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "german",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
