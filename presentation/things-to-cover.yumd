1. definitions (constituency tree, dependency tree, discontinuous)
2. existing solutions
3. architecture: show what is new
   1. embeddings (word + character-level + "modifiers") (modifiers are new)
   2. encoder + decoder
   3. biaffine -- conditional probability
      1. attachment scores
      2. order scores
      3. morphology
   4. batch beam search algorithm (fast)
4. results
problems with probabilistic parsers
review problems I encountered and how I solved them

review uses for this

regularisation techniques
review the multitask paper


things to do:
1. get results
2. learn markov chains
3. revise deep learning book
4. learn first 10 chapters of info theory
5. revise word2vec and sent2vec
6. revise how transformers and lstms work
7. revise tree-based methods
8. bias-variance tradeoff
9. copy the algorithms in "parsing as reduction" into the appendix
1. overview of compsci ways to parse
1. understand what the f1 score means
1. find out about the continuous consittuency parsing for english
1. give words to the "latent factors"

all the mlps have nonlinearities!202ß

skip-gram with nega- tive sampling model  (word2vec)

make a slide justifying why your method now spans the entire **biaffine space**

what does disc f1 mean???

Daniel Kondratyuk, Tomás Gavenciak, Milan Straka, and Jan Hajic
(2018), LemmaTag: Jointly tagging and lemmatizing for morphologically-rich
languages with BRNNs

what is bert?

overview of all the cited/compard models