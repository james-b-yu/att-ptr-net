1. definitions (constituency tree, dependency tree, discontinuous)
2. existing solutions
3. architecture: show what is new
   1. embeddings (word + character-level + "modifiers") (modifiers are new)
   2. encoder + decoder
   3. biaffine -- conditional probability
      1. attachment scores
      2. order scores
      3. morphology
   4. batch beam search algorithm (fast)
4. results
problems with probabilistic parsers
review problems I encountered and how I solved them

review uses for this

regularisation techniques
review the multitask paper


things to do:
1. DONE get results
1. DONE overview of compsci ways to parse
1. DONE understand what the f1 score means
1. DONE find out about the continuous consittuency parsing for english
1. DONE overview of all the cited/compard models
2. learn markov chains
3. revise deep learning book
   - DONE regularisation
   - DONE optimisation
4. learn first 10 chapters of info theory
6. revise how transformers and lstms work
   - what is bert?
7. revise tree-based methods
8. bias-variance tradeoff
1. give words to the "latent factors"
5. revise word2vec and sent2vec
   - skip-gram with negative sampling model  (word2vec)
1. resnet
1. why cross entropy, not mse
1. why softmax?

all the mlps have nonlinearities!

QUADRATIC DISCRIMINANT ANALYSIS
how to improve model
appliations
what is reinforcement learning?!!
SVM
