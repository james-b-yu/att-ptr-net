{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from german_parser.util.const import CONSTS\n",
    "from german_parser.util.dataloader import TigerDatasetGenerator\n",
    "import dill as pickle\n",
    "from string import punctuation\n",
    "from german_parser.model import TigerModel\n",
    "from german_parser.util import BatchUnionFind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataloader, train_new_words), (dev_dataloader, dev_new_words), (test_dataloader, test_new_words), character_set, character_flag_generators, inverse_word_dict, inverse_sym_dict = pickle.load(open(\"required_vars.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TigerModel(\n",
    "#     word_embedding_params=TigerModel.WordEmbeddingParams(char_set=character_set, char_flag_generators=character_flag_generators, char_internal_embedding_dim=100,\n",
    "#                                    char_part_embedding_dim=100, \n",
    "#                                    word_part_embedding_dim=100, \n",
    "#                                    char_internal_window_size=3,\n",
    "#                                    word_dict=inverse_word_dict),\n",
    "#     enc_lstm_params=TigerModel.LSTMParams(\n",
    "#         hidden_size=512,\n",
    "#         bidirectional=True,\n",
    "#         num_layers=3),\n",
    "#     dec_lstm_params=TigerModel.LSTMParams(\n",
    "#         hidden_size=512,\n",
    "#         bidirectional=False,\n",
    "#         num_layers=1\n",
    "#         ),\n",
    "#         enc_attention_mlp_dim=512,\n",
    "#         dec_attention_mlp_dim=512,\n",
    "#         enc_label_mlp_dim=128,\n",
    "#         dec_label_mlp_dim=128,\n",
    "#         num_biaffine_attention_classes=2,\n",
    "#         num_constituent_labels=len(inverse_sym_dict),\n",
    "#         enc_attachment_mlp_dim=128,\n",
    "#         dec_attachment_mlp_dim=64,\n",
    "#         max_attachment_order=train_dataloader.dataset.attachment_orders.max() + 1\n",
    "#     )\n",
    "# model = model.to(device=\"cuda\") # type: ignore\n",
    "\n",
    "model: TigerModel = pickle.load(open(\"./models/tiger_model_2023_09_04-02_35_02_PM_epoch_24.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (x, l, target_ex, target_lab, target_att) = next(enumerate(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(\"cpu\")\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tree(model: TigerModel, input: tuple[torch.Tensor, torch.Tensor], new_words_dict: dict[int, str] | None):\n",
    "    \"\"\"NOTE: indices are 1-indexed. index 0 corresponds to virtual root of D-tree (which is different from virtual root of C-tree)\n",
    "\n",
    "    Args:\n",
    "        model (TigerModel): _description_\n",
    "        input (tuple[torch.Tensor, torch.Tensor]): _description_\n",
    "        new_words_dict (dict[int, str] | None): _description_\n",
    "    \"\"\"    \n",
    "    x, lengths = input\n",
    "    self_attention, constituent_labels, attachment_orders, indices = model.forward(input, new_words_dict)\n",
    "    # self_attention has size (B, T, T + 1)\n",
    "\n",
    "    B, T, *_ = self_attention.shape\n",
    "    uf = BatchUnionFind(B, model.beam_size, N=T + 1, device=model.dummy_param.device)\n",
    "\n",
    "    joint_logits = torch.zeros(B, model.beam_size, device=model.dummy_param.device) # (B, K)\n",
    "\n",
    "    edges = torch.zeros(B, model.beam_size, T, dtype=torch.long, device=model.dummy_param.device) # (B, K, T); m[b, k, t - 1] is the 1-indexed parent of 1-indexed node t, in batch b, beam k\n",
    "\n",
    "    for t in range(T):\n",
    "        relevant_batches = t < lengths # used for updating the final arcs. otherwise, we get nans in batch b if t >= sentence_length[b]\n",
    "\n",
    "        arc_probs = self_attention[:, t, :].log_softmax(dim=-1) # (B, T + 1)\n",
    "\n",
    "        candidate_joint_logits = joint_logits[:, :, None] + arc_probs[:, None, :] # (B, K, T + 1); the heuristic we would like to maximise\n",
    "\n",
    "        children = torch.tensor(t + 1, device=model.dummy_param.device) # all batches and beams share the same child index (1-indexed)\n",
    "        parents = torch.arange(0, T + 1, 1, device=model.dummy_param.device, dtype=torch.long).repeat(B, model.beam_size, 1) # (B, K, T + 1), where m[b, k, t] = t to represent the index of each parent \n",
    "\n",
    "        no_cycles_mask = uf.is_same_set(children.expand_as(parents), parents) # (B, K, T + 1); m[b, k, s + 1] is true if in batch b, beam k, joining child (t + 1) and parent (s + 1) would lead to a cycle\n",
    "        no_cycles_mask[:, :, 1:] |= ~indices.unsqueeze(1).to(device=model.dummy_param.device)\n",
    "\n",
    "        candidate_joint_logits[no_cycles_mask] = -torch.inf # mask cycles out so they can never be a maximiser\n",
    "        flattened_top_candidate_idx = candidate_joint_logits.flatten(-2, -1).topk(k=model.beam_size, dim=-1).indices # (B, K) in range [0, (K * T + 1)); for each batch, find top 10 best performing parent-beam combinations\n",
    "        \n",
    "        top_parents = parents.flatten(-2, -1).gather(index=flattened_top_candidate_idx, dim=-1) # (B, K); for each batch and beam, get the 1-indexed id of the parent we want to attach\n",
    "\n",
    "        batch_names = torch.arange(model.beam_size, device=model.dummy_param.device, dtype=torch.long).view(1, -1, 1).expand(B, -1, T + 1).flatten(-2, -1) # (B, K, T + 1); m[b, k, s] = k for all b, s, k\n",
    "        used_batches = batch_names.gather(index=flattened_top_candidate_idx, dim=-1) # (B, K) where each element is in the range [0, K). m[b, k] tells you what the kth new beam should be\n",
    "\n",
    "        new_data = uf.data.gather(index=used_batches.unsqueeze(-1).expand_as(uf.data), dim=1)\n",
    "        new_rank = uf.rank.gather(index=used_batches.unsqueeze(-1).expand_as(uf.rank), dim=1)\n",
    "        new_edges = edges.gather(index=used_batches.unsqueeze(-1).expand_as(edges), dim=1)\n",
    "        new_joint_logits = candidate_joint_logits.flatten(-2, -1).gather(index=flattened_top_candidate_idx, dim=-1)\n",
    "\n",
    "        uf.data[relevant_batches] = new_data[relevant_batches]\n",
    "        uf.rank[relevant_batches] = new_rank[relevant_batches]\n",
    "        edges[relevant_batches] = new_edges[relevant_batches]\n",
    "        joint_logits[relevant_batches] = new_joint_logits[relevant_batches]\n",
    "\n",
    "        uf.union(children.expand_as(top_parents), top_parents)\n",
    "\n",
    "        edges[:, :, t] = top_parents\n",
    "\n",
    "        pass\n",
    "\n",
    "    num_labels = constituent_labels.shape[-1]\n",
    "    num_attachment_orders = attachment_orders.shape[-1]\n",
    "\n",
    "    best_edges = edges[torch.arange(edges.shape[0]), joint_logits.argmax(dim=-1)] # (B, T) containing elements in range [0, T + 1), where m[b, t - 1] denotes the 1-indexed parent of 1-indexed node t\n",
    "\n",
    "    label_logits_best_edges = constituent_labels.gather(index=best_edges[:, :, None, None].expand(-1, -1, -1, num_labels), dim=2).squeeze(2)\n",
    "    attachment_logits_best_edges = attachment_orders.gather(index=best_edges[:, :, None, None].expand(-1, -1, -1, num_attachment_orders), dim=2).squeeze(2)\n",
    "\n",
    "    labels_best_edges = label_logits_best_edges.argmax(-1)\n",
    "    attachment_orders_best_edges = attachment_logits_best_edges.argmax(-1)\n",
    "\n",
    "    return best_edges, labels_best_edges, attachment_orders_best_edges, (edges, joint_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_edges, labels_best_edges, attachment_orders_best_edges, (edges, joint_logits) = find_tree(model, (x, l), test_new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, (x, l, *_) in enumerate(test_dataloader):\n",
    "#     best_edges, labels_best_edges, attachment_orders_best_edges, (edges, joint_logits) = find_tree(model, (x, l), test_new_words)\n",
    "#     print(f\"{n} of {len(test_dataloader)}\", end=\"\\r\")\n",
    "#     for i in range(len(x)):\n",
    "#         if not joint_logits[i].allclose(joint_logits[i,0]):\n",
    "#             print(\"YES!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_num = 2\n",
    "ex = best_edges[s_num, :l[s_num]].to(\"cpu\")\n",
    "lab = labels_best_edges[s_num, :l[s_num]]\n",
    "att = attachment_orders_best_edges[s_num, :l[s_num]]\n",
    "\n",
    "t_ex = target_ex[s_num, :l[s_num]].to(\"cpu\")\n",
    "t_lab = target_lab[s_num, :l[s_num]].to(\"cpu\")\n",
    "t_att = target_att[s_num, :l[s_num]].to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreePlot[{{27->1, \"VP#1\"}, {1->2, \"PP#1\"}, {1->3, \"PP#1\"}, {1->4, \"PP#1\"}, {4->5, \"PP#1\"}, {4->6, \"PP#1\"}, {4->7, \"PP#1\"}, {1->8, \"PP#1\"}, {8->9, \"PP#1\"}, {20->10, \"VROOT#2\"}, {16->11, \"VP#1\"}, {11->12, \"PP#1\"}, {14->13, \"AP#1\"}, {15->14, \"NP#1\"}, {18->15, \"S#1\"}, {17->16, \"VP#1\"}, {18->17, \"S#1\"}, {1->18, \"PP#1\"}, {20->19, \"VROOT#2\"}, {0->20, \"DROOT#1\"}, {27->21, \"VP#1\"}, {24->22, \"NP#1\"}, {24->23, \"NP#1\"}, {27->24, \"VP#1\"}, {27->25, \"VP#1\"}, {25->26, \"PP#1\"}, {20->27, \"S#1\"}, {20->28, \"VROOT#2\"}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\n"
     ]
    }
   ],
   "source": [
    "edges = []\n",
    "\n",
    "for i, (i_parent, i_lab, i_att) in enumerate(zip(ex, lab, att), 1):\n",
    "    edges.append(f\"{{{i_parent}->{i}, \\\"{inverse_sym_dict[i_lab.item()]}#{i_att}\\\"}}\")\n",
    "\n",
    "print(f\"TreePlot[{{{', '.join(edges)}}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreePlot[{{27->1, \"VP#1\"}, {1->2, \"PP#1\"}, {1->3, \"PP#1\"}, {1->4, \"PP#1\"}, {4->5, \"PP#1\"}, {4->6, \"PP#1\"}, {4->7, \"PP#1\"}, {1->8, \"PP#1\"}, {8->9, \"PP#1\"}, {20->10, \"VROOT#2\"}, {16->11, \"VP#1\"}, {11->12, \"PP#1\"}, {15->13, \"NP#1\"}, {15->14, \"NP#1\"}, {18->15, \"S#1\"}, {17->16, \"VP#1\"}, {18->17, \"S#1\"}, {1->18, \"PP#1\"}, {20->19, \"VROOT#2\"}, {0->20, \"DROOT#1\"}, {20->21, \"S#1\"}, {24->22, \"NP#1\"}, {24->23, \"NP#1\"}, {27->24, \"VP#1\"}, {24->25, \"NP#1\"}, {25->26, \"PP#1\"}, {20->27, \"S#1\"}, {20->28, \"VROOT#2\"}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\n"
     ]
    }
   ],
   "source": [
    "target_edges = []\n",
    "\n",
    "for i, (i_parent, i_lab, i_att) in enumerate(zip(t_ex, t_lab, t_att), 1):\n",
    "    target_edges.append(f\"{{{i_parent}->{i}, \\\"{inverse_sym_dict[i_lab.item()]}#{i_att}\\\"}}\")\n",
    "\n",
    "print(f\"TreePlot[{{{', '.join(target_edges)}}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"./models/epoch_25_cpu_eval.pickle\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "german",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
