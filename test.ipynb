{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from german_parser.util.const import CONSTS\n",
    "from german_parser.util.dataloader import TigerDatasetGenerator\n",
    "import dill as pickle\n",
    "from string import punctuation\n",
    "from german_parser.model import TigerModel\n",
    "from german_parser.util import BatchUnionFind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataloader, train_new_words), (dev_dataloader, dev_new_words), (test_dataloader, test_new_words), character_set, character_flag_generators, inverse_word_dict, inverse_sym_dict = pickle.load(open(\"required_vars.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = TigerModel(\n",
    "#     word_embedding_params=TigerModel.WordEmbeddingParams(char_set=character_set, char_flag_generators=character_flag_generators, char_internal_embedding_dim=100,\n",
    "#                                    char_part_embedding_dim=100, \n",
    "#                                    word_part_embedding_dim=100, \n",
    "#                                    char_internal_window_size=3,\n",
    "#                                    word_dict=inverse_word_dict),\n",
    "#     enc_lstm_params=TigerModel.LSTMParams(\n",
    "#         hidden_size=512,\n",
    "#         bidirectional=True,\n",
    "#         num_layers=3),\n",
    "#     dec_lstm_params=TigerModel.LSTMParams(\n",
    "#         hidden_size=512,\n",
    "#         bidirectional=False,\n",
    "#         num_layers=1\n",
    "#         ),\n",
    "#         enc_attention_mlp_dim=512,\n",
    "#         dec_attention_mlp_dim=512,\n",
    "#         enc_label_mlp_dim=128,\n",
    "#         dec_label_mlp_dim=128,\n",
    "#         num_biaffine_attention_classes=2,\n",
    "#         num_constituent_labels=len(inverse_sym_dict),\n",
    "#         enc_attachment_mlp_dim=128,\n",
    "#         dec_attachment_mlp_dim=64,\n",
    "#         max_attachment_order=train_dataloader.dataset.attachment_orders.max() + 1\n",
    "#     )\n",
    "# model = model.to(device=\"cuda\") # type: ignore\n",
    "\n",
    "model: TigerModel = pickle.load(open(\"./models/tiger_model_2023_09_04-02_35_02_PM_epoch_24.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (x, l, *_) = next(enumerate(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tree(model: TigerModel, input: tuple[torch.Tensor, torch.Tensor], new_words_dict: dict[int, str] | None):\n",
    "    \"\"\"NOTE: indices are 1-indexed. index 0 corresponds to virtual root of D-tree (which is different from virtual root of C-tree)\n",
    "\n",
    "    Args:\n",
    "        model (TigerModel): _description_\n",
    "        input (tuple[torch.Tensor, torch.Tensor]): _description_\n",
    "        new_words_dict (dict[int, str] | None): _description_\n",
    "    \"\"\"    \n",
    "    x, lengths = input\n",
    "    self_attention, constituent_labels, attachment_orders, indices = model.forward(input, new_words_dict)\n",
    "    # self_attention has size (B, T, T + 1)\n",
    "\n",
    "    B, T, *_ = self_attention.shape\n",
    "    uf = BatchUnionFind(B, model.beam_size, N=T + 1, device=model.dummy_param.device)\n",
    "\n",
    "    joint_logits = torch.zeros(B, model.beam_size, device=model.dummy_param.device) # (B, K)\n",
    "\n",
    "    edges = torch.zeros(B, model.beam_size, T, dtype=torch.long, device=model.dummy_param.device) # (B, K, T); m[b, k, t - 1] is the 1-indexed parent of 1-indexed node t, in batch b, beam k\n",
    "\n",
    "    for t in range(T):\n",
    "        relevant_batches = t < lengths # used for updating the final arcs. otherwise, we get nans in batch b if t >= sentence_length[b]\n",
    "\n",
    "        arc_probs = self_attention[:, t, :].log_softmax(dim=-1) # (B, T + 1)\n",
    "\n",
    "        candidate_joint_logits = joint_logits[:, :, None] + arc_probs[:, None, :] # (B, K, T + 1); the heuristic we would like to maximise\n",
    "\n",
    "        children = torch.tensor(t + 1, device=model.dummy_param.device) # all batches and beams share the same child index (1-indexed)\n",
    "        parents = torch.arange(0, T + 1, 1, device=model.dummy_param.device, dtype=torch.long).repeat(B, model.beam_size, 1) # (B, K, T + 1), where m[b, k, t] = t to represent the index of each parent \n",
    "\n",
    "        no_cycles_mask = uf.is_same_set(children.expand_as(parents), parents) # (B, K, T + 1); m[b, k, s + 1] is true if in batch b, beam k, joining child (t + 1) and parent (s + 1) would lead to a cycle\n",
    "        no_cycles_mask[:, :, 1:] |= ~indices.unsqueeze(1).to(device=model.dummy_param.device)\n",
    "\n",
    "        candidate_joint_logits[no_cycles_mask] = -torch.inf # mask cycles out so they can never be a maximiser\n",
    "        flattened_top_candidate_idx = candidate_joint_logits.flatten(-2, -1).argsort(dim=-1, descending=True)[:, :model.beam_size] # (B, K) in range [0, (K * T + 1)); for each batch, find top 10 best performing parent-beam combinations\n",
    "        \n",
    "        top_parents = parents.flatten(-2, -1).gather(index=flattened_top_candidate_idx, dim=-1) # (B, K); for each batch and beam, get the 1-indexed id of the parent we want to attach\n",
    "\n",
    "        batch_names = torch.arange(model.beam_size, device=model.dummy_param.device, dtype=torch.long).view(1, -1, 1).expand(B, -1, T + 1).flatten(-2, -1) # (B, K, T + 1); m[b, k, s] = k for all b, s, k\n",
    "        used_batches = batch_names.gather(index=flattened_top_candidate_idx, dim=-1) # (B, K) where each element is in the range [0, K). m[b, k] tells you what the kth new beam should be\n",
    "\n",
    "        new_data = uf.data.gather(index=used_batches.unsqueeze(-1).expand_as(uf.data), dim=1)\n",
    "        new_rank = uf.rank.gather(index=used_batches.unsqueeze(-1).expand_as(uf.rank), dim=1)\n",
    "        new_edges = edges.gather(index=used_batches.unsqueeze(-1).expand_as(edges), dim=1)\n",
    "        new_joint_logits = candidate_joint_logits.flatten(-2, -1).gather(index=flattened_top_candidate_idx, dim=-1)\n",
    "\n",
    "        uf.data[relevant_batches] = new_data[relevant_batches]\n",
    "        uf.rank[relevant_batches] = new_rank[relevant_batches]\n",
    "        edges[relevant_batches] = new_edges[relevant_batches]\n",
    "        joint_logits[relevant_batches] = new_joint_logits[relevant_batches]\n",
    "\n",
    "        uf.union(children.expand_as(top_parents), top_parents)\n",
    "\n",
    "        edges[:, :, t] = top_parents\n",
    "\n",
    "        pass\n",
    "\n",
    "    return edges, joint_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/envs/german/lib/python3.11/site-packages/torch/nn/modules/rnn.py:815: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
      "  result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    }
   ],
   "source": [
    "edges, joint_logits = find_tree(model, (x, l), test_new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [0, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        [0, 3, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 2, 2, 2, 2, 2, 2, 2],\n",
       "        [0, 3, 1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "         3, 3, 3, 3, 3, 3, 3, 3, 3, 3],\n",
       "        [0, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges[31, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/james/anaconda3/envs/german/lib/python3.11/site-packages/torch/nn/modules/rnn.py:815: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters(). (Triggered internally at ../aten/src/ATen/native/cudnn/RNN.cpp:982.)\n",
      "  result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132 of 133\r"
     ]
    }
   ],
   "source": [
    "for n, (x, l, *_) in enumerate(test_dataloader):\n",
    "    edges, joint_logits = find_tree(model, (x, l), test_new_words)\n",
    "    print(f\"{n} of {len(test_dataloader)}\", end=\"\\r\")\n",
    "    for i in range(len(x)):\n",
    "        if not joint_logits[i].allclose(joint_logits[i,0]):\n",
    "            print(\"YES!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (x, l, *_) = next(enumerate(test_dataloader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "german",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
