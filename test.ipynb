{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from german_parser.util.const import CONSTS\n",
    "from german_parser.util.dataloader import TigerDatasetGenerator\n",
    "import dill as pickle\n",
    "from string import punctuation\n",
    "from german_parser.model import TigerModel\n",
    "from german_parser.util import BatchUnionFind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataloader, train_new_words), (dev_dataloader, dev_new_words), (test_dataloader, test_new_words), character_set, character_flag_generators, inverse_word_dict, inverse_sym_dict = pickle.load(open(\"required_vars.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TigerModel(\n",
    "    word_embedding_params=TigerModel.WordEmbeddingParams(char_set=character_set, char_flag_generators=character_flag_generators, char_internal_embedding_dim=100,\n",
    "                                   char_part_embedding_dim=100, \n",
    "                                   word_part_embedding_dim=100, \n",
    "                                   char_internal_window_size=3,\n",
    "                                   word_dict=inverse_word_dict),\n",
    "    enc_lstm_params=TigerModel.LSTMParams(\n",
    "        hidden_size=512,\n",
    "        bidirectional=True,\n",
    "        num_layers=3),\n",
    "    dec_lstm_params=TigerModel.LSTMParams(\n",
    "        hidden_size=512,\n",
    "        bidirectional=False,\n",
    "        num_layers=1\n",
    "        ),\n",
    "        enc_attention_mlp_dim=512,\n",
    "        dec_attention_mlp_dim=512,\n",
    "        enc_label_mlp_dim=128,\n",
    "        dec_label_mlp_dim=128,\n",
    "        num_biaffine_attention_classes=2,\n",
    "        num_constituent_labels=len(inverse_sym_dict),\n",
    "        enc_attachment_mlp_dim=128,\n",
    "        dec_attachment_mlp_dim=64,\n",
    "        max_attachment_order=train_dataloader.dataset.attachment_orders.max() + 1\n",
    "    )\n",
    "# model = model.to(device=\"cuda\") # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (x, l, *_) = next(enumerate(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tree(model: TigerModel, input: tuple[torch.Tensor, torch.Tensor], new_words_dict: dict[int, str] | None):\n",
    "    \"\"\"NOTE: indices are 1-indexed. index 0 corresponds to virtual root of D-tree (which is different from virtual root of C-tree)\n",
    "\n",
    "    Args:\n",
    "        model (TigerModel): _description_\n",
    "        input (tuple[torch.Tensor, torch.Tensor]): _description_\n",
    "        new_words_dict (dict[int, str] | None): _description_\n",
    "    \"\"\"    \n",
    "    x, lengths = input\n",
    "    self_attention, constituent_labels, attachment_orders, indices = model.forward(input, new_words_dict)\n",
    "    # self_attention has size (B, T, T + 1)\n",
    "\n",
    "    B, T, *_ = self_attention.shape\n",
    "    uf = BatchUnionFind(B, model.beam_size, N=T + 1, device=model.dummy_param.device)\n",
    "\n",
    "    joint_logits = torch.zeros(B, model.beam_size) # (B, K)\n",
    "\n",
    "    for t in range(T):\n",
    "        arc_probs = self_attention[:, t, :].log_softmax(dim=-1)\n",
    "\n",
    "        candidate_joint_logits = joint_logits[:, :, None] + arc_probs[:, None, :] # (B, K, T + 1)\n",
    "\n",
    "        child_idx = torch.fill(torch.empty(B, model.beam_size, T + 1, device=\"cpu\", dtype=torch.long), t)\n",
    "\n",
    "        # arc_probs_argsort = arc_probs.argsort(dim=-1, descending=True)\n",
    "        # candidate_head_idx = torch.zeros()\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_tree(model, (x, l), train_new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "german",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
