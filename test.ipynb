{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from german_parser.util.const import CONSTS\n",
    "from german_parser.util.dataloader import TigerDatasetGenerator\n",
    "import dill as pickle\n",
    "from string import punctuation\n",
    "from german_parser.model import TigerModel\n",
    "from german_parser.util import BatchUnionFind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataloader, train_new_words), (dev_dataloader, dev_new_words), (test_dataloader, test_new_words), character_set, character_flag_generators, inverse_word_dict, inverse_sym_dict = pickle.load(open(\"required_vars.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: TigerModel = pickle.load(open(\"./models/tiger_model_2023_09_05-03_04_35_PM_epoch_5.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TigerModel(\n",
       "  (word_embedding): WordEmbedding(\n",
       "    (word_cnn): WordCNN(\n",
       "      (embeddings): Embedding(104, 100, padding_idx=1)\n",
       "      (conv): Conv1d(105, 100, kernel_size=(3,), stride=(1,))\n",
       "    )\n",
       "    (embeddings): Embedding(25019, 100, padding_idx=1)\n",
       "  )\n",
       "  (enc_lstm): LSTMSkip(\n",
       "    (chain): ModuleList(\n",
       "      (0): LSTM(200, 512, batch_first=True, bidirectional=True)\n",
       "      (1-2): 2 x LSTM(1024, 512, batch_first=True, bidirectional=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0.2, inplace=False)\n",
       "  )\n",
       "  (dec_lstm): LSTMSkip(\n",
       "    (chain): ModuleList(\n",
       "      (0): LSTM(1024, 512, batch_first=True)\n",
       "    )\n",
       "    (dropout): Dropout(p=0, inplace=False)\n",
       "  )\n",
       "  (enc_final_cell_to_dec_init_cell): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (enc_attention_mlp): Linear(in_features=1024, out_features=512, bias=True)\n",
       "  (dec_attention_mlp): Linear(in_features=512, out_features=512, bias=True)\n",
       "  (enc_label_mlp): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (dec_label_mlp): Linear(in_features=512, out_features=128, bias=True)\n",
       "  (enc_attachment_mlp): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (dec_attachment_mlp): Linear(in_features=512, out_features=64, bias=True)\n",
       "  (biaffine_attention): BiAffine()\n",
       "  (biaffine_constituent_classifier): BiAffine()\n",
       "  (biaffine_attachment_classifier): BiAffine()\n",
       ")"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_en = enumerate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (x, l, target_ex, target_lab, target_att) = next(dl_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tree(model: TigerModel, input: tuple[torch.Tensor, torch.Tensor], new_words_dict: dict[int, str] | None):\n",
    "    \"\"\"NOTE: indices are 1-indexed. index 0 corresponds to virtual root of D-tree (which is different from virtual root of C-tree)\n",
    "\n",
    "    Args:\n",
    "        model (TigerModel): _description_\n",
    "        input (tuple[torch.Tensor, torch.Tensor]): _description_\n",
    "        new_words_dict (dict[int, str] | None): _description_\n",
    "    \"\"\"    \n",
    "    x, lengths = input\n",
    "    self_attention, constituent_labels, attachment_orders, indices = model.forward(input, new_words_dict)\n",
    "    # self_attention has size (B, T, T + 1)\n",
    "\n",
    "    B, T, *_ = self_attention.shape\n",
    "    uf = BatchUnionFind(B, model.beam_size, N=T + 1, device=model.dummy_param.device)\n",
    "\n",
    "    joint_logits = torch.zeros(B, model.beam_size, device=model.dummy_param.device) # (B, K)\n",
    "\n",
    "    edges = torch.zeros(B, model.beam_size, T, dtype=torch.long, device=model.dummy_param.device) # (B, K, T); m[b, k, t - 1] is the 1-indexed parent of 1-indexed node t, in batch b, beam k\n",
    "\n",
    "    for t in range(T):\n",
    "        relevant_batches = t < lengths # used for updating the final arcs. otherwise, we get nans in batch b if t >= sentence_length[b]\n",
    "\n",
    "        arc_probs = self_attention[:, t, :].log_softmax(dim=-1) # (B, T + 1)\n",
    "\n",
    "        candidate_joint_logits = joint_logits[:, :, None] + arc_probs[:, None, :] # (B, K, T + 1); the heuristic we would like to maximise\n",
    "\n",
    "        children = torch.tensor(t + 1, device=model.dummy_param.device) # all batches and beams share the same child index (1-indexed)\n",
    "        parents = torch.arange(0, T + 1, 1, device=model.dummy_param.device, dtype=torch.long).repeat(B, model.beam_size, 1) # (B, K, T + 1), where m[b, k, t] = t to represent the index of each parent \n",
    "\n",
    "        no_cycles_mask = uf.is_same_set(children.expand_as(parents), parents) # (B, K, T + 1); m[b, k, s + 1] is true if in batch b, beam k, joining child (t + 1) and parent (s + 1) would lead to a cycle\n",
    "        no_cycles_mask[:, :, 1:] |= ~indices.unsqueeze(1).to(device=model.dummy_param.device)\n",
    "\n",
    "        candidate_joint_logits[no_cycles_mask] = -torch.inf # mask cycles out so they can never be a maximiser\n",
    "        flattened_top_candidate_idx = candidate_joint_logits.flatten(-2, -1).topk(k=model.beam_size, dim=-1).indices # (B, K) in range [0, (K * T + 1)); for each batch, find top 10 best performing parent-beam combinations\n",
    "        \n",
    "        top_parents = parents.flatten(-2, -1).gather(index=flattened_top_candidate_idx, dim=-1) # (B, K); for each batch and beam, get the 1-indexed id of the parent we want to attach\n",
    "\n",
    "        batch_names = torch.arange(model.beam_size, device=model.dummy_param.device, dtype=torch.long).view(1, -1, 1).expand(B, -1, T + 1).flatten(-2, -1) # (B, K, T + 1); m[b, k, s] = k for all b, s, k\n",
    "        used_batches = batch_names.gather(index=flattened_top_candidate_idx, dim=-1) # (B, K) where each element is in the range [0, K). m[b, k] tells you what the kth new beam should be\n",
    "\n",
    "        new_data = uf.data.gather(index=used_batches.unsqueeze(-1).expand_as(uf.data), dim=1)\n",
    "        new_rank = uf.rank.gather(index=used_batches.unsqueeze(-1).expand_as(uf.rank), dim=1)\n",
    "        new_edges = edges.gather(index=used_batches.unsqueeze(-1).expand_as(edges), dim=1)\n",
    "        new_joint_logits = candidate_joint_logits.flatten(-2, -1).gather(index=flattened_top_candidate_idx, dim=-1)\n",
    "\n",
    "        uf.data[relevant_batches] = new_data[relevant_batches]\n",
    "        uf.rank[relevant_batches] = new_rank[relevant_batches]\n",
    "        edges[relevant_batches] = new_edges[relevant_batches]\n",
    "        joint_logits[relevant_batches] = new_joint_logits[relevant_batches]\n",
    "\n",
    "        uf.union(children.expand_as(top_parents), top_parents)\n",
    "\n",
    "        edges[:, :, t] = top_parents\n",
    "\n",
    "        pass\n",
    "\n",
    "    num_labels = constituent_labels.shape[-1]\n",
    "    num_attachment_orders = attachment_orders.shape[-1]\n",
    "\n",
    "    best_edges = edges[torch.arange(edges.shape[0]), joint_logits.argmax(dim=-1)] # (B, T) containing elements in range [0, T + 1), where m[b, t - 1] denotes the 1-indexed parent of 1-indexed node t\n",
    "\n",
    "    label_logits_best_edges = constituent_labels.gather(index=best_edges[:, :, None, None].expand(-1, -1, -1, num_labels), dim=2).squeeze(2)\n",
    "    attachment_logits_best_edges = attachment_orders.gather(index=best_edges[:, :, None, None].expand(-1, -1, -1, num_attachment_orders), dim=2).squeeze(2)\n",
    "\n",
    "    labels_best_edges = label_logits_best_edges.argmax(-1)\n",
    "    attachment_orders_best_edges = attachment_logits_best_edges.argmax(-1)\n",
    "\n",
    "    return best_edges, labels_best_edges, attachment_orders_best_edges, (edges, joint_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_edges, labels_best_edges, attachment_orders_best_edges, (edges, joint_logits) = find_tree(model, (x, l), test_new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, (x, l, *_) in enumerate(test_dataloader):\n",
    "#     best_edges, labels_best_edges, attachment_orders_best_edges, (edges, joint_logits) = find_tree(model, (x, l), test_new_words)\n",
    "#     print(f\"{n} of {len(test_dataloader)}\", end=\"\\r\")\n",
    "#     for i in range(len(x)):\n",
    "#         if not joint_logits[i].allclose(joint_logits[i,0]):\n",
    "#             print(\"YES!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_num = 0\n",
    "\n",
    "words = x[s_num, :l[s_num]].to(\"cpu\")\n",
    "\n",
    "ex = best_edges[s_num, :l[s_num]].to(\"cpu\")\n",
    "lab = labels_best_edges[s_num, :l[s_num]].to(\"cpu\")\n",
    "att = attachment_orders_best_edges[s_num, :l[s_num]].to(\"cpu\")\n",
    "\n",
    "t_ex = target_ex[s_num, :l[s_num]].to(\"cpu\")\n",
    "t_lab = target_lab[s_num, :l[s_num]].to(\"cpu\")\n",
    "t_att = target_att[s_num, :l[s_num]].to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreePlot[{{3->1, \"NP#1\"}, {3->2, \"NP#1\"}, {4->3, \"S#1\"}, {0->4, \"DROOT#1\"}, {4->5, \"S#1\"}, {5->6, \"CNP#2\"}, {8->7, \"NP#1\"}, {5->8, \"CNP#2\"}, {10->9, \"NP#1\"}, {8->10, \"NP#1\"}, {16->11, \"NP#1\"}, {4->12, \"VROOT#2\"}, {16->13, \"S#1\"}, {13->14, \"PP#1\"}, {16->15, \"NP#1\"}, {4->16, \"S#1\"}, {23->17, \"VROOT#2\"}, {34->18, \"S#1\"}, {33->19, \"VP#1\"}, {19->20, \"PP#1\"}, {19->21, \"PP#1\"}, {23->22, \"VROOT#2\"}, {0->23, \"DROOT#1\"}, {26->24, \"NP#1\"}, {26->25, \"NP#1\"}, {41->26, \"S#1\"}, {41->27, \"S#1\"}, {30->28, \"PP#1\"}, {30->29, \"PP#1\"}, {41->30, \"S#1\"}, {30->31, \"PP#1\"}, {30->32, \"PP#1\"}, {34->33, \"S#1\"}, {23->34, \"S#1\"}, {23->35, \"VROOT#2\"}, {41->36, \"S#1\"}, {41->37, \"S#1\"}, {39->38, \"NP#1\"}, {41->39, \"S#1\"}, {41->40, \"S#1\"}, {34->41, \"S#1\"}, {4->42, \"VROOT#2\"}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\n"
     ]
    }
   ],
   "source": [
    "edges = []\n",
    "\n",
    "for i, (i_parent, i_lab, i_att) in enumerate(zip(ex, lab, att), 1):\n",
    "    edges.append(f\"{{{i_parent}->{i}, \\\"{inverse_sym_dict[i_lab.item()]}#{i_att}\\\"}}\")\n",
    "\n",
    "print(f\"TreePlot[{{{', '.join(edges)}}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreePlot[{{3->1, \"NP#1\"}, {3->2, \"NP#1\"}, {4->3, \"S#1\"}, {0->4, \"DROOT#1\"}, {4->5, \"S#1\"}, {5->6, \"CNP#1\"}, {8->7, \"NP#1\"}, {5->8, \"CNP#1\"}, {10->9, \"NP#1\"}, {4->10, \"S#1\"}, {4->11, \"S#1\"}, {4->12, \"VROOT#3\"}, {33->13, \"VP#1\"}, {13->14, \"PP#1\"}, {16->15, \"NP#1\"}, {13->16, \"PP#1\"}, {4->17, \"VROOT#3\"}, {13->18, \"CO#2\"}, {18->19, \"AP#1\"}, {19->20, \"PP#1\"}, {19->21, \"PP#1\"}, {4->22, \"VROOT#3\"}, {4->23, \"CS#2\"}, {26->24, \"NP#1\"}, {26->25, \"NP#1\"}, {23->26, \"S#1\"}, {33->27, \"VP#1\"}, {27->28, \"PP#1\"}, {27->29, \"PP#1\"}, {33->30, \"VP#1\"}, {30->31, \"PP#1\"}, {30->32, \"PP#1\"}, {34->33, \"VP#1\"}, {23->34, \"S#1\"}, {4->35, \"VROOT#3\"}, {41->36, \"S#1\"}, {41->37, \"S#1\"}, {39->38, \"NP#1\"}, {41->39, \"S#1\"}, {41->40, \"S#1\"}, {33->41, \"VP#1\"}, {4->42, \"VROOT#3\"}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\n"
     ]
    }
   ],
   "source": [
    "target_edges = []\n",
    "\n",
    "for i, (i_parent, i_lab, i_att) in enumerate(zip(t_ex, t_lab, t_att), 1):\n",
    "    target_edges.append(f\"{{{i_parent}->{i}, \\\"{inverse_sym_dict[i_lab.item()]}#{i_att}\\\"}}\")\n",
    "\n",
    "print(f\"TreePlot[{{{', '.join(target_edges)}}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from german_parser.util.c_and_d import Dependency, DependencyTree, ConstituentTree, Terminal\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_d_tree(heads: torch.Tensor, labels: torch.Tensor, orders: torch.Tensor, word_ids: torch.Tensor, word_dict: dict[int, str]):\n",
    "    num_words = len(heads)\n",
    "    modifiers: dict[int, dict[int, list[Dependency]]] = defaultdict(lambda: defaultdict(lambda x=None: []))\n",
    "    terminals: dict[int, Terminal] = {}\n",
    "\n",
    "    for child, (head, label, order, word) in enumerate(zip(heads, labels, orders, word_ids), start=1):\n",
    "        head = int(head.item())\n",
    "        label = int(label.item())\n",
    "        order = int(order.item())\n",
    "        word = int(word.item())\n",
    "\n",
    "        terminals[child] = Terminal(\n",
    "            idx=child,\n",
    "            word=word_dict[word]\n",
    "        )\n",
    "\n",
    "        # child with head of 0 is the root node of the d-tree; don't add it to modifiers dict\n",
    "        if head == 0:\n",
    "            continue\n",
    "        \n",
    "        modifiers[head][order].append(Dependency(head=head, modifier=child, sym=inverse_sym_dict[label]))\n",
    "\n",
    "    return DependencyTree(modifiers=modifiers, terminals=terminals, num_words=num_words)\n",
    "\n",
    "def create_c_tree(heads: torch.Tensor, labels: torch.Tensor, orders: torch.Tensor, word_ids: torch.Tensor, word_dict: dict[int, str]):\n",
    "    d_tree = create_d_tree(heads, labels, orders, word_ids, word_dict)\n",
    "    return ConstituentTree.from_d_tree(d_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m c_tree \u001b[39m=\u001b[39m create_c_tree(heads\u001b[39m=\u001b[39;49mex, labels\u001b[39m=\u001b[39;49mlab, orders\u001b[39m=\u001b[39;49matt, word_ids\u001b[39m=\u001b[39;49mwords, word_dict\u001b[39m=\u001b[39;49m{\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minverse_word_dict, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m{\u001b[39m-\u001b[39;49mi: w \u001b[39mfor\u001b[39;49;00m i, w \u001b[39min\u001b[39;49;00m test_new_words\u001b[39m.\u001b[39;49mitems()}})\n\u001b[1;32m      2\u001b[0m t_c_tree \u001b[39m=\u001b[39m create_c_tree(heads\u001b[39m=\u001b[39mt_ex, labels\u001b[39m=\u001b[39mt_lab, orders\u001b[39m=\u001b[39mt_att, word_ids\u001b[39m=\u001b[39mwords, word_dict\u001b[39m=\u001b[39m{\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minverse_word_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m{\u001b[39m-\u001b[39mi: w \u001b[39mfor\u001b[39;00m i, w \u001b[39min\u001b[39;00m test_new_words\u001b[39m.\u001b[39mitems()}})\n",
      "Cell \u001b[0;32mIn[35], line 30\u001b[0m, in \u001b[0;36mcreate_c_tree\u001b[0;34m(heads, labels, orders, word_ids, word_dict)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_c_tree\u001b[39m(heads: torch\u001b[39m.\u001b[39mTensor, labels: torch\u001b[39m.\u001b[39mTensor, orders: torch\u001b[39m.\u001b[39mTensor, word_ids: torch\u001b[39m.\u001b[39mTensor, word_dict: \u001b[39mdict\u001b[39m[\u001b[39mint\u001b[39m, \u001b[39mstr\u001b[39m]):\n\u001b[1;32m     29\u001b[0m     d_tree \u001b[39m=\u001b[39m create_d_tree(heads, labels, orders, word_ids, word_dict)\n\u001b[0;32m---> 30\u001b[0m     \u001b[39mreturn\u001b[39;00m ConstituentTree\u001b[39m.\u001b[39;49mfrom_d_tree(d_tree)\n",
      "File \u001b[0;32m~/programming/ml/german/german_parser/util/c_and_d.py:494\u001b[0m, in \u001b[0;36md_to_c\u001b[0;34m(cls, d)\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[39myield\u001b[39;00m the_id\n\u001b[1;32m    492\u001b[0m non_pre_terminal_id_generator \u001b[39m=\u001b[39m counter(\u001b[39m1000000\u001b[39m)\n\u001b[0;32m--> 494\u001b[0m \u001b[39mfor\u001b[39;00m h \u001b[39min\u001b[39;00m d\u001b[39m.\u001b[39mget_post_order_traversal():\n\u001b[1;32m    495\u001b[0m     \u001b[39m# create pre-terminals\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     v_id \u001b[39m=\u001b[39m h\n\u001b[1;32m    497\u001b[0m     \u001b[39massert\u001b[39;00m v_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m used_ids, \u001b[39m\"\u001b[39m\u001b[39mRan out of ids to use!\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[0;32m~/programming/ml/german/german_parser/util/c_and_d.py:468\u001b[0m, in \u001b[0;36mDependencyTree.get_post_order_traversal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    467\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_post_order_traversal\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 468\u001b[0m     \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_post_order_traversal(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_tree_map(), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_root())\n",
      "File \u001b[0;32m~/programming/ml/german/german_parser/util/c_and_d.py:439\u001b[0m, in \u001b[0;36mDependencyTree.get_root\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     modifier_words\u001b[39m.\u001b[39madd(a\u001b[39m.\u001b[39mmodifier)\n\u001b[1;32m    438\u001b[0m root_set \u001b[39m=\u001b[39m all_words \u001b[39m-\u001b[39m modifier_words\n\u001b[0;32m--> 439\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mlen\u001b[39m(root_set) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    441\u001b[0m \u001b[39mreturn\u001b[39;00m root_set\u001b[39m.\u001b[39mpop()\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "c_tree = create_c_tree(heads=ex, labels=lab, orders=att, word_ids=words, word_dict={**inverse_word_dict, **{-i: w for i, w in test_new_words.items()}})\n",
    "t_c_tree = create_c_tree(heads=t_ex, labels=t_lab, orders=t_att, word_ids=words, word_dict={**inverse_word_dict, **{-i: w for i, w in test_new_words.items()}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bracket(c_tree: ConstituentTree, node: int|None=None, ignore_pre_terminal_sym: bool=False, ignore_non_terminal_sym: bool=False, ignore_all_syms: bool=False):\n",
    "    if node is None:\n",
    "        return get_bracket(c_tree,\n",
    "                           node=c_tree.root,\n",
    "                           ignore_pre_terminal_sym=ignore_pre_terminal_sym,\n",
    "                           ignore_non_terminal_sym=ignore_non_terminal_sym,\n",
    "                           ignore_all_syms=ignore_all_syms\n",
    "                           )\n",
    "    \n",
    "    c = c_tree.constituents[node]\n",
    "    if c.is_pre_terminal:\n",
    "        return f\"({'?' if ignore_pre_terminal_sym or ignore_all_syms else c.sym} {c.id}={c_tree.terminals[node].word})\"\n",
    "    \n",
    "    res = f\"({'?' if ignore_non_terminal_sym or ignore_all_syms else c.sym} \"\n",
    "    for child in c.children:\n",
    "        res += get_bracket(c_tree,\n",
    "                           node=child,\n",
    "                           ignore_pre_terminal_sym=ignore_pre_terminal_sym,\n",
    "                           ignore_non_terminal_sym=ignore_non_terminal_sym,\n",
    "                           ignore_all_syms=ignore_all_syms\n",
    "                           )\n",
    "        \n",
    "    res += \")\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(VZ (ADV (PPOSS (? 4=rÃ¤umten)(? 2=zweijÃ¤hrige)(NP (? 3=Bedenkzeit)(? 1=Eine))(CAC (? 5=Major)(? 6=und)(NP (? 8=Commonwealth-Kollegen)(? 7=seine)))(NP (? 10=Nigerianern)(? 9=den))(APPO (? 13=nach)(? 14=Ablauf)(NP (? 16=Frist)(? 11=ein)(? 15=dieser))))(VVINF (? 18=pÃ¼nktlich)(APPO (? 19=zum)(? 20=nÃ¤chsten)(? 21=Commonwealth-Gipfel)))(PPOSS (? 23=soll)(? 25=afrikanische)(NP (? 26=Staat)(? 24=der))(APPO (? 27=in)(? 28=aller))(? 29=Form)(VVINF (? 34=werden)(VVINF (? 33=ausgeschlossen)(APPO (? 30=aus)(? 31=der)(? 32=Gemeinschaft)(PPOSS (? 41=nachkommt)(? 36=wenn)(? 37=er)(NP (? 39=Demokratisierungs-Forderungen)(? 38=den))(? 40=nicht)))))))(? 12=-)(? 17=,)(? 22=,)(? 35=,)(? 42=.))'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bracket(c_tree, ignore_all_syms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(VROOT (CS (S (? 4=rÃ¤umten)(NP (? 3=Bedenkzeit)(? 1=Eine)(? 2=zweijÃ¤hrige))(CNP (? 5=Major)(? 6=und)(NP (? 8=Commonwealth-Kollegen)(? 7=seine)))(NP (? 10=Nigerianern)(? 9=den))(? 11=ein))(S (? 23=soll)(NP (? 26=Staat)(? 24=der)(? 25=afrikanische))(VP (? 34=werden)(VP (? 33=ausgeschlossen)(CO (PP (? 13=nach)(? 14=Ablauf)(NP (? 16=Frist)(? 15=dieser)))(AP (? 18=pÃ¼nktlich)(PP (? 19=zum)(? 20=nÃ¤chsten)(? 21=Commonwealth-Gipfel))))(PP (? 27=in)(? 28=aller)(? 29=Form))(PP (? 30=aus)(? 31=der)(? 32=Gemeinschaft))(S (? 41=nachkommt)(? 36=wenn)(? 37=er)(NP (? 39=Demokratisierungs-Forderungen)(? 38=den))(? 40=nicht))))))(? 12=-)(? 17=,)(? 22=,)(? 35=,)(? 42=.))'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bracket(t_c_tree, ignore_all_syms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class B(object):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for &: 'B' and 'B'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m a \u001b[39m&\u001b[39;49m b\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for &: 'B' and 'B'"
     ]
    }
   ],
   "source": [
    "a & b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "german",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
