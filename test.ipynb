{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from german_parser.util.const import CONSTS\n",
    "from german_parser.util.dataloader import TigerDatasetGenerator\n",
    "import dill as pickle\n",
    "from string import punctuation\n",
    "from german_parser.model import TigerModel\n",
    "from german_parser.util import BatchUnionFind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_dataloader, train_new_words), (dev_dataloader, dev_new_words), (test_dataloader, test_new_words), character_set, character_flag_generators, inverse_word_dict, inverse_sym_dict = pickle.load(open(\"required_vars.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: TigerModel = pickle.load(open(\"./models/epoch_25_cpu_eval.pickle\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_en = enumerate(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, (x, l, target_ex, target_lab, target_att) = next(dl_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tree(model: TigerModel, input: tuple[torch.Tensor, torch.Tensor], new_words_dict: dict[int, str] | None):\n",
    "    \"\"\"NOTE: indices are 1-indexed. index 0 corresponds to virtual root of D-tree (which is different from virtual root of C-tree)\n",
    "\n",
    "    Args:\n",
    "        model (TigerModel): _description_\n",
    "        input (tuple[torch.Tensor, torch.Tensor]): _description_\n",
    "        new_words_dict (dict[int, str] | None): _description_\n",
    "    \"\"\"    \n",
    "    x, lengths = input\n",
    "    self_attention, constituent_labels, attachment_orders, indices = model.forward(input, new_words_dict)\n",
    "    # self_attention has size (B, T, T + 1)\n",
    "\n",
    "    B, T, *_ = self_attention.shape\n",
    "    uf = BatchUnionFind(B, model.beam_size, N=T + 1, device=model.dummy_param.device)\n",
    "\n",
    "    joint_logits = torch.zeros(B, model.beam_size, device=model.dummy_param.device) # (B, K)\n",
    "\n",
    "    edges = torch.zeros(B, model.beam_size, T, dtype=torch.long, device=model.dummy_param.device) # (B, K, T); m[b, k, t - 1] is the 1-indexed parent of 1-indexed node t, in batch b, beam k\n",
    "\n",
    "    same_as_beams = torch.zeros(B, model.beam_size) # m[b, k] == 1 if beam k in batch b is equal to beam 1 in batch b. allows us to keep track of duplicates. duplicates are when m[b, k] != k\n",
    "    \n",
    "    def beams_are_unique():\n",
    "        return same_as_beams == torch.arange(model.beam_size).repeat(B, 1) # return where m[b, k] == same_as_beams[b, k] != k\n",
    "\n",
    "\n",
    "    for t in range(T):\n",
    "        relevant_batches = t < lengths # used for updating the final arcs. otherwise, we get nans in batch b if t >= sentence_length[b]\n",
    "\n",
    "        arc_probs = self_attention[:, t, :].log_softmax(dim=-1) # (B, T + 1)\n",
    "\n",
    "        candidate_joint_logits = joint_logits[:, :, None] + arc_probs[:, None, :] # (B, K, T + 1); the heuristic we would like to maximise\n",
    "\n",
    "        children = torch.tensor(t + 1, device=model.dummy_param.device) # all batches and beams share the same child index (1-indexed)\n",
    "        parents = torch.arange(0, T + 1, 1, device=model.dummy_param.device, dtype=torch.long).repeat(B, model.beam_size, 1) # (B, K, T + 1), where m[b, k, t] = t to represent the index of each parent \n",
    "\n",
    "        no_cycles_mask = uf.is_same_set(children.expand_as(parents), parents) # (B, K, T + 1); m[b, k, s + 1] is true if in batch b, beam k, joining child (t + 1) and parent (s + 1) would lead to a cycle\n",
    "        no_cycles_mask[:, :, 1:] |= ~indices.unsqueeze(1).to(device=model.dummy_param.device) # avoid setting words as head that are beyond the end of the sentence\n",
    "        no_cycles_mask |= ~beams_are_unique()[:, :, None]\n",
    "\n",
    "\n",
    "        candidate_joint_logits[no_cycles_mask] = -torch.inf # mask cycles out so they can never be a maximiser\n",
    "        flattened_top_candidate_idx = candidate_joint_logits.flatten(-2, -1).topk(k=model.beam_size, dim=-1).indices # (B, K) in range [0, (K * T + 1)); for each batch, find top 10 best performing parent-beam combinations\n",
    "        \n",
    "        top_parents = parents.flatten(-2, -1).gather(index=flattened_top_candidate_idx, dim=-1) # (B, K); for each batch and beam, get the 1-indexed id of the parent we want to attach\n",
    "\n",
    "        batch_names = torch.arange(model.beam_size, device=model.dummy_param.device, dtype=torch.long).view(1, -1, 1).expand(B, -1, T + 1).flatten(-2, -1) # (B, K, T + 1); m[b, k, s] = k for all b, s, k\n",
    "        used_batches = batch_names.gather(index=flattened_top_candidate_idx, dim=-1) # (B, K) where each element is in the range [0, K). m[b, k] tells you what the kth new beam should be\n",
    "\n",
    "        same_as_beams = used_batches # we have copied over these beams, so for now, these beams must be equal\n",
    "\n",
    "        new_data = uf.data.gather(index=used_batches.unsqueeze(-1).expand_as(uf.data), dim=1)\n",
    "        new_rank = uf.rank.gather(index=used_batches.unsqueeze(-1).expand_as(uf.rank), dim=1)\n",
    "        new_edges = edges.gather(index=used_batches.unsqueeze(-1).expand_as(edges), dim=1)\n",
    "        new_joint_logits = candidate_joint_logits.flatten(-2, -1).gather(index=flattened_top_candidate_idx, dim=-1)\n",
    "        \n",
    "\n",
    "        uf.data[relevant_batches] = new_data[relevant_batches]\n",
    "        uf.rank[relevant_batches] = new_rank[relevant_batches]\n",
    "        edges[relevant_batches] = new_edges[relevant_batches]\n",
    "        joint_logits[relevant_batches] = new_joint_logits[relevant_batches]\n",
    "\n",
    "        uf.union(children.expand_as(top_parents), top_parents)\n",
    "\n",
    "        edges[:, :, t] = top_parents\n",
    "        new_unique_beams = top_parents.gather(index=same_as_beams, dim=-1) != top_parents  # indicates the beams that have BECOME unqiue: given any batch b, suppose beam j was a copy of beam k. suppose the new parent for beam j is different to the new parent of beam k. then m[b, j] = True\n",
    "        same_as_beams[new_unique_beams] = torch.arange(model.beam_size).repeat(B, 1)[new_unique_beams]\n",
    "\n",
    "        pass\n",
    "\n",
    "    num_labels = constituent_labels.shape[-1]\n",
    "    num_attachment_orders = attachment_orders.shape[-1]\n",
    "\n",
    "    best_edges = edges[torch.arange(edges.shape[0]), joint_logits.argmax(dim=-1)] # (B, T) containing elements in range [0, T + 1), where m[b, t - 1] denotes the 1-indexed parent of 1-indexed node t\n",
    "\n",
    "    label_logits_best_edges = constituent_labels.gather(index=best_edges[:, :, None, None].expand(-1, -1, -1, num_labels), dim=2).squeeze(2)\n",
    "    attachment_logits_best_edges = attachment_orders.gather(index=best_edges[:, :, None, None].expand(-1, -1, -1, num_attachment_orders), dim=2).squeeze(2)\n",
    "\n",
    "    labels_best_edges = label_logits_best_edges.argmax(-1)\n",
    "    attachment_orders_best_edges = attachment_logits_best_edges.argmax(-1)\n",
    "\n",
    "    return best_edges, labels_best_edges, attachment_orders_best_edges, (edges, joint_logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_edges, labels_best_edges, attachment_orders_best_edges, (edges, joint_logits) = find_tree(model, (x, l), test_new_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for n, (x, l, *_) in enumerate(test_dataloader):\n",
    "#     best_edges, labels_best_edges, attachment_orders_best_edges, (edges, joint_logits) = find_tree(model, (x, l), test_new_words)\n",
    "#     print(f\"{n} of {len(test_dataloader)}\", end=\"\\r\")\n",
    "#     for i in range(len(x)):\n",
    "#         if not joint_logits[i].allclose(joint_logits[i,0]):\n",
    "#             print(\"YES!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_num = 0\n",
    "\n",
    "words = x[s_num, :l[s_num]].to(\"cpu\")\n",
    "\n",
    "ex = best_edges[s_num, :l[s_num]].to(\"cpu\")\n",
    "lab = labels_best_edges[s_num, :l[s_num]].to(\"cpu\")\n",
    "att = attachment_orders_best_edges[s_num, :l[s_num]].to(\"cpu\")\n",
    "\n",
    "t_ex = target_ex[s_num, :l[s_num]].to(\"cpu\")\n",
    "t_lab = target_lab[s_num, :l[s_num]].to(\"cpu\")\n",
    "t_att = target_att[s_num, :l[s_num]].to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreePlot[{{11->1, \"S#1\"}, {1->2, \"PP#1\"}, {11->3, \"VROOT#2\"}, {5->4, \"PP#1\"}, {1->5, \"PP#1\"}, {5->6, \"PP#1\"}, {5->7, \"PP#1\"}, {5->8, \"PP#1\"}, {8->9, \"PP#1\"}, {11->10, \"VROOT#2\"}, {0->11, \"DROOT#1\"}, {13->12, \"NP#1\"}, {11->13, \"S#1\"}, {11->14, \"VROOT#2\"}, {24->15, \"S#1\"}, {18->16, \"NP#1\"}, {18->17, \"NP#1\"}, {24->18, \"S#1\"}, {18->19, \"NP#1\"}, {19->20, \"PP#1\"}, {19->21, \"PP#1\"}, {23->22, \"AP#1\"}, {24->23, \"S#1\"}, {13->24, \"NP#1\"}, {11->25, \"VROOT#2\"}, {33->26, \"S#1\"}, {28->27, \"NP#1\"}, {33->28, \"S#1\"}, {32->29, \"VP#1\"}, {32->30, \"VP#1\"}, {30->31, \"AVP#1\"}, {33->32, \"S#1\"}, {22->33, \"AVP#1\"}, {11->34, \"VROOT#2\"}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\n"
     ]
    }
   ],
   "source": [
    "edges = []\n",
    "\n",
    "for i, (i_parent, i_lab, i_att) in enumerate(zip(ex, lab, att), 1):\n",
    "    edges.append(f\"{{{i_parent}->{i}, \\\"{inverse_sym_dict[i_lab.item()]}#{i_att}\\\"}}\")\n",
    "\n",
    "print(f\"TreePlot[{{{', '.join(edges)}}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TreePlot[{{11->1, \"S#1\"}, {1->2, \"PP#1\"}, {11->3, \"VROOT#2\"}, {5->4, \"PP#1\"}, {1->5, \"PP#1\"}, {5->6, \"PP#1\"}, {5->7, \"PP#1\"}, {5->8, \"PP#1\"}, {8->9, \"PP#1\"}, {11->10, \"VROOT#2\"}, {0->11, \"DROOT#1\"}, {13->12, \"NP#1\"}, {11->13, \"S#1\"}, {11->14, \"VROOT#2\"}, {24->15, \"S#1\"}, {18->16, \"NP#1\"}, {18->17, \"NP#1\"}, {24->18, \"S#1\"}, {18->19, \"NP#1\"}, {19->20, \"PP#1\"}, {19->21, \"PP#1\"}, {23->22, \"AP#1\"}, {24->23, \"S#1\"}, {13->24, \"NP#1\"}, {11->25, \"VROOT#2\"}, {33->26, \"S#1\"}, {28->27, \"NP#1\"}, {33->28, \"S#1\"}, {32->29, \"VP#1\"}, {32->30, \"VP#1\"}, {30->31, \"AVP#1\"}, {33->32, \"S#1\"}, {22->33, \"AVP#1\"}, {11->34, \"VROOT#2\"}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\n"
     ]
    }
   ],
   "source": [
    "target_edges = []\n",
    "\n",
    "for i, (i_parent, i_lab, i_att) in enumerate(zip(t_ex, t_lab, t_att), 1):\n",
    "    target_edges.append(f\"{{{i_parent}->{i}, \\\"{inverse_sym_dict[i_lab.item()]}#{i_att}\\\"}}\")\n",
    "\n",
    "print(f\"TreePlot[{{{', '.join(target_edges)}}}, Top, 0, VertexLabels -> Automatic, DirectedEdges -> True]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from german_parser.util.c_and_d import Dependency, DependencyTree, ConstituentTree, Terminal\n",
    "from collections import defaultdict\n",
    "\n",
    "def create_d_tree(heads: torch.Tensor, labels: torch.Tensor, orders: torch.Tensor, word_ids: torch.Tensor, word_dict: dict[int, str]):\n",
    "    num_words = len(heads)\n",
    "    modifiers: dict[int, dict[int, list[Dependency]]] = defaultdict(lambda: defaultdict(lambda x=None: []))\n",
    "    terminals: dict[int, Terminal] = {}\n",
    "\n",
    "    for child, (head, label, order, word) in enumerate(zip(heads, labels, orders, word_ids), start=1):\n",
    "        head = int(head.item())\n",
    "        label = int(label.item())\n",
    "        order = int(order.item())\n",
    "        word = int(word.item())\n",
    "\n",
    "        terminals[child] = Terminal(\n",
    "            idx=child,\n",
    "            word=word_dict[word]\n",
    "        )\n",
    "\n",
    "        # child with head of 0 is the root node of the d-tree; don't add it to modifiers dict\n",
    "        if head == 0:\n",
    "            continue\n",
    "        \n",
    "        modifiers[head][order].append(Dependency(head=head, modifier=child, sym=inverse_sym_dict[label]))\n",
    "\n",
    "    return DependencyTree(modifiers=modifiers, terminals=terminals, num_words=num_words)\n",
    "\n",
    "def create_c_tree(heads: torch.Tensor, labels: torch.Tensor, orders: torch.Tensor, word_ids: torch.Tensor, word_dict: dict[int, str]):\n",
    "    d_tree = create_d_tree(heads, labels, orders, word_ids, word_dict)\n",
    "    return ConstituentTree.from_d_tree(d_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_tree = create_c_tree(heads=ex, labels=lab, orders=att, word_ids=words, word_dict={**inverse_word_dict, **{-i: w for i, w in test_new_words.items()}})\n",
    "t_c_tree = create_c_tree(heads=t_ex, labels=t_lab, orders=t_att, word_ids=words, word_dict={**inverse_word_dict, **{-i: w for i, w in test_new_words.items()}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bracket(c_tree: ConstituentTree, node: int|None=None, ignore_pre_terminal_sym: bool=False, ignore_non_terminal_sym: bool=False, ignore_all_syms: bool=False):\n",
    "    if node is None:\n",
    "        return get_bracket(c_tree,\n",
    "                           node=c_tree.root,\n",
    "                           ignore_pre_terminal_sym=ignore_pre_terminal_sym,\n",
    "                           ignore_non_terminal_sym=ignore_non_terminal_sym,\n",
    "                           ignore_all_syms=ignore_all_syms\n",
    "                           )\n",
    "    \n",
    "    c = c_tree.constituents[node]\n",
    "    if c.is_pre_terminal:\n",
    "        return f\"({'?' if ignore_pre_terminal_sym or ignore_all_syms else c.sym} {c.id}={c_tree.terminals[node].word})\"\n",
    "    \n",
    "    res = f\"({'?' if ignore_non_terminal_sym or ignore_all_syms else c.sym} \"\n",
    "    for child in c.children:\n",
    "        res += get_bracket(c_tree,\n",
    "                           node=child,\n",
    "                           ignore_pre_terminal_sym=ignore_pre_terminal_sym,\n",
    "                           ignore_non_terminal_sym=ignore_non_terminal_sym,\n",
    "                           ignore_all_syms=ignore_all_syms\n",
    "                           )\n",
    "        \n",
    "    res += \")\"\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(VROOT (S (? 11=bestehe)(PP (? 1=Bei)(? 2=Großversuchen)(PP (? 5=bei)(? 4=wie)(? 6=der)(? 7=AOK)(PP (? 8=in)(? 9=Sachsen))))(NP (? 13=Gefahr)(? 12=die)(S (? 24=werde)(? 15=daß)(NP (? 18=Druck)(? 16=der)(? 17=soziale)(PP (? 19=auf)(? 20=die)(? 21=Patienten)))(AP (? 23=groß)(AVP (? 22=so)(S (? 33=sei)(? 26=daß)(NP (? 28=Freiwilligkeit)(? 27=die))(VP (? 32=gewährleistet)(? 29=faktisch)(AVP (? 30=nicht)(? 31=mehr)))))))))(? 3=,)(? 10=,)(? 14=,)(? 25=,)(? 34=.))'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bracket(c_tree, ignore_all_syms=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'(VROOT (S (? 11=bestehe)(PP (? 1=Bei)(? 2=Großversuchen)(PP (? 5=bei)(? 4=wie)(? 6=der)(? 7=AOK)(PP (? 8=in)(? 9=Sachsen))))(NP (? 13=Gefahr)(? 12=die)(S (? 24=werde)(? 15=daß)(NP (? 18=Druck)(? 16=der)(? 17=soziale)(PP (? 19=auf)(? 20=die)(? 21=Patienten)))(AP (? 23=groß)(AVP (? 22=so)(S (? 33=sei)(? 26=daß)(NP (? 28=Freiwilligkeit)(? 27=die))(VP (? 32=gewährleistet)(? 29=faktisch)(AVP (? 30=nicht)(? 31=mehr)))))))))(? 3=,)(? 10=,)(? 14=,)(? 25=,)(? 34=.))'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_bracket(t_c_tree, ignore_all_syms=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "german",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
